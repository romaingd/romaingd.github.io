<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>ICLR 2018 - 1 - Divergences</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Divergences" property="og:site_name">
  
    <meta content="ICLR 2018 - 1" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="ICLR 2018 1st article" property="og:description">
  
  
    <meta content="http://localhost:4000/ICLR/" property="og:url">
  
  
    <meta content="2018-08-21T01:00:00+02:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/profile-picture.jpg" property="og:image">
  
  
    
  
  
    
    <meta content="ICLR" property="article:tag">
    
    <meta content="ICLR 2018" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="ICLR 2018 - 1">
  
  
    <meta name="twitter:url" content="http://localhost:4000/ICLR/">
  
  
    <meta name="twitter:description" content="ICLR 2018 1st article">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/profile-picture.jpg">
  

	<meta name="description" content="ICLR 2018 1st article">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/profile-picture.jpg" alt="Romain Girard"></a>
      </div>
      <div class="author-name">Romain Girard</div>
      <p>I currently work as a <strong> data scientist </strong> at Implicity, where I develop machine learning techniques to study strokes and heart failure. <br><br> I am strongly interested in both <strong>data science for healthcare</strong> and <strong>theoretical machine learning</strong>. <br><br> I graduated in 2018 from École polytechnique. <br><br> I hold a M.Sc. degree from the MVA <i>(Mathématiques, Vision, Apprentissage)</i> master at ENS Paris-Saclay, with an emphasis on machine learning and statistics.
</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        <!-- 
          <li><a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
         -->
        
          <li class="github"><a href="http://github.com/romaingd" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/romain-girard-4396ab11b/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:romain.girard+Divergences@polytechnique.edu"><i class="fa fa-envelope-o" aria-hidden="true"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2018 &copy; Romain Girard</p>
      <p>Flexible Jekyll theme by Artem Sheludko</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">ICLR 2018 - 1</h1>
        <div class="page-date"><span>2018, Aug 21&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <h1 id="on-the-convergence-of-adam-and-beyond">On the convergence of ADAM and beyond</h1>

<h2 id="s-reddi-s-kale-s-kumar">S. Reddi, S. Kale, S. Kumar</h2>

<hr />

<h2 id="abstract">Abstract</h2>

<p>ADAM has become completely dominant in deep learning over the past few years.
This stochastic optimization algorithm is a variant of SGD, in the same vein
as ADAGRAD, insofar that it adjusts the learning rate on a per-feature basis
based on past gradients. However, ADAM specifically uses an exponential moving
average of past gradients, limiting the reliance to the past few gradients.</p>

<p>The authors show that this sort-term memory makes the convergence proof of ADAM
flawed. Intuitively, informative gradients that happen rarely are killed out
too fast by the short-term memory. The authors provide a simple counterexample
where ADAM converges to the worst solution possible.</p>

<p>Although it is of high importance, this result doesn’t change the fact that
ADAM tends to work very well in deep learning, where data distribution
is heuristically quite smooth. It seems that the variant provided by the authors
(AMSGRAD) doesn’t translate into much better performance.</p>

<hr />

<h2 id="i---introduction">I - Introduction</h2>

<ul>
  <li>
    <p><strong>SGD (Stochastic Gradient Descent)</strong> : Update the parameters of a model by
a step in the direction of the negative gradient of the loss evaluated
on a <em>minibatch</em>.</p>
  </li>
  <li>
    <p>Due to the large amount of data involved, SGD is the dominant method to
train deep neural networks (DNN) today.</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p><strong>ADAGRAD (Adapative Gradient)</strong> : Variant of SGD that scales coordinates
of the gradient by square roots of some form of <em>averaging of past values</em>.</p>
  </li>
  <li>
    <p>SGD variants in ADAGRAD’s vein are popular and successful because they
<strong>adjust the learning rate on a per-feature basis</strong>.</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p>ADAGRAD’s performance falls in non-convex and dense settings: using <em>all</em> past
gradients makes the learning rate decay too quick, and early updates lose
too much impact.</p>
  </li>
  <li>
    <p><strong>ADAM (Adaptive Moment estimation)</strong> : ADAGRAD idea + mitigate the rapid
learning rate decay using an exponential moving average of squared past
gradients (limit reliance to only the <em>past few gradients</em>)</p>
  </li>
  <li>
    <p>This is also the idea of RMSPROP, ADADELTA, NADAM, …</p>
  </li>
  <li>
    <p>Empirically, <strong>ADAM fails to converge when important information happens
rarely, because it is forgotten too fast</strong>
(“<em>when some minibatches provide large
gradients but only quite rarely; although they are quite informative, their
influence dies out rather quickly due to exponential moving average</em>”)</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>In this paper :
    <ul>
      <li><strong>Counterexample of a convex optimization problem where ADAM
does not converge</strong></li>
      <li>Localization of the <strong>error in ADAM’s convergence proof</strong></li>
      <li>ADAM variants with <strong>long-term gradient memory</strong></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="ii---preliminaries">II - Preliminaries</h2>

<h3 id="optimization-setup">Optimization setup</h3>

<ul>
  <li>
    <p>Online optimization problem in the full information feedback setting
(minimize regret)</p>
  </li>
  <li>
    <p>At each time step $t$ :</p>
    <ul>
      <li>Pick a point $x_t \in \mathcal{F}$
<strong>(parameters of the model, e.g. weights)</strong></li>
      <li>Gain access to a loss function $f_t$
<strong>(loss of the model on the next minibatch)</strong></li>
      <li>Incur loss $f_t (x_t)$</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><strong>SGD</strong> : $x_{t+1} = \Pi_{\mathcal{F}} (x_t - \alpha_t g_t)$ with
$g_t = \nabla f_t(x_t)$
(gradient descent projected onto the set of feasible points)</li>
</ul>

<p><br /></p>

<h3 id="generic-adaptive-methods">Generic adaptive methods</h3>

<p><br />Access</p>

<!-- <center> -->

<p><img src="/assets/img/Gini_index_map.svg" alt="lsfkj" class="center-image" /></p>

<!-- ![Generic Adaptive Method Setup](pictures/01-generic_adaptive.png) -->

<center>

<b>Generic framework for adaptive algorithms</b>

</center>

<p><br /></p>

<ul>
  <li>Additional notes :
    <ul>
      <li>$\alpha_t$ step size</li>
      <li>$\alpha_t V_t^{-1/2}$ learning rate</li>
      <li>Restriction to $V_t = \text{diag}(v_t)$</li>
      <li>Decreasing step size is required for convergence</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="sgd">SGD</h4>

<center>

![SGD phi and psi](pictures/01-SGD.png)

**SGD - Specifications in the framework** </center>

<p><br /></p>

<ul>
  <li>Aggressive learning rate decay $\alpha / \sqrt{t}$</li>
</ul>

<p><br /></p>

<h4 id="adagrad">ADAGRAD</h4>

<center>

![ADAGRAD phi and psi](pictures/01-ADAGRAD.png)

**ADAGRAD - Specifications in the framework** </center>

<p><br /></p>

<ul>
  <li>Modest learning rate decay $\alpha / \sqrt{\sum_i g_{i,j}^2}$</li>
</ul>

<p><br /></p>

<h4 id="adam">ADAM</h4>

<center>

![ADAM phi and psi](pictures/01-ADAM.png)

**ADAM - Specifications in the framework (main formulation)** </center>

<center>

![ADAM moment formulation](pictures/01-ADAM_moment.png)

**ADAM - Moment update formulation (alternative formulation)** </center>
<p><br /></p>

<ul>
  <li>
    <p>ADAGRAD with momentum (exponential moving average)</p>
  </li>
  <li>
    <p>The momentum term with $\beta_1 &gt; 0$ significantly boosts the performance,
especially in deep learning.</p>
  </li>
</ul>

<hr />

<h2 id="iii---the-non-convergence-of-adam">III - The non-convergence of ADAM</h2>

<ul>
  <li><strong>The proof of convergence of ADAM (given in Kingma &amp; Ba, 2015) is wrong.</strong></li>
</ul>

<center>

![ADAM error](pictures/01-ADAM_error.png)

**This quantity should be semi-definite positive.** <br />
**It is for SGD and ADAGRAD,
but not for ADAM.** </center>
<p><br /></p>

<ul>
  <li>The error essentially lies in the fact that the learning rate does not
always decay.</li>
</ul>

<p><br /></p>

<center>

![ADAM counterexample](pictures/01-ADAM_counterexample.png)

**Simple counterexample where ADAM converges to the worst solution.** <br />
<strong> $\mathcal{F} = [-1,1]$, $\ \ C&gt;2$, $\ \ \beta_1=0$,
$\ \ \beta_2=1/(1+C^2)$ </strong>
</center>
<p><br /></p>

<ul>
  <li>
    <p><strong>There is an online convex optimization problem where ADAM fails.</strong></p>
  </li>
  <li>
    <p>One could think that adding a small constant $\epsilon$ to $v_t$ would solve
the problem. Actually, although it helps, <strong>for any $\epsilon &gt; 0$, there is
an online convex optimization problem where ADAM fails.</strong></p>
  </li>
  <li>
    <p>One could think that using a large $\beta_2$ would solve the problem.
Actually, although it helps, <strong>for any $\beta_1, \beta_2 \in [0,1)$ such that
$\beta_1 &lt; \sqrt{\beta_2}$, there exists an online convex optimization problem
where ADAM fails.</strong> (Note that this condition is typically satisfied with
suggested parameters)</p>
  </li>
  <li>
    <p>One could think that this problem is specific to online optimization problems,
and that stochastic optimization problems would not be affected by this issue.
Actually, although stochastic optimization is typically easier, <strong>for any
$\beta_1, \beta_2 \in [0,1)$ such that $\beta_1 &lt; \sqrt{\beta_2}$,
there exists an online convex optimization problem where ADAM fails.</strong></p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>This means one has to use “problem-dependent” update hyperparameters ;
in high-dimensional settings, this typically means using different
hyperparameters for each dimension, which defeats the purpose of adaptive
algorithms.</li>
</ul>

<hr />

<h2 id="iv---a-new-exponential-moving-average-variant--amsgrad">IV - A new exponential moving average variant : AMSGRAD</h2>

<center>

![AMSGRAD](pictures/01-AMSGRAD.png)

</center>

<p><br /></p>

<ul>
  <li>
    <p>The key difference between AMSGRAD and ADAM is that the former maintains
the maximum of all previously seen $v_t$, and uses it to normalize the running
average of the gradient instead of $v_t$ (ADAM).</p>
  </li>
  <li>
    <p>In other words, <strong>to ensure a non-increasing learning rate, AMSGRAD uses
the maximum value of the normalization term, instead of its current value.</strong></p>
  </li>
  <li>
    <p>Proof of convergence is provided, with a data-dependent ensured regret of
<strong>$O(\sqrt{T})$</strong>, which can be considerably <strong>better than $O(\sqrt{dT})$ of
SGD.</strong></p>
  </li>
</ul>

      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=ICLR 2018 - 1&url=http://localhost:4000/ICLR/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/ICLR/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/ICLR/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#ICLR" class="tag">&#35; ICLR</a>
          
            <a href="/tags#ICLR 2018" class="tag">&#35; ICLR 2018</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(', '\\)'] ],
      displayMath: [ ['$$','$$'], ['\[', '\]'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</html>
