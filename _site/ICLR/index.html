<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>ADAM's convergence proof is flawed - Divergences</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Divergences" property="og:site_name">
  
    <meta content="ADAM's convergence proof is flawed" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="[ICLR 2018 - 1] "On the convergence of ADAM and beyond", Reddi et al., 2018
" property="og:description">
  
  
    <meta content="http://localhost:4000/ICLR/" property="og:url">
  
  
    <meta content="2018-08-19T01:00:00+02:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/2018-08-19-ICLR-thumbnail.png" property="og:image">
  
  
    
  
  
    
    <meta content="ICLR" property="article:tag">
    
    <meta content="ICLR 2018" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="ADAM's convergence proof is flawed">
  
  
    <meta name="twitter:url" content="http://localhost:4000/ICLR/">
  
  
    <meta name="twitter:description" content="[ICLR 2018 - 1] "On the convergence of ADAM and beyond", Reddi et al., 2018
">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/2018-08-19-ICLR-thumbnail.png">
  

	<meta name="description" content="[ICLR 2018 - 1] "On the convergence of ADAM and beyond", Reddi et al., 2018
">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/profile-picture.jpg" alt="Romain Girard"></a>
      </div>
      <div class="author-name">Romain Girard</div>
      <p>I currently work as a <strong> data scientist </strong> at Implicity, where I develop machine learning techniques to study strokes and heart failure. <br><br> I am strongly interested in both <strong>data science for healthcare</strong> and <strong>theoretical machine learning</strong>. <br><br> I graduated in 2018 from École polytechnique. <br><br> I hold a M.Sc. degree from the MVA <i>(Mathématiques, Vision, Apprentissage)</i> master at ENS Paris-Saclay, with an emphasis on machine learning and statistics.
</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        <!-- 
          <li><a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
         -->
        
          <li class="github"><a href="http://github.com/romaingd" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/romain-girard-4396ab11b/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:romain.girard+Divergences@polytechnique.edu"><i class="fa fa-envelope-o" aria-hidden="true"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2018 &copy; Romain Girard</p>
      <p>Flexible Jekyll theme by Artem Sheludko</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/2018-08-19-ICLR-thumbnail.png alt="ADAM's convergence proof is flawed">
        
          <figcaption>An example where ADAM fails to converge to the true solution</figcaption>
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">ADAM's convergence proof is flawed</h1>
        <div class="page-date"><span>2018, Aug 19&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p><br /></p>

<hr />

<h4 id="iclr-2018---1st-article">ICLR 2018 - 1st article</h4>

<p><em>In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Each oral paper is analyzed and
commented in an accessible way.</em></p>

<p><em>This article is based on the paper</em>
<a href="https://openreview.net/pdf?id=ryQu7f-RZ">On the convergence of ADAM and beyond</a>
<em>by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.</em></p>

<hr />

<p><br /></p>

<p>If you asked people in the deep learning community what their favorite optimization
algorithm is, you’d probably see <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a>
in the top 2 (with Stochastic gradient descent + momentum),
far ahead of alternatives like Adagrad or RMSProp (we’ll come to
these in a minute). <b>Adam has become very popular in deep
learning since it was proposed by Kingma &amp; Ba in December 2014</b>.
The reasons are easy to
understand: it exhibits impressive performance, uses many important ideas of
previous works, comes with predefined settings that already work very well,
and does not require careful hyper-parameter tuning.
<em>Better performance with less work, what more do you want?</em></p>

<p>Naturally, great power comes with great responsibilities, even for optimization
algorithms like Adam. At the very least, we expect some theoretical guarantees
on the convergence properties, to make sure that the algorithm is actually doing
its job when required. This is a necessary sanity check, confirming by
the maths that we can legitimately have faith in its correctness.</p>

<p>The convergence proof was provided in the original Adam paper.
However, our authors show in their paper that <strong>this original proof is flawed,
and Adam does not correctly converge in all problems</strong>, as illustrated by this
theorem:</p>

<blockquote>
  <p><strong>Theorem 3.</strong> [With mild assumptions on the parameters], there is a
stochastic convex optimization problem for which Adam does not converge to the
optimal solution.</p>
</blockquote>

<p>How have we reached this point, where the most widely used
optimization algorithm in deep learning does not converge in some simple
convex problems? Let’s go back some years, and see what lead us there.</p>

<p><br /><br /></p>

<h2 id="gradient-descent-and-adaptive-methods">Gradient descent and adaptive methods</h2>

<p>Let’s define our online optimization framework.
At each time step $t$:</p>
<ul>
  <li>The algorithm picks a
point $x_t$ (weights of the model) in the feasible set $\mathcal{F}$.</li>
  <li>We then
get access to the next mini-batch, with the associated loss function $f_t$,
and we incur the loss $f_t(x_t)$.</li>
</ul>

<p>Usually, our goal is to find an optimal
parameter $x$ such that the loss $f(x)$ on the entire training set is minimal
(intermediate loss functions $f_t$ are used as stochastic approximations of
$f$). Here, in the (equivalent) online setup, the goal
is to <strong>minimize the total regret at time $T$</strong>:</p>

<script type="math/tex; mode=display">R_T = \sum_{i=1}^T f_t(x_t) - \min_{x \in
\mathcal{F}} (\sum_{i=1}^T f_t(x))</script>

<p>A large number of optimization algorithms were proposed to achieve this goal.
For a detailed and intuitive overview (and introduction to gradient descent),
I recommend this excellent
<a href="http://ruder.io/optimizing-gradient-descent/index.html">blog post</a> by Sebastian
Ruder. We will just quickly go over a couple major steps that lead us to Adam.</p>

<ol>
  <li><strong>Mini-batch/Stochastic gradient descent</strong> is the first variant of gradient
descent that actually works decently well in deep learning. In our online setup,
it is reframed as <em>online gradient descent</em>, where the points $x_t$ are updated
by moving in the opposite direction of the gradient $\nabla f_t(x_t)$,
computed on the available mini-batch. The update size is determined by a
learning rate $\alpha_t$ (typically $\alpha/\sqrt{t}$ for some $\alpha$). <br />
We thus get the <strong>update rule of SGD:
$\ x_{t+1} = \Pi_\mathcal{F}(x_t - \alpha_t g_t)$</strong>.</li>
</ol>

<p>Although the convergence of SGD requires a decreasing learning rate, choosing
an adequate learning rate decrease schedule can be painful. Aggressive decays,
such as $\alpha/\sqrt{t}$, or small learning rates, yield very slow convergence
and mediocre performance. On the other hand, gentle decays and high learning
rates yield very unstable training, even divergence sometimes. To overcome these
issues, <strong>adaptive methods</strong> have been developed around the key idea that <strong>each
weight, that is each coordinate of $x_t$, should be updated using its own
learning rate</strong>, automatically computed based on the knowledge of past updates.
This way, parameters that are frequently updated take only small steps (to avoid
divergence), while parameters that are rarely updated take rather huge steps
(to speed up convergence). This is summed up in the generic adaptive framework
below:</p>

<p><img src="/assets/img/2018-08-19-adaptive-methods.png" alt="Adaptive methods" class="center-image" /></p>

<p>We call $\alpha_t$ the step size, $\alpha_t/\sqrt{V_t}$ the learning rate,
and restrict ourselves to diagonal variants $V_t = \text{diag}(v_t)$.
This framework essentially allows us to compute a different learning rate for
each weight, by rescaling the step size (common to all weights) with a function
of past gradients (of the loss function with respect to the considered weight,
thus <em>weight-specific</em>). Using this framework, we can follow the evolution of
adaptive methods over time:</p>

<ol>
  <li><strong>(SGD) Stochastic gradient descent</strong>
    <center>$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = \mathbb{I}$</center>
    <p>SGD is also an adaptive method, with a specific strategy of not adapting at
all: it forgets everything about the past of each weight, and relies only on
the current gradient to perform the update, with a step size
(and learning rate) $\alpha_t =\alpha/\sqrt{t}$, an aggressive decay.<br /><br /></p>
  </li>
  <li><strong>(AdaGrad) Adaptive gradient descent</strong>
    <center>$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}(\frac{1}{t} \sum_{i=1}^t g_i^2)}$</center>
    <p>With the same step size $\alpha_t = \alpha/\sqrt{t}$, this yields an adaptive
and much more reasonable learning rate of $\alpha/\sqrt{\sum_i g_{ij}}$ for
a given parameter index $j$. When the gradients ${g_{ij}}$ are sparse, i.e.
parameter $j$ is not frequently updated, this <strong>considerably speeds up
convergence</strong> (updates are still rare, but much bigger than with SGD).<br /><br /></p>
  </li>
  <li><strong>(RMSProp)</strong>
    <center>$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta)\sum_{i=1}^t \beta^{t-i} g_i^2)}$</center>
    <p>RMSProp first considered restricting the average of past gradients to
a fixed window, instead of the entire past, to avoid shrinking the learning
rates to virtually zero too quickly. In practice, this is implemented
by an exponentially moving average of past gradients.<br /><br /></p>
  </li>
  <li><strong>(Adam) Adaptive momentum estimation</strong>
    <center>$\phi_t(g_1, ..., g_t) = (1-\beta_1)\sum_{i=1}^t \beta_1^{t-i} g_i^2{\ }\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta_2)\sum_{i=1}^t \beta_2^{t-i} g_i^2)}$</center>
    <p>Adam adds to RMSProp the idea of momentum (see Ruder’s post) to further
speed up convergence. The momentum parameter $\beta_1 &gt; 0$ considerably
improves the performance of the algorithm, especially in deep learning.
Combining momentum with adaptive methods, Adam is very efficient, and
immensely popular.</p>
  </li>
</ol>

<p><br /><br /></p>

<h2 id="the-non-convergence-of-adam">The non-convergence of Adam</h2>

<p><br /><br /></p>

<h2 id="amsgrad-a-new-and-fixed-variant">AMSGrad, a new (and fixed) variant</h2>

<p><br /><br /></p>

<h2 id="does-it-matter">Does it matter?</h2>

<p><br /><br /></p>

<hr />

<p>This finding is both very important, and not too worrisome in practice. It is
very important that empirical observations of non-convergence are finally backed
up by theoretical findings and justifications, and that <em>someone checked the
maths</em>. However, this is not too worrisome, since Adam <em>does</em> nonetheless work
very well in practice in most applications, especially real-world ones.
Furthermore, the spotted issue can be easily patched, and the authors
propose a variant of Adam with a more robust convergence proof, with the same
performance level.</p>

<p><br /></p>

<hr />

<h2 id="abstract">Abstract</h2>

<p>ADAM has become completely dominant in deep learning over the past few years.
This stochastic optimization algorithm is a variant of SGD, in the same vein
as ADAGRAD, insofar that it adjusts the learning rate on a per-feature basis
based on past gradients. However, ADAM specifically uses an exponential moving
average of past gradients, limiting the reliance to the past few gradients.</p>

<p>The authors show that this sort-term memory makes the convergence proof of ADAM
flawed. Intuitively, informative gradients that happen rarely are killed out
too fast by the short-term memory. The authors provide a simple counterexample
where ADAM converges to the worst solution possible.</p>

<p>Although it is of high importance, this result doesn’t change the fact that
ADAM tends to work very well in deep learning, where data distribution
is heuristically quite smooth. It seems that the variant provided by the authors
(AMSGRAD) doesn’t translate into much better performance.</p>

<hr />

<h2 id="i---introduction">I - Introduction</h2>

<ul>
  <li>
    <p><strong>SGD (Stochastic Gradient Descent)</strong> : Update the parameters of a model by
a step in the direction of the negative gradient of the loss evaluated
on a <em>minibatch</em>.</p>
  </li>
  <li>
    <p>Due to the large amount of data involved, SGD is the dominant method to
train deep neural networks (DNN) today.</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p><strong>ADAGRAD (Adapative Gradient)</strong> : Variant of SGD that scales coordinates
of the gradient by square roots of some form of <em>averaging of past values</em>.</p>
  </li>
  <li>
    <p>SGD variants in ADAGRAD’s vein are popular and successful because they
<strong>adjust the learning rate on a per-feature basis</strong>.</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p>ADAGRAD’s performance falls in non-convex and dense settings: using <em>all</em> past
gradients makes the learning rate decay too quick, and early updates lose
too much impact.</p>
  </li>
  <li>
    <p><strong>ADAM (Adaptive Moment estimation)</strong> : ADAGRAD idea + mitigate the rapid
learning rate decay using an exponential moving average of squared past
gradients (limit reliance to only the <em>past few gradients</em>)</p>
  </li>
  <li>
    <p>This is also the idea of RMSPROP, ADADELTA, NADAM, …</p>
  </li>
  <li>
    <p>Empirically, <strong>ADAM fails to converge when important information happens
rarely, because it is forgotten too fast</strong>
(“<em>when some minibatches provide large
gradients but only quite rarely; although they are quite informative, their
influence dies out rather quickly due to exponential moving average</em>”)</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>In this paper :
    <ul>
      <li><strong>Counterexample of a convex optimization problem where ADAM
does not converge</strong></li>
      <li>Localization of the <strong>error in ADAM’s convergence proof</strong></li>
      <li>ADAM variants with <strong>long-term gradient memory</strong></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="ii---preliminaries">II - Preliminaries</h2>

<h3 id="optimization-setup">Optimization setup</h3>

<ul>
  <li>
    <p>Online optimization problem in the full information feedback setting
(minimize regret)</p>
  </li>
  <li>
    <p>At each time step $t$ :</p>
    <ul>
      <li>Pick a point $x_t \in \mathcal{F}$
<strong>(parameters of the model, e.g. weights)</strong></li>
      <li>Gain access to a loss function $f_t$
<strong>(loss of the model on the next minibatch)</strong></li>
      <li>Incur loss $f_t (x_t)$</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><strong>SGD</strong> : $x_{t+1} = \Pi_{\mathcal{F}} (x_t - \alpha_t g_t)$ with
$g_t = \nabla f_t(x_t)$
(gradient descent projected onto the set of feasible points)</li>
</ul>

<p><br /></p>

<h3 id="generic-adaptive-methods">Generic adaptive methods</h3>

<p><br />Access</p>

<!-- <center> -->

<p><img src="/assets/img/Gini_index_map.svg" alt="lsfkj" class="center-image" /></p>

<!-- ![Generic Adaptive Method Setup](pictures/01-generic_adaptive.png) -->

<center>

<b>Generic framework for adaptive algorithms</b>

</center>

<p><br /></p>

<ul>
  <li>Additional notes :
    <ul>
      <li>$\alpha_t$ step size</li>
      <li>$\alpha_t V_t^{-1/2}$ learning rate</li>
      <li>Restriction to $V_t = \text{diag}(v_t)$</li>
      <li>Decreasing step size is required for convergence</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="sgd">SGD</h4>

<center>

![SGD phi and psi](pictures/01-SGD.png)

**SGD - Specifications in the framework** </center>

<p><br /></p>

<ul>
  <li>Aggressive learning rate decay $\alpha / \sqrt{t}$</li>
</ul>

<p><br /></p>

<h4 id="adagrad">ADAGRAD</h4>

<center>

![ADAGRAD phi and psi](pictures/01-ADAGRAD.png)

**ADAGRAD - Specifications in the framework** </center>

<p><br /></p>

<ul>
  <li>Modest learning rate decay $\alpha / \sqrt{\sum_i g_{i,j}^2}$</li>
</ul>

<p><br /></p>

<h4 id="adam">ADAM</h4>

<center>

![ADAM phi and psi](pictures/01-ADAM.png)

**ADAM - Specifications in the framework (main formulation)** </center>

<center>

![ADAM moment formulation](pictures/01-ADAM_moment.png)

**ADAM - Moment update formulation (alternative formulation)** </center>
<p><br /></p>

<ul>
  <li>
    <p>ADAGRAD with momentum (exponential moving average)</p>
  </li>
  <li>
    <p>The momentum term with $\beta_1 &gt; 0$ significantly boosts the performance,
especially in deep learning.</p>
  </li>
</ul>

<hr />

<h2 id="iii---the-non-convergence-of-adam">III - The non-convergence of ADAM</h2>

<ul>
  <li><strong>The proof of convergence of ADAM (given in Kingma &amp; Ba, 2015) is wrong.</strong></li>
</ul>

<center>

![ADAM error](pictures/01-ADAM_error.png)

**This quantity should be semi-definite positive.** <br />
**It is for SGD and ADAGRAD,
but not for ADAM.** </center>
<p><br /></p>

<ul>
  <li>The error essentially lies in the fact that the learning rate does not
always decay.</li>
</ul>

<p><br /></p>

<center>

![ADAM counterexample](pictures/01-ADAM_counterexample.png)

**Simple counterexample where ADAM converges to the worst solution.** <br />
<strong> $\mathcal{F} = [-1,1]$, $\ \ C&gt;2$, $\ \ \beta_1=0$,
$\ \ \beta_2=1/(1+C^2)$ </strong>
</center>
<p><br /></p>

<ul>
  <li>
    <p><strong>There is an online convex optimization problem where ADAM fails.</strong></p>
  </li>
  <li>
    <p>One could think that adding a small constant $\epsilon$ to $v_t$ would solve
the problem. Actually, although it helps, <strong>for any $\epsilon &gt; 0$, there is
an online convex optimization problem where ADAM fails.</strong></p>
  </li>
  <li>
    <p>One could think that using a large $\beta_2$ would solve the problem.
Actually, although it helps, <strong>for any $\beta_1, \beta_2 \in [0,1)$ such that
$\beta_1 &lt; \sqrt{\beta_2}$, there exists an online convex optimization problem
where ADAM fails.</strong> (Note that this condition is typically satisfied with
suggested parameters)</p>
  </li>
  <li>
    <p>One could think that this problem is specific to online optimization problems,
and that stochastic optimization problems would not be affected by this issue.
Actually, although stochastic optimization is typically easier, <strong>for any
$\beta_1, \beta_2 \in [0,1)$ such that $\beta_1 &lt; \sqrt{\beta_2}$,
there exists an online convex optimization problem where ADAM fails.</strong></p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>This means one has to use “problem-dependent” update hyperparameters ;
in high-dimensional settings, this typically means using different
hyperparameters for each dimension, which defeats the purpose of adaptive
algorithms.</li>
</ul>

<hr />

<h2 id="iv---a-new-exponential-moving-average-variant--amsgrad">IV - A new exponential moving average variant : AMSGRAD</h2>

<center>

![AMSGRAD](pictures/01-AMSGRAD.png)

</center>

<p><br /></p>

<ul>
  <li>
    <p>The key difference between AMSGRAD and ADAM is that the former maintains
the maximum of all previously seen $v_t$, and uses it to normalize the running
average of the gradient instead of $v_t$ (ADAM).</p>
  </li>
  <li>
    <p>In other words, <strong>to ensure a non-increasing learning rate, AMSGRAD uses
the maximum value of the normalization term, instead of its current value.</strong></p>
  </li>
  <li>
    <p>Proof of convergence is provided, with a data-dependent ensured regret of
<strong>$O(\sqrt{T})$</strong>, which can be considerably <strong>better than $O(\sqrt{dT})$ of
SGD.</strong></p>
  </li>
</ul>

      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=ADAM's convergence proof is flawed&url=http://localhost:4000/ICLR/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/ICLR/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/ICLR/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#ICLR" class="tag">&#35; ICLR</a>
          
            <a href="/tags#ICLR 2018" class="tag">&#35; ICLR 2018</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(', '\\)'] ],
      displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</html>
