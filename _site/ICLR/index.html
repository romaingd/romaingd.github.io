<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>ADAM's convergence proof is flawed - Divergences</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Divergences" property="og:site_name">
  
    <meta content="ADAM's convergence proof is flawed" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="[ICLR 2018 - 1] "On the convergence of ADAM and beyond", Reddi et al., 2018
" property="og:description">
  
  
    <meta content="http://localhost:4000/ICLR/" property="og:url">
  
  
    <meta content="2018-08-19T01:00:00+02:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/2018-08-19-ICLR-thumbnail.png" property="og:image">
  
  
    
  
  
    
    <meta content="ICLR" property="article:tag">
    
    <meta content="ICLR 2018" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="ADAM's convergence proof is flawed">
  
  
    <meta name="twitter:url" content="http://localhost:4000/ICLR/">
  
  
    <meta name="twitter:description" content="[ICLR 2018 - 1] "On the convergence of ADAM and beyond", Reddi et al., 2018
">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/2018-08-19-ICLR-thumbnail.png">
  

	<meta name="description" content="[ICLR 2018 - 1] "On the convergence of ADAM and beyond", Reddi et al., 2018
">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/profile-picture.jpg" alt="Romain Girard"></a>
      </div>
      <div class="author-name">Romain Girard</div>
      <p>I currently work as a <strong> data scientist </strong> at Implicity, where I develop machine learning techniques to study strokes and heart failure. <br><br> I am strongly interested in both <strong>data science for healthcare</strong> and <strong>theoretical machine learning</strong>. <br><br> I graduated in 2018 from École polytechnique. <br><br> I hold a M.Sc. degree from the MVA <i>(Mathématiques, Vision, Apprentissage)</i> master at ENS Paris-Saclay, with an emphasis on machine learning and statistics.
</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        <!-- 
          <li><a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
         -->
        
          <li class="github"><a href="http://github.com/romaingd" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/romain-girard-4396ab11b/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:romain.girard+Divergences@polytechnique.edu"><i class="fa fa-envelope-o" aria-hidden="true"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2018 &copy; Romain Girard</p>
      <p>Flexible Jekyll theme by Artem Sheludko</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/2018-08-19-ICLR-thumbnail.png alt="ADAM's convergence proof is flawed">
        
          <figcaption>An example where ADAM fails to converge to the true solution</figcaption>
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">ADAM's convergence proof is flawed</h1>
        <div class="page-date"><span>2018, Aug 19&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p><br /></p>

<hr />

<h4 id="iclr-2018---1st-article">ICLR 2018 - 1st article</h4>

<p><em>In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Each oral paper is analyzed and
commented in an accessible way.</em></p>

<p><em>This article is based on the paper</em>
<a href="https://openreview.net/pdf?id=ryQu7f-RZ">On the convergence of ADAM and beyond</a>
<em>by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.</em></p>

<hr />

<p><br /></p>

<p>If you asked people in the deep learning community what their favorite optimization
algorithm is, you’d probably see Adam on top by a large margin, far ahead of
alternatives like Adagrad, RMSProp or SGD with Nesterov momentum (we’ll come to
all these names in a minute). <b>Adam has become completely dominant in deep
learning since it came out in December 2014</b>. The reasons are easy to
understand: it exhibits impressive performance, uses many important ideas of
previous works, and comes with predefined settings that already work very well
in most optimization problems.
<em>Better performance with less work, what more do you want?</em></p>

<p>Naturally, we would like to have some theoretical guarantees on the convergence
properties of the algorithm. This is a necessary sanity check, confirming by
the maths that we can legitimately trust the algorithm to be useful in all
situations covered. Well, it turns out that <strong>Adam does not pass this sanity
test (and that the original convergence proof is flawed)</strong>, as stated by this
theorem of the paper:</p>

<blockquote>
  <p><strong>Theorem 3.</strong> [With mild assumptions on the parameters], there is a
stochastic convex optimization problem for which Adam does not converge to the
optimal solution.</p>
</blockquote>

<p>This finding is both very important, and not too worrisome in practice. It is
very important that empirical observations of non-convergence are finally backed
up by theoretical findings and justifications, and that <em>someone checked the
maths</em>. However, this is not too worrisome, since Adam <em>does</em> nonetheless work
very well in practice in most applications, especially real-world ones.
Furthermore, the spotted issue can be easily patched up, and the authors
propose a variant of Adam with a more robust convergence proof, with the same
performance level.</p>

<p><br /></p>

<hr />

<h2 id="abstract">Abstract</h2>

<p>ADAM has become completely dominant in deep learning over the past few years.
This stochastic optimization algorithm is a variant of SGD, in the same vein
as ADAGRAD, insofar that it adjusts the learning rate on a per-feature basis
based on past gradients. However, ADAM specifically uses an exponential moving
average of past gradients, limiting the reliance to the past few gradients.</p>

<p>The authors show that this sort-term memory makes the convergence proof of ADAM
flawed. Intuitively, informative gradients that happen rarely are killed out
too fast by the short-term memory. The authors provide a simple counterexample
where ADAM converges to the worst solution possible.</p>

<p>Although it is of high importance, this result doesn’t change the fact that
ADAM tends to work very well in deep learning, where data distribution
is heuristically quite smooth. It seems that the variant provided by the authors
(AMSGRAD) doesn’t translate into much better performance.</p>

<hr />

<h2 id="i---introduction">I - Introduction</h2>

<ul>
  <li>
    <p><strong>SGD (Stochastic Gradient Descent)</strong> : Update the parameters of a model by
a step in the direction of the negative gradient of the loss evaluated
on a <em>minibatch</em>.</p>
  </li>
  <li>
    <p>Due to the large amount of data involved, SGD is the dominant method to
train deep neural networks (DNN) today.</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p><strong>ADAGRAD (Adapative Gradient)</strong> : Variant of SGD that scales coordinates
of the gradient by square roots of some form of <em>averaging of past values</em>.</p>
  </li>
  <li>
    <p>SGD variants in ADAGRAD’s vein are popular and successful because they
<strong>adjust the learning rate on a per-feature basis</strong>.</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p>ADAGRAD’s performance falls in non-convex and dense settings: using <em>all</em> past
gradients makes the learning rate decay too quick, and early updates lose
too much impact.</p>
  </li>
  <li>
    <p><strong>ADAM (Adaptive Moment estimation)</strong> : ADAGRAD idea + mitigate the rapid
learning rate decay using an exponential moving average of squared past
gradients (limit reliance to only the <em>past few gradients</em>)</p>
  </li>
  <li>
    <p>This is also the idea of RMSPROP, ADADELTA, NADAM, …</p>
  </li>
  <li>
    <p>Empirically, <strong>ADAM fails to converge when important information happens
rarely, because it is forgotten too fast</strong>
(“<em>when some minibatches provide large
gradients but only quite rarely; although they are quite informative, their
influence dies out rather quickly due to exponential moving average</em>”)</p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>In this paper :
    <ul>
      <li><strong>Counterexample of a convex optimization problem where ADAM
does not converge</strong></li>
      <li>Localization of the <strong>error in ADAM’s convergence proof</strong></li>
      <li>ADAM variants with <strong>long-term gradient memory</strong></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="ii---preliminaries">II - Preliminaries</h2>

<h3 id="optimization-setup">Optimization setup</h3>

<ul>
  <li>
    <p>Online optimization problem in the full information feedback setting
(minimize regret)</p>
  </li>
  <li>
    <p>At each time step $t$ :</p>
    <ul>
      <li>Pick a point $x_t \in \mathcal{F}$
<strong>(parameters of the model, e.g. weights)</strong></li>
      <li>Gain access to a loss function $f_t$
<strong>(loss of the model on the next minibatch)</strong></li>
      <li>Incur loss $f_t (x_t)$</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><strong>SGD</strong> : $x_{t+1} = \Pi_{\mathcal{F}} (x_t - \alpha_t g_t)$ with
$g_t = \nabla f_t(x_t)$
(gradient descent projected onto the set of feasible points)</li>
</ul>

<p><br /></p>

<h3 id="generic-adaptive-methods">Generic adaptive methods</h3>

<p><br />Access</p>

<!-- <center> -->

<p><img src="/assets/img/Gini_index_map.svg" alt="lsfkj" class="center-image" /></p>

<!-- ![Generic Adaptive Method Setup](pictures/01-generic_adaptive.png) -->

<center>

<b>Generic framework for adaptive algorithms</b>

</center>

<p><br /></p>

<ul>
  <li>Additional notes :
    <ul>
      <li>$\alpha_t$ step size</li>
      <li>$\alpha_t V_t^{-1/2}$ learning rate</li>
      <li>Restriction to $V_t = \text{diag}(v_t)$</li>
      <li>Decreasing step size is required for convergence</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="sgd">SGD</h4>

<center>

![SGD phi and psi](pictures/01-SGD.png)

**SGD - Specifications in the framework** </center>

<p><br /></p>

<ul>
  <li>Aggressive learning rate decay $\alpha / \sqrt{t}$</li>
</ul>

<p><br /></p>

<h4 id="adagrad">ADAGRAD</h4>

<center>

![ADAGRAD phi and psi](pictures/01-ADAGRAD.png)

**ADAGRAD - Specifications in the framework** </center>

<p><br /></p>

<ul>
  <li>Modest learning rate decay $\alpha / \sqrt{\sum_i g_{i,j}^2}$</li>
</ul>

<p><br /></p>

<h4 id="adam">ADAM</h4>

<center>

![ADAM phi and psi](pictures/01-ADAM.png)

**ADAM - Specifications in the framework (main formulation)** </center>

<center>

![ADAM moment formulation](pictures/01-ADAM_moment.png)

**ADAM - Moment update formulation (alternative formulation)** </center>
<p><br /></p>

<ul>
  <li>
    <p>ADAGRAD with momentum (exponential moving average)</p>
  </li>
  <li>
    <p>The momentum term with $\beta_1 &gt; 0$ significantly boosts the performance,
especially in deep learning.</p>
  </li>
</ul>

<hr />

<h2 id="iii---the-non-convergence-of-adam">III - The non-convergence of ADAM</h2>

<ul>
  <li><strong>The proof of convergence of ADAM (given in Kingma &amp; Ba, 2015) is wrong.</strong></li>
</ul>

<center>

![ADAM error](pictures/01-ADAM_error.png)

**This quantity should be semi-definite positive.** <br />
**It is for SGD and ADAGRAD,
but not for ADAM.** </center>
<p><br /></p>

<ul>
  <li>The error essentially lies in the fact that the learning rate does not
always decay.</li>
</ul>

<p><br /></p>

<center>

![ADAM counterexample](pictures/01-ADAM_counterexample.png)

**Simple counterexample where ADAM converges to the worst solution.** <br />
<strong> $\mathcal{F} = [-1,1]$, $\ \ C&gt;2$, $\ \ \beta_1=0$,
$\ \ \beta_2=1/(1+C^2)$ </strong>
</center>
<p><br /></p>

<ul>
  <li>
    <p><strong>There is an online convex optimization problem where ADAM fails.</strong></p>
  </li>
  <li>
    <p>One could think that adding a small constant $\epsilon$ to $v_t$ would solve
the problem. Actually, although it helps, <strong>for any $\epsilon &gt; 0$, there is
an online convex optimization problem where ADAM fails.</strong></p>
  </li>
  <li>
    <p>One could think that using a large $\beta_2$ would solve the problem.
Actually, although it helps, <strong>for any $\beta_1, \beta_2 \in [0,1)$ such that
$\beta_1 &lt; \sqrt{\beta_2}$, there exists an online convex optimization problem
where ADAM fails.</strong> (Note that this condition is typically satisfied with
suggested parameters)</p>
  </li>
  <li>
    <p>One could think that this problem is specific to online optimization problems,
and that stochastic optimization problems would not be affected by this issue.
Actually, although stochastic optimization is typically easier, <strong>for any
$\beta_1, \beta_2 \in [0,1)$ such that $\beta_1 &lt; \sqrt{\beta_2}$,
there exists an online convex optimization problem where ADAM fails.</strong></p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>This means one has to use “problem-dependent” update hyperparameters ;
in high-dimensional settings, this typically means using different
hyperparameters for each dimension, which defeats the purpose of adaptive
algorithms.</li>
</ul>

<hr />

<h2 id="iv---a-new-exponential-moving-average-variant--amsgrad">IV - A new exponential moving average variant : AMSGRAD</h2>

<center>

![AMSGRAD](pictures/01-AMSGRAD.png)

</center>

<p><br /></p>

<ul>
  <li>
    <p>The key difference between AMSGRAD and ADAM is that the former maintains
the maximum of all previously seen $v_t$, and uses it to normalize the running
average of the gradient instead of $v_t$ (ADAM).</p>
  </li>
  <li>
    <p>In other words, <strong>to ensure a non-increasing learning rate, AMSGRAD uses
the maximum value of the normalization term, instead of its current value.</strong></p>
  </li>
  <li>
    <p>Proof of convergence is provided, with a data-dependent ensured regret of
<strong>$O(\sqrt{T})$</strong>, which can be considerably <strong>better than $O(\sqrt{dT})$ of
SGD.</strong></p>
  </li>
</ul>

      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=ADAM's convergence proof is flawed&url=http://localhost:4000/ICLR/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/ICLR/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/ICLR/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#ICLR" class="tag">&#35; ICLR</a>
          
            <a href="/tags#ICLR 2018" class="tag">&#35; ICLR 2018</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(', '\\)'] ],
      displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</html>
