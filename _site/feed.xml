<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-16T13:46:01+02:00</updated><id>http://localhost:4000/</id><title type="html">Divergences</title><subtitle>Working in different directions
</subtitle><author><name>Romain Girard</name></author><entry><title type="html">Neural Machine Translation of the Cambridge meme</title><link href="http://localhost:4000/ICLR_2-nmt_cambridge_meme/" rel="alternate" type="text/html" title="Neural Machine Translation of the Cambridge meme" /><published>2018-09-10T01:00:00+02:00</published><updated>2018-09-10T01:00:00+02:00</updated><id>http://localhost:4000/ICLR_2-nmt_cambridge_meme</id><content type="html" xml:base="http://localhost:4000/ICLR_2-nmt_cambridge_meme/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iclr-2018---2nd-article&quot;&gt;ICLR 2018 - 2nd article&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Each oral paper is analyzed and
commented in an accessible way.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is based on the paper&lt;/em&gt;
&lt;a href=&quot;https://openreview.net/pdf?id=BJ8vJebC-&quot;&gt;Synthetic and natural noise both break Neural Machine Translation&lt;/a&gt;
&lt;em&gt;by Yonatan Belinkov and Yonatan Bisk.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you are reading these lines, there is a good chance that you have been
wandering the Internet for quite some time now. This means that there is also
a good chance that you’ve stumbled upon the &lt;em&gt;Cambridge research&lt;/em&gt; meme:&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht
oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and
lsat ltteer be at the rghit pclae.
&lt;/div&gt;

&lt;p&gt;There is even a greater chance that you have, for more or less legitimate
reasons, used Google Translate (or even better, its European equivalent, DeepL).
Of course, those automated translation systems have improved a lot in the past
few years; what was laughable at a couple years ago is now quite reasonable for
a number of languages. For simple texts, the translation even feels quite
natural, although not idiomatic (try translating “Il tombe des cordes.”, which
is the usual French equivalent of “It’s raining cats and dogs.”).&lt;/p&gt;

&lt;p&gt;Going back to the meme, you would probably have no trouble translating it to
whatever language you are familiar with. But how do automated translation
systems cope with this difficult situation? Let’s try translating a French
version into English:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Original &lt;br /&gt;French &lt;br /&gt; scrambled&lt;/td&gt;
      &lt;td&gt;Sleon un cehcruher de l’Uvinertisé de Cmabrigde, l’odrre des ltteers dnas un mot n’a pas d’ipmrotncae, la suele coshe ipmrotnate est que la pmeirère et la drenèire lteetrs sinoet à la bnnoe pclae.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Human translation&lt;/td&gt;
      &lt;td&gt;According to a researcher at Cambridge University, it doesn’t matter in what order the letters in a word are, the only important thing is that the first and last letters are in the right place.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Google Translate&lt;/td&gt;
      &lt;td&gt;According to an opinion of the Uvinised of Cmabrigde, the name of the ltteers in one word does not have a name, but the name of the message is that the letter and the letter are very brief at the same time.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepL&lt;/td&gt;
      &lt;td&gt;According to a cehcruher of the Uvinertized of Cmabrigde, the odd of the ltteers in a word has no ipmrotncae, the only coshe ipmrotnate is that the pmeirere and the drenere lteetrs sinoetrs in the bnnoe pclae.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This is quite unconvincing, to say the least. In other words, Google Translation
and DeepL are both very sensitive to this type of noise, whereas humans
seem impressively robust to it. This is just a symptom of the frailty of
automated translation systems:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While typos and noise are not new to NLP, our systems are rarely trained to
explicitly address them, as we instead hope that the relevant noise will occur
in the training data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It turns out that this kind of wishful thinking leaves &lt;strong&gt;Neural Machine
Translation (NMT) systems very
brittle&lt;/strong&gt;, with their performance dropping quickly in presence of noise. How can
we evaluate this lack of robustness to noise, and what can be done to improve
our models? Let’s jump into the paper.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evaluating-the-robustness-of-current-models&quot;&gt;Evaluating the robustness of current models&lt;/h2&gt;

&lt;p&gt;We would like to evaluate the performance of NMT models, and how it is affected
by noise. To do that we will, unsurprisingly, have to answer 3 questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Which performance metric?&lt;/li&gt;
  &lt;li&gt;Which NMT models?&lt;/li&gt;
  &lt;li&gt;Which kind of noise?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-performance-metric-bleu&quot;&gt;The performance metric: BLEU&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;&gt;BLEU&lt;/a&gt; (bilingual evaluation
understudy)&lt;/strong&gt; is a popular metric to evaluate the similarity between the
machine translation and a professional human translation. The idea behind it is
fairly simple: it is a modification of precision, computed on candidate
sentences (by comparison with reference sentences), and then averaged to produce
a corpus-level score of the translation’s quality.&lt;/p&gt;

&lt;p&gt;Let’s first remember why we don’t use precision as metric for this task.
Consider a sentence $s$ to be translated, with human reference translation $r$
and candidate translation $c$. Then precision is a score between 0 and 1
computed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{Precision}(c, r) =
  \dfrac{\sum_{w \in W(c)} m_w(c) \cdot \mathbb{1}_{w \in W(r)}}
    {\sum_{w \in W(c)} m_w(c)}&lt;/script&gt;

&lt;p&gt;with $W(c)$ the set of unique words that appear in $c$, and $m_w(c)$ the number
of occurences of $w$ in $c$. Precision essentially measures the proportion of
words of the candidate translation that actually appear in the reference
translation. To see why this is not sufficient, have a look at the table below.
Usually precision is considered coupled with recall to overcome such issues;
we don’t, because recall can be artificially inflated when considering multiple
references (see &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Reference&lt;/th&gt;
      &lt;th&gt;Candidate&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;BLEU (bigram)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;“I like trains a lot”&lt;/td&gt;
      &lt;td&gt;“I like”&lt;/td&gt;
      &lt;td&gt;$\dfrac{1+1}{2} = 1$&lt;/td&gt;
      &lt;td&gt;$\dfrac{1}{1} \cdot e^{1 - 2/5} = 0.55$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“I like trains a lot”&lt;/td&gt;
      &lt;td&gt;“a a a a a”&lt;/td&gt;
      &lt;td&gt;$\dfrac{5}{5} = 1$&lt;/td&gt;
      &lt;td&gt;$\dfrac{0}{4} \cdot 1 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“I like trains a lot”&lt;/td&gt;
      &lt;td&gt;“I like trains a a”&lt;/td&gt;
      &lt;td&gt;$\dfrac{5}{5} = 1$&lt;/td&gt;
      &lt;td&gt;$\dfrac{3}{4} \cdot 1 = 0.75$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“It is raining cats and dogs”&lt;/td&gt;
      &lt;td&gt;“It is pouring”&lt;/td&gt;
      &lt;td&gt;$\dfrac{2}{3} = 0.66$&lt;/td&gt;
      &lt;td&gt;$\dfrac{1}{2} \cdot e^{1 - 3/6} = 0.30$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To solve those issues, the BLEU score brings a couple of modifications to the
precision:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Consider the n-grams $G(c)$ instead of the words $W(c)$. &lt;br /&gt;This helps with
the fluency of the translation: with words only, “trains lot I a like” gets the
same score as “I like trains a lot”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replace $\ \ \textrm{ }m_w(c) \cdot 1_{w \in W(r)}\ \ \textrm{ }$ by $\ \
\textrm{ }\min (m_w(c), m_w(r)) \cdot {1_{w \in W(r)}}$. &lt;br /&gt;This basically
says that, in the second example, only the first “a” will be counted as correct,
since “a” appears only once in the reference.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiply by a factor $\ \ \min\left(1, \exp \left(1 - \frac{\textrm{length of reference
corpus}} {\textrm{length of candidate corpus}}\right)\right)$. &lt;br /&gt;Indeed, even with the
previous modifications, the constructed score still favors short translations
(see the first example). We therefore penalize translations that are shorter on
average than the reference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As illustrated in the last row of the table, even with these modifications the
final score is not ideal, since a perfectly natural candidate translation gets
a low score. It is, however, still much better than precision in a wide range
of situations. In the end, the BLEU score takes the following form
for our example sentences:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{BLEU}(c, r) =
  \dfrac{\sum_{g \in G(c)} \min (m_g(r), m_g(c)) \cdot \mathbb{1}_{g \in G(r)}}
    {\sum_{g \in G(c)} m_g(c)}
  \cdot \min\left(1, \exp \left({1 -
  \frac{\textrm{length}(r)}{\textrm{length}(c)}}\right)\right)&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-nmt-models-nematus-and-char2char&quot;&gt;The NMT models: &lt;strong&gt;Nematus&lt;/strong&gt; and &lt;strong&gt;char2char&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;This will be much quicker. We want to see how state-of-the-art models
with different architectures (and especially different accesses to words,
sub-word units or characters) are able to cope with noise. The authors ran their
experiments on three distinct models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.03017&quot;&gt;&lt;strong&gt;char2char&lt;/strong&gt;&lt;/a&gt; (Lee et al., 2017) - a
sequence-to-sequence model with attention, trained on characters to characters
(&lt;a href=&quot;https://github.com/nyu-dl/dl4mt-c2c&quot;&gt;implementation&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://aclweb.org/anthology/E17-3017&quot;&gt;&lt;strong&gt;Nematus&lt;/strong&gt;&lt;/a&gt; (Sennrich et al., 2017) -
a competition-winning sequence-to-sequence model with some architectural
modifications that enable operating on sub-word units
(&lt;a href=&quot;http://data.statmt.org/rsennrich/wmt16_systems/&quot;&gt;implementation&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.06615&quot;&gt;&lt;strong&gt;charCNN&lt;/strong&gt;&lt;/a&gt; (Kim et al., 2015) - an
attentional sequence-to-sequence model based on a character convolutional
neural network. To quote the authors, “this model retains the notion of a word
but learns a character-dependent representation of words”, and “performs well on
morphologically rich languages”
(&lt;a href=&quot;https://github.com/harvardnlp/seq2seq-attn&quot;&gt;implementation&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-noise-types-nat-key-swap-mid-rand&quot;&gt;The noise types: Nat, Key, Swap, Mid, Rand&lt;/h3&gt;

&lt;p&gt;We finally need to define a number of noise types that we will use to perturb
the models.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Example&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Original&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;weather&lt;/td&gt;
      &lt;td&gt;Original correct word&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Nat&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;whether&lt;/td&gt;
      &lt;td&gt;Natural noise, e.g. spelling mistake&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;qeather&lt;/td&gt;
      &lt;td&gt;Replace a letter with an adjacent key (QWERTY keyboard)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Swap&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;wetaher&lt;/td&gt;
      &lt;td&gt;Swap two letters except the first and last ones&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Mid&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;whaeter&lt;/td&gt;
      &lt;td&gt;Scramble the letters except the first and last ones&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Rand&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;raewhte&lt;/td&gt;
      &lt;td&gt;Scramble all letters&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;facing-the-truth---the-inability-to-translate-noisy-texts&quot;&gt;Facing the truth - the (in)ability to translate noisy texts&lt;/h3&gt;

&lt;p&gt;With all the tools in hand, it is now time to experiment and face the truth:
how well are our NMT models able to cope with the different noise types? I’m
definitely too lazy to run all the tests by myself, so I’ll steal the paper’s
results instead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-noise_results.png&quot; alt=&quot;Experiments with noise&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-noise_plots.png&quot; alt=&quot;Performance decrease&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whatever the language, noise has a striking effect on performance.
As a quick note, the results presented in the table aren’t comparable across
noise types, since the noise intensity is not the same in all cases. For a fair
comparison, have a look at the graphs (German to English): surprisingly enough,
the Rand noise isn’t much worse than Swap, despite yielding much more dramatic
changes of the tokens. It is actually &lt;em&gt;very&lt;/em&gt; surprising, since Swap is strictly
included in Rand. This would mean that the NMT model (especially Nematus)
is basically unable to recover a word with virtually any Swap mistake.
Yet, the authors’ conclusion still holds true:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The important thing to note is that even small amounts of noise lead to
substantial drops in performance. […] This is true for both natural noise and
all kinds of synthetic noise. […] The degradation in quality is especially
severe in light of the humans’ ability to understand noisy texts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-noise&quot;&gt;Dealing with noise&lt;/h2&gt;

&lt;p&gt;Now that we know where we are, what can be done to increase the robustness of
our models? The authors propose two natural ideas, and show their effect on
performance:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;All models presented rely on word structure to build a representation. This
structure is, however, altered to some extent by most of the studied noise
types (Swap, Mid, Rand). The idea is therefore to make some architectural
modifications in order to learn a representation of words that is invariant to
their structure.&lt;/li&gt;
  &lt;li&gt;Training on noisy examples has been regularly reported to increase model
robustness to noise. Let’s try that too.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;structure-invariant-representations&quot;&gt;Structure invariant representations&lt;/h3&gt;

&lt;p&gt;As is clear in their architecture, all three of the models under study are
&lt;em&gt;by design&lt;/em&gt; sensitive to word structure, at a character level (due to
convolutional layers or the sub-word units considered). There is a fair chance
that a model that is &lt;em&gt;insensitive&lt;/em&gt; to this structure would be more robust to
noises that affect this structure; nothing groundbreaking there.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perhaps the simplest such model is to take the average character embedding as
a word representation. This model, referred to as meanChar, first generates a
word representation by averaging character embeddings, and then proceeds
with a word-level encoder similar to the charCNN model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-meanchar_performance.png&quot; alt=&quot;meanChar performance&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt; Performance of meanChar in different settings. By design, Scr is the
same as Vanilla for this model. &lt;/center&gt;

&lt;p&gt;As is clear in the table, meanChar turns out to be pretty good for translating
scrambled text (much better than charCNN). Note that, by design, this model
sees no difference between vanilla and scrambled texts; the Scr performance
reported can therefore be compared to the vanilla performance of other models.
However, meanChar is still very sensitive to Nat and Key noise types, that do
not resemble Rand.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Personal note&lt;/strong&gt;:
this model seems particularly &lt;em&gt;blunt&lt;/em&gt;. While it is true that averaging
all character embeddings removes the reliance on word structure, it also
discards a huge part of the information brought by individual character. Unless
you are using an embedding dimension that is of the order of the size of your
alphabet, the
signal transmitted to the convolutional layer dismisses &lt;em&gt;some&lt;/em&gt; information
about the &lt;em&gt;presence&lt;/em&gt; of individual characters, in addition to throwing away
all information about &lt;em&gt;order&lt;/em&gt;.
In other words, not only anagrams will get the same representation, but also
some totally different words could by random chance.
One could argue that, with a good encoding
(e.g. simply using the powers of 2, which is basically cheating by pretending
that a full-dimensional one-hot vector can be reduced to a 1-dimensional scalar
value), we could always recover the presences; however I cannot think of one
that would suit the convolutional structure of the next layer.&lt;/p&gt;

&lt;p&gt;Have a look at Figures 2 and 3 below, depicting the charCNN vs. meanChar
word representations. I am pretty sure
that a better word representation can be found, discarding only the structure
while preserving the presence (instead of “jeter le bébé avec l’eau du bain”,
like we say in French). A first thought is a complete &lt;strong&gt;graph of individual
characters&lt;/strong&gt; (see Figure 4), with a graph-convolutional layer that would follow.
This seems to me like a more natural generalization of the previous models:
thinking in terms of graphs where nodes are the characters, previous
representations saw a word as a chain of characters, each of them linked only to
the next one by a directed edge. Discarding structure would simply mean linking
all nodes together by undirected edges.
This could even
be adapted to retain &lt;em&gt;some weak&lt;/em&gt; information about the structure by &lt;strong&gt;weighting
edges&lt;/strong&gt; using the original order (the closer two characters in the original
word, the higher the weight; see Figure 5).
While not completely structure-invariant, this has a potential to make the
model more robust to
structure change, while retaining all information about the presence of
individual characters and not forgetting all about structure like in meanChar.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_chain.png&quot; alt=&quot;Graph chain&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt; &lt;strong&gt;Figure 2&lt;/strong&gt; - `charCNN` word representation
as a chain of character embeddings. &lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_meanchar.png&quot; alt=&quot;Graph chain&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt; &lt;strong&gt;Figure 3&lt;/strong&gt; - `meanChar` word representation as an average
of character embeddings. &lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_complete.png&quot; alt=&quot;Graph complete&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt; &lt;strong&gt;Figure 4&lt;/strong&gt; - `graphChar` word
representation as a complete graph of character embeddings. &lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_weighted.png&quot; alt=&quot;Graph weighted&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt; &lt;strong&gt;Figure 5&lt;/strong&gt; - `w-graphChar` word
representation as a weighted graph of character embeddings. The weights, here
arbitrary, could reflect the structure to some extent. &lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you asked people in the deep learning community what their favorite optimization
algorithm is, you’d probably see &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;&gt;Adam&lt;/a&gt;
in the top 2 (with Stochastic gradient descent + momentum),
far ahead of alternatives like Adagrad or RMSProp (we’ll come to
these in a minute). &lt;b&gt;Adam has become very popular in deep
learning since it was proposed by Kingma &amp;amp; Ba in December 2014&lt;/b&gt;.
The reasons are easy to
understand: it exhibits impressive performance, uses many important ideas of
previous works, comes with predefined settings that already work very well,
and does not require careful hyper-parameter tuning.
&lt;em&gt;Better performance with less work, what more do you want?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Naturally, great power comes with great responsibilities, even for optimization
algorithms like Adam. At the very least, we expect some theoretical guarantees
on the convergence properties, to make sure that the algorithm is actually doing
its job when required. This is a necessary sanity check, confirming by
the maths that we can legitimately have faith in its correctness.&lt;/p&gt;

&lt;p&gt;The convergence proof was provided in the original Adam paper.
However, our authors show in their paper that &lt;strong&gt;this original proof is flawed,
and Adam does not correctly converge in all problems&lt;/strong&gt;, as illustrated by this
theorem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; [With mild assumptions on the parameters], there is a
stochastic convex optimization problem for which Adam does not converge to the
optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;How did we reach this point, where the most widely used
optimization algorithm in deep learning does not converge in some simple
convex problems? Let’s go back some years, and see what lead us there.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-and-adaptive-methods&quot;&gt;Gradient descent and adaptive methods&lt;/h2&gt;

&lt;p&gt;To facilitate analysis, let’s define our online optimization framework.
At each time step $t$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The algorithm picks a
point $x_t$ (weights of the model) in the feasible set $\mathcal{F}$.&lt;/li&gt;
  &lt;li&gt;We then
get access to the next mini-batch, with the associated loss function $f_t$,
and we incur the loss $f_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, our goal is to find an optimal
parameter $x$ such that the loss $f(x)$ on the entire training set is minimal
(intermediate loss functions $f_t$ are used as stochastic approximations of
$f$). Here, in the (equivalent) online setup, the goal
is to &lt;strong&gt;minimize the total regret at time $T$&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_T = \sum_{i=1}^T f_t(x_t) - \min_{x \in
\mathcal{F}} (\sum_{i=1}^T f_t(x))&lt;/script&gt;

&lt;p&gt;A large number of optimization algorithms were proposed to achieve this goal.
For a detailed and intuitive overview (and introduction to gradient descent),
I recommend this excellent
&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;blog post&lt;/a&gt; by Sebastian
Ruder. We will just quickly go over a couple major steps that lead us to Adam.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Mini-batch/Stochastic gradient descent&lt;/strong&gt; is the first variant of gradient
descent that actually works decently well in deep learning. In our online setup,
it is reframed as &lt;em&gt;online gradient descent&lt;/em&gt;, where the points $x_t$ are updated
by moving in the opposite direction of the gradient $g_t = \nabla f_t(x_t)$,
computed on the available mini-batch. The update size is determined by a
learning rate $\alpha_t$ (typically $\alpha/\sqrt{t}$ for some $\alpha$),
and the result is projected back to the feasible set thanks to the projection
operator $\Pi_{\mathcal{F}}$. &lt;br /&gt;
We thus get the &lt;strong&gt;update rule of SGD:
$\ x_{t+1} = \Pi_\mathcal{F}(x_t - \alpha_t g_t)$&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although the convergence of SGD requires a decreasing learning rate, choosing
an adequate learning rate decrease schedule can be painful. Aggressive decays,
such as $\alpha/\sqrt{t}$, or small learning rates, yield very slow convergence
and mediocre performance. On the other hand, gentle decays and high learning
rates yield very unstable training, even divergence sometimes. To overcome these
issues, &lt;strong&gt;adaptive methods&lt;/strong&gt; have been developed around the key idea that &lt;strong&gt;each
weight, that is each coordinate of $x_t$, should be updated using its own
learning rate&lt;/strong&gt;, automatically computed based on the knowledge of past updates.
This way, parameters that are frequently updated take only small steps (to avoid
divergence), while parameters that are rarely updated take rather huge steps
(to speed up convergence). This is summed up in the generic adaptive framework
below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-adaptive-methods.png&quot; alt=&quot;Adaptive methods&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We call $\alpha_t$ the step size, $\alpha_t/\sqrt{V_t}$ the learning rate,
and restrict ourselves to diagonal variants $V_t = \text{diag}(v_t)$.
This framework essentially allows us to compute a different learning rate for
each weight, by rescaling the step size (common to all weights) with a function
of past gradients (of the loss function with respect to the considered weight,
thus &lt;em&gt;weight-specific&lt;/em&gt;). Using this framework, we can follow the evolution of
adaptive methods over time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;SGD&lt;/a&gt;) Stochastic gradient descent&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = \mathbb{I}$&lt;/center&gt;
    &lt;p&gt;SGD is also an adaptive method, with a specific strategy of not adapting at
all: it forgets everything about the past of each weight, and relies only on
the current gradient to perform the update, with a step size
(and learning rate) $\alpha_t =\alpha/\sqrt{t}$, an aggressive decay.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;AdaGrad&lt;/a&gt;) Adaptive gradient descent&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}(\dfrac{1}{t} \sum_{i=1}^t g_i^2)}$&lt;/center&gt;
    &lt;p&gt;With the same step size $\alpha_t = \alpha/\sqrt{t}$, this yields an adaptive
and much more reasonable learning rate of $\alpha/\sqrt{\sum_i g_{ij}}$ for
a the $j$-th weight. When the gradients ${g_{ij}}$ are sparse, i.e.
the $j$-th weight is not frequently updated, this &lt;strong&gt;considerably speeds up
convergence&lt;/strong&gt; (updates are still rare, but much bigger than with SGD).&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://www.coursera.org/lecture/neural-networks/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude-YQHki&quot;&gt;RMSProp&lt;/a&gt;)&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta)\sum_{i=1}^t \beta^{t-i} g_i^2)}$&lt;/center&gt;
    &lt;p&gt;Despite its huge benefits in some settings, AdaGrad tends to shrink the
learning rates of frequently updated parameters, very quickly to virtually
zero. To overcome this issue, RMSProp restricts the average of past gradients
to a fixed window, instead of the entire past, to avoid shrinking the
learning rates to virtually zero too quickly. In practice, this is
implemented by an &lt;strong&gt;exponentially moving average of past gradients&lt;/strong&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;&gt;Adam&lt;/a&gt;) Adaptive momentum estimation&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = (1-\beta_1)\sum_{i=1}^t \beta_1^{t-i} g_i^2{\ }\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta_2)\sum_{i=1}^t \beta_2^{t-i} g_i^2)}$&lt;/center&gt;
    &lt;p&gt;To further speed up the convergence, Adam adds to RMSProp the idea of
&lt;strong&gt;momentum&lt;/strong&gt; (instead of moving in the direction
of the current gradient only, move in a direction that is a weighted average
of current gradient and previous update, like a rolling ball’s movement is
is influenced by the both the current slope and its momentum; see
&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;Ruder’s post&lt;/a&gt;).
The momentum parameter $\beta_1&amp;gt;0$ considerably
improves the performance of the algorithm, especially in deep learning.
Combining momentum with adaptive methods, Adam is very efficient, and
immensely popular.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-non-convergence-of-adam&quot;&gt;The non-convergence of Adam&lt;/h2&gt;

&lt;p&gt;So, what’s wrong with Adam? It turns out that in the convergence proof given
by Kingma &amp;amp; Ba, a flaw resides in the consideration of a specific quantity,
namely:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ \Gamma_{t+1} = \bigg(\dfrac{\sqrt{V_{t+1}}}{\alpha_{t+1}} -
\dfrac{\sqrt{V_{t}}}{\alpha_{t}}\bigg)&lt;/script&gt;

&lt;p&gt;Intuitively, $\Gamma_t$ measures the
variations of the inverse of the learning rate. As we saw, convergence requires
a &lt;strong&gt;non-increasing learning rate&lt;/strong&gt; (otherwise the algorithm oscillates too much,
or even diverges), which directly translates to $\Gamma_t \succeq O$ (easy to
see with a diagonal $V_t$). This is the case for both SGD ($v_t$ constant,
$\alpha_t$ decreasing) and AdaGrad ($v_t$ non-decreasing,
$\alpha_t$ decreasing). However, it is no longer the case for Adam (nor RMSProp)
because of the exponentially moving average of past gradients.&lt;/p&gt;

&lt;p&gt;This is actually a huge issue, meaning that in some cases Adam actually
converges to the worst solution possible. Following the example provided by the
authors, consider $C&amp;gt;2$ and $\mathcal{F} = [-1,1]$, and the following losses:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-ADAM_counterexample2.png&quot; alt=&quot;ADAM counterexample&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The optimal solution would be $x=-1$. However, choosing $\beta_1 = 0$ and
$\beta_2 = {1}/{(1+C^2)}$, Adam actually converges to the worst possible point
$x = +1$. The intuition for this behavior is the following: although the
algorithm observes a large gradient $C$ every $3$ steps,
&lt;strong&gt;it forgets (scales down) this large gradient too quickly (due to the
exponential weights)&lt;/strong&gt; to counterbalance the wrong but most frequent updates.&lt;/p&gt;

&lt;p&gt;So you thought Adam was reliable in all convex (“easy”) cases? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt; There is an online convex optimization problem where Adam has
non-zero average regret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wait, but this is for a specific choice of parameters. We allowed the choice of
a very small $\beta_2$, and to quote the paper, “this example also provides
intuition for why large $\beta_2$ is advisable […], and indeed in practice
using large $\beta_2$ helps”. So maybe we are safe when using correctly chosen
parameters? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt; For any constant $\beta_1,\beta_2 \in [0,1)$ such that $\beta_1
&amp;lt; \sqrt{\beta_2}$ [typically satisfied by default settings], there is an online
convex optimization problem where Adam has non-zero average regret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;All right, but this was in online optimization, where one could carefully design
a sequence of loss functions to specifically fool Adam. We should be fine with
stochastic optimization where such a design is impossible, right? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; For any constant $\beta_1,\beta_2 \in [0,1)$ such that $\beta_1
&amp;lt; \sqrt{\beta_2}$ [typically satisfied by default settings], there is
a stochastic convex optimization problem for which Adam does not converge to the
optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now this is actually a major concern, since stochastic convex optimization is
actually one of the simplest problems that we expect Adam to be able to solve
(and deep learning consists mostly of non-convex stochastic optimization,
which is reputed much harder). In practice, “fixing Adam” in cases of bad
convergence behavior would typically require using different hyper-parameters
for each dimension which, well, makes you wonder why you’re using adaptive
methods in the first place (especially given the very high number of dimensions
in typical deep learning applications).&lt;/p&gt;

&lt;p&gt;Another important note is that this analysis remains valid for &lt;strong&gt;any adaptive
method that is based on exponentially weighted moving averages (EWMA) of the
gradients&lt;/strong&gt;, including RMSProp, AdaDelta and NAdam, which are therefore also
flawed.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;amsgrad-a-new-and-fixed-variant&quot;&gt;AMSGrad, a new (and fixed) variant&lt;/h2&gt;

&lt;p&gt;Now that the maths have spoken and revealed the problem with Adam,
what can be done? EWMA-based algorithms
have actually brought a lot in terms of performance improvement and robustness
to hyper-parameters choice, and it would be quite a pity to simply throw it all
away for just a hole discovered in the proof.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-AMSGRAD.png&quot; alt=&quot;AMSGrad&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To overcome this issue, the authors propose a new variant, AMSGrad, described
in the algorithm above, that we explain in the next few lines.
Remember that the issue with Adam resides in $\Gamma_t$, which should be
semi-definite positive, while in some cases it is not because
$\frac{v_{t+1, j}}{\alpha_{t+1}} - \frac{v_{t, j}}{\alpha_{t}} &amp;lt; 0$ for some
index $j$. Well, a quick hotfix would be to replace $v_{t+1}$ by $\max(v_t,
v_{t+1})$, which would ensure $\Gamma_t \succeq 0$. But this would bias our
computations of the EWMA…
However we can decouple the two processes by keeping
two running variables: one “true” running variable $v_{t+1}$ used to accurately
compute the EWMA (correctly accounting for the past), and one “used-in-updates”
running variable ${\hat v_{t+1}} = {\max(v_{t+1}, \hat v_t)}$ used to ensure
a non-increasing learning rate (hence the correctness of the algorithm).&lt;/p&gt;

&lt;p&gt;Theoretical justifications of AMSGrad back up this claim: for reasonable choices
of parameters, the regret is bounded in $O(\sqrt{T})$. Additionally, for a
specific choice of momentum decay $\beta_{1t} = \beta_1 \lambda^{t-1}$, the
authors prove a bound on the regret that can hugely benefit from sparse
gradients, potentially “considerably better than $O(\sqrt{dT})$ regret of SGD”.&lt;/p&gt;

&lt;p&gt;Does this translate into good empirical performance? The paper provides the
convergence curves for a synthetic example that is close to the previous
example (with $\mathcal{F} = [-1, 1]$, the optimal solution being $x=-1$):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-synthetic_example.png&quot; alt=&quot;Synthetic example&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-convergence_curves.png&quot; alt=&quot;Convergence curves&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This example shows that AMSGrad works on the synthetic example where Adam fails
to converge to the true solution, which is always a good sign, since it was why
it was designed in the first place. But what about standard convergence
benchmarks on more interesting problems, like logistic regression or neural
networks? Reddi et al. provide graphics that claim for AMSGrad
equal or better performance compared to Adam for logistic regression,
feed-forward neural net (1 hidden layer) on MNIST, and CIFARNET. However, in an
independent
&lt;a href=&quot;https://fdlm.github.io/post/amsgrad/&quot;&gt;implementation by Filip Korzeniowski&lt;/a&gt;
tested in various settings (you should definitely check his post for extensive
details and comparisons), the experiments do not support any claim of
practical difference between Adam and AMSGrad. We show below the validation
accuracy (what we care about in the end) he gets for both algorithm on CIFAR-10.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-comparison.png&quot; alt=&quot;Comparison&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt;Validation accuracy on CIFAR-10 (VGG-16).
&lt;br /&gt;Adam in blue, AMSGrad in red.
&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;does-it-matter&quot;&gt;Does it matter?&lt;/h2&gt;

&lt;p&gt;In my opinion, it is pretty clear that the main interest of the paper lies
in pointing out the flaw in Adam’s proof, and how it affects the convergence
behavior in some cases. AMSGrad sounded somewhat more like a hotfix than a real
replacement, providing a sounder algorithm while maintaining Adam’s good
performance. AMSGrad will not change the course of stochastic optimization’s
(nor deep learning’s) history, may it even come to someday replace Adam
in practice. &lt;em&gt;In the end, it doesn’t even matter (that much)&lt;/em&gt; ; a true
“breakthrough” replacement is yet to come.&lt;/p&gt;

&lt;p&gt;What is more interesting is the meta-aspect of the proof-checking side of this
paper. It seems very important to me that empirical observations of bad
convergence behavior are finally backed up by theoretical justifications, in
other words, that &lt;strong&gt;someone finally checked the maths&lt;/strong&gt;. I find it actually
surprising that despite its “reasonable” length of 4 pages, no one checked the
proof and spotted this mistake, while in the meantime Adam got increasingly
popular and widely used. Note that I barely ever read the proofs myself. I would
tend to link this phenomenon to a deeper trend in the machine/deep learning
community: people (and scientists are people too) get really excited about the
results, the performance and the new opportunities brought by an exponentially
growing field, and we just forget how to do good science along the way. The
reproducibility crisis denounced by researchers like Joëlle Pineau
(especially in reinforcement learning) is one example; the fact that everyone
skips the proofs to go straight to the next exciting paper is another.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="ICLR 2018" /><category term="Neural Machine Translation" /><category term="Noise" /><category term="NLP" /><category term="ICLR" /><summary type="html"></summary></entry><entry><title type="html">The Gini coefficient is not the best inequality metric</title><link href="http://localhost:4000/gini-coefficient/" rel="alternate" type="text/html" title="The Gini coefficient is not the best inequality metric" /><published>2018-08-21T00:00:00+02:00</published><updated>2018-08-21T00:00:00+02:00</updated><id>http://localhost:4000/gini-coefficient</id><content type="html" xml:base="http://localhost:4000/gini-coefficient/">&lt;p&gt;Or is it?&lt;/p&gt;

&lt;p&gt;I’d like this thing not to be included&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Miscellaneous" /><category term="Econometrics" /><category term="Inequality" /><category term="Atkinson" /><summary type="html">Or is it?</summary></entry><entry><title type="html">Adam’s convergence proof is flawed</title><link href="http://localhost:4000/ICLR/" rel="alternate" type="text/html" title="Adam's convergence proof is flawed" /><published>2018-08-19T01:00:00+02:00</published><updated>2018-08-19T01:00:00+02:00</updated><id>http://localhost:4000/ICLR</id><content type="html" xml:base="http://localhost:4000/ICLR/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iclr-2018---1st-article&quot;&gt;ICLR 2018 - 1st article&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Each oral paper is analyzed and
commented in an accessible way.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is based on the paper&lt;/em&gt;
&lt;a href=&quot;https://openreview.net/pdf?id=ryQu7f-RZ&quot;&gt;On the convergence of Adam and beyond&lt;/a&gt;
&lt;em&gt;by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you asked people in the deep learning community what their favorite optimization
algorithm is, you’d probably see &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;
in the top 2 (with Stochastic gradient descent + momentum),
far ahead of alternatives like Adagrad or RMSProp (we’ll come to
these in a minute). &lt;b&gt;Adam has become very popular in deep
learning since it was proposed by Kingma &amp;amp; Ba in December 2014&lt;/b&gt;.
The reasons are easy to
understand: it exhibits impressive performance, uses many important ideas of
previous works, comes with predefined settings that already work very well,
and does not require careful hyper-parameter tuning.
&lt;em&gt;Better performance with less work, what more do you want?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Naturally, great power comes with great responsibilities, even for optimization
algorithms like Adam. At the very least, we expect some theoretical guarantees
on the convergence properties, to make sure that the algorithm is actually doing
its job when required. This is a necessary sanity check, confirming by
the maths that we can legitimately have faith in its correctness.&lt;/p&gt;

&lt;p&gt;The convergence proof was provided in the original Adam paper.
However, our authors show in their paper that &lt;strong&gt;this original proof is flawed,
and Adam does not correctly converge in all problems&lt;/strong&gt;, as illustrated by this
theorem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; [With mild assumptions on the parameters], there is a
stochastic convex optimization problem for which Adam does not converge to the
optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;How did we reach this point, where the most widely used
optimization algorithm in deep learning does not converge in some simple
convex problems? Let’s go back some years, and see what lead us there.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-and-adaptive-methods&quot;&gt;Gradient descent and adaptive methods&lt;/h2&gt;

&lt;p&gt;To facilitate analysis, let’s define our online optimization framework.
At each time step $t$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The algorithm picks a
point $x_t$ (weights of the model) in the feasible set $\mathcal{F}$.&lt;/li&gt;
  &lt;li&gt;We then
get access to the next mini-batch, with the associated loss function $f_t$,
and we incur the loss $f_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, our goal is to find an optimal
parameter $x$ such that the loss $f(x)$ on the entire training set is minimal
(intermediate loss functions $f_t$ are used as stochastic approximations of
$f$). Here, in the (equivalent) online setup, the goal
is to &lt;strong&gt;minimize the total regret at time $T$&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_T = \sum_{i=1}^T f_t(x_t) - \min_{x \in
\mathcal{F}} \left(\sum_{i=1}^T f_t(x)\right)&lt;/script&gt;

&lt;p&gt;A large number of optimization algorithms were proposed to achieve this goal.
For a detailed and intuitive overview (and introduction to gradient descent),
I recommend this excellent
&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;blog post&lt;/a&gt; by Sebastian
Ruder. We will just quickly go over a couple major steps that lead us to Adam.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Mini-batch/Stochastic gradient descent&lt;/strong&gt; is the first variant of gradient
descent that actually works decently well in deep learning. In our online setup,
it is reframed as &lt;em&gt;online gradient descent&lt;/em&gt;, where the points $x_t$ are updated
by moving in the opposite direction of the gradient $g_t = \nabla f_t(x_t)$,
computed on the available mini-batch. The update size is determined by a
learning rate $\alpha_t$ (typically $\alpha/\sqrt{t}$ for some $\alpha$),
and the result is projected back to the feasible set thanks to the projection
operator $\Pi_{\mathcal{F}}$. &lt;br /&gt;
We thus get the &lt;strong&gt;update rule of SGD:
$\ x_{t+1} = \Pi_\mathcal{F}(x_t - \alpha_t g_t)$&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although the convergence of SGD requires a decreasing learning rate, choosing
an adequate learning rate decrease schedule can be painful. Aggressive decays,
such as $\alpha/\sqrt{t}$, or small learning rates, yield very slow convergence
and mediocre performance. On the other hand, gentle decays and high learning
rates yield very unstable training, even divergence sometimes. To overcome these
issues, &lt;strong&gt;adaptive methods&lt;/strong&gt; have been developed around the key idea that &lt;strong&gt;each
weight, that is each coordinate of $x_t$, should be updated using its own
learning rate&lt;/strong&gt;, automatically computed based on the knowledge of past updates.
This way, parameters that are frequently updated take only small steps (to avoid
divergence), while parameters that are rarely updated take rather huge steps
(to speed up convergence). This is summed up in the generic adaptive framework
below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-adaptive-methods.png&quot; alt=&quot;Adaptive methods&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We call $\alpha_t$ the step size, $\alpha_t/\sqrt{V_t}$ the learning rate,
and restrict ourselves to diagonal variants $V_t = \text{diag}(v_t)$.
This framework essentially allows us to compute a different learning rate for
each weight, by rescaling the step size (common to all weights) with a function
of past gradients (of the loss function with respect to the considered weight,
thus &lt;em&gt;weight-specific&lt;/em&gt;). Using this framework, we can follow the evolution of
adaptive methods over time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;SGD&lt;/a&gt;) Stochastic gradient descent&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = \mathbb{I}$&lt;/center&gt;
    &lt;p&gt;SGD is also an adaptive method, with a specific strategy of not adapting at
all: it forgets everything about the past of each weight, and relies only on
the current gradient to perform the update, with a step size
(and learning rate) $\alpha_t =\alpha/\sqrt{t}$, an aggressive decay.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;AdaGrad&lt;/a&gt;) Adaptive gradient descent&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}(\dfrac{1}{t} \sum_{i=1}^t g_i^2)}$&lt;/center&gt;
    &lt;p&gt;With the same step size $\alpha_t = \alpha/\sqrt{t}$, this yields an adaptive
and much more reasonable learning rate of $\alpha/\sqrt{\sum_i g_{ij}}$ for
a the $j$-th weight. When the gradients ${g_{ij}}$ are sparse, i.e.
the $j$-th weight is not frequently updated, this &lt;strong&gt;considerably speeds up
convergence&lt;/strong&gt; (updates are still rare, but much bigger than with SGD).&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://www.coursera.org/lecture/neural-networks/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude-YQHki&quot;&gt;RMSProp&lt;/a&gt;)&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta)\sum_{i=1}^t \beta^{t-i} g_i^2)}$&lt;/center&gt;
    &lt;p&gt;Despite its huge benefits in some settings, AdaGrad tends to shrink the
learning rates of frequently updated parameters, very quickly to virtually
zero. To overcome this issue, RMSProp restricts the average of past gradients
to a fixed window, instead of the entire past, to avoid shrinking the
learning rates to virtually zero too quickly. In practice, this is
implemented by an &lt;strong&gt;exponentially moving average of past gradients&lt;/strong&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;&gt;Adam&lt;/a&gt;) Adaptive momentum estimation&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = (1-\beta_1)\sum_{i=1}^t \beta_1^{t-i} g_i^2{\ }\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta_2)\sum_{i=1}^t \beta_2^{t-i} g_i^2)}$&lt;/center&gt;
    &lt;p&gt;To further speed up the convergence, Adam adds to RMSProp the idea of
&lt;strong&gt;momentum&lt;/strong&gt; (instead of moving in the direction
of the current gradient only, move in a direction that is a weighted average
of current gradient and previous update, like a rolling ball’s movement is
is influenced by the both the current slope and its momentum; see
&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;Ruder’s post&lt;/a&gt;).
The momentum parameter $\beta_1&amp;gt;0$ considerably
improves the performance of the algorithm, especially in deep learning.
Combining momentum with adaptive methods, Adam is very efficient, and
immensely popular.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-non-convergence-of-adam&quot;&gt;The non-convergence of Adam&lt;/h2&gt;

&lt;p&gt;So, what’s wrong with Adam? It turns out that in the convergence proof given
by Kingma &amp;amp; Ba, a flaw resides in the consideration of a specific quantity,
namely:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ \Gamma_{t+1} = \left(\dfrac{\sqrt{V_{t+1}}}{\alpha_{t+1}} -
\dfrac{\sqrt{V_{t}}}{\alpha_{t}}\right)&lt;/script&gt;

&lt;p&gt;Intuitively, $\Gamma_t$ measures the
variations of the inverse of the learning rate. As we saw, convergence requires
a &lt;strong&gt;non-increasing learning rate&lt;/strong&gt; (otherwise the algorithm oscillates too much,
or even diverges), which directly translates to $\Gamma_t \succeq O$ (easy to
see with a diagonal $V_t$). This is the case for both SGD ($v_t$ constant,
$\alpha_t$ decreasing) and AdaGrad ($v_t$ non-decreasing,
$\alpha_t$ decreasing). However, it is no longer the case for Adam (nor RMSProp)
because of the exponentially moving average of past gradients.&lt;/p&gt;

&lt;p&gt;This is actually a huge issue, meaning that in some cases Adam actually
converges to the worst solution possible. Following the example provided by the
authors, consider $C&amp;gt;2$ and $\mathcal{F} = [-1,1]$, and the following losses:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-ADAM_counterexample2.png&quot; alt=&quot;ADAM counterexample&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The optimal solution would be $x=-1$. However, choosing $\beta_1 = 0$ and
$\beta_2 = {1}/{(1+C^2)}$, Adam actually converges to the worst possible point
$x = +1$. The intuition for this behavior is the following: although the
algorithm observes a large gradient $C$ every $3$ steps,
&lt;strong&gt;it forgets (scales down) this large gradient too quickly (due to the
exponential weights)&lt;/strong&gt; to counterbalance the wrong but most frequent updates.&lt;/p&gt;

&lt;p&gt;So you thought Adam was reliable in all convex (“easy”) cases? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt; There is an online convex optimization problem where Adam has
non-zero average regret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wait, but this is for a specific choice of parameters. We allowed the choice of
a very small $\beta_2$, and to quote the paper, “this example also provides
intuition for why large $\beta_2$ is advisable […], and indeed in practice
using large $\beta_2$ helps”. So maybe we are safe when using correctly chosen
parameters? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt; For any constant $\beta_1,\beta_2 \in [0,1)$ such that $\beta_1
&amp;lt; \sqrt{\beta_2}$ [typically satisfied by default settings], there is an online
convex optimization problem where Adam has non-zero average regret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;All right, but this was in online optimization, where one could carefully design
a sequence of loss functions to specifically fool Adam. We should be fine with
stochastic optimization where such a design is impossible, right? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; For any constant $\beta_1,\beta_2 \in [0,1)$ such that $\beta_1
&amp;lt; \sqrt{\beta_2}$ [typically satisfied by default settings], there is
a stochastic convex optimization problem for which Adam does not converge to the
optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now this is actually a major concern, since stochastic convex optimization is
actually one of the simplest problems that we expect Adam to be able to solve
(and deep learning consists mostly of non-convex stochastic optimization,
which is reputed much harder). In practice, “fixing Adam” in cases of bad
convergence behavior would typically require using different hyper-parameters
for each dimension which, well, makes you wonder why you’re using adaptive
methods in the first place (especially given the very high number of dimensions
in typical deep learning applications).&lt;/p&gt;

&lt;p&gt;Another important note is that this analysis remains valid for &lt;strong&gt;any adaptive
method that is based on exponentially weighted moving averages (EWMA) of the
gradients&lt;/strong&gt;, including RMSProp, AdaDelta and NAdam, which are therefore also
flawed.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;amsgrad-a-new-and-fixed-variant&quot;&gt;AMSGrad, a new (and fixed) variant&lt;/h2&gt;

&lt;p&gt;Now that the maths have spoken and revealed the problem with Adam,
what can be done? EWMA-based algorithms
have actually brought a lot in terms of performance improvement and robustness
to hyper-parameters choice, and it would be quite a pity to simply throw it all
away for just a hole discovered in the proof.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-AMSGRAD.png&quot; alt=&quot;AMSGrad&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To overcome this issue, the authors propose a new variant, AMSGrad, described
in the algorithm above, that we explain in the next few lines.
Remember that the issue with Adam resides in $\Gamma_t$, which should be
semi-definite positive, while in some cases it is not because
$\frac{v_{t+1, j}}{\alpha_{t+1}} - \frac{v_{t, j}}{\alpha_{t}} &amp;lt; 0$ for some
index $j$. Well, a quick hotfix would be to replace $v_{t+1}$ by $\max(v_t,
v_{t+1})$, which would ensure $\Gamma_t \succeq 0$. But this would bias our
computations of the EWMA…
However we can decouple the two processes by keeping
two running variables: one “true” running variable $v_{t+1}$ used to accurately
compute the EWMA (correctly accounting for the past), and one “used-in-updates”
running variable ${\hat v_{t+1}} = {\max(v_{t+1}, \hat v_t)}$ used to ensure
a non-increasing learning rate (hence the correctness of the algorithm).&lt;/p&gt;

&lt;p&gt;Theoretical justifications of AMSGrad back up this claim: for reasonable choices
of parameters, the regret is bounded in $O(\sqrt{T})$. Additionally, for a
specific choice of momentum decay $\beta_{1t} = \beta_1 \lambda^{t-1}$, the
authors prove a bound on the regret that can hugely benefit from sparse
gradients, potentially “considerably better than $O(\sqrt{dT})$ regret of SGD”.&lt;/p&gt;

&lt;p&gt;Does this translate into good empirical performance? The paper provides the
convergence curves for a synthetic example that is close to the previous
example (with $\mathcal{F} = [-1, 1]$, the optimal solution being $x=-1$):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-synthetic_example.png&quot; alt=&quot;Synthetic example&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-convergence_curves.png&quot; alt=&quot;Convergence curves&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This example shows that AMSGrad works on the synthetic example where Adam fails
to converge to the true solution, which is always a good sign, since it was why
it was designed in the first place. But what about standard convergence
benchmarks on more interesting problems, like logistic regression or neural
networks? Reddi et al. provide graphics that claim for AMSGrad
equal or better performance compared to Adam for logistic regression,
feed-forward neural net (1 hidden layer) on MNIST, and CIFARNET. However, in an
independent
&lt;a href=&quot;https://fdlm.github.io/post/amsgrad/&quot;&gt;implementation by Filip Korzeniowski&lt;/a&gt;
tested in various settings (you should definitely check his post for extensive
details and comparisons), the experiments do not support any claim of
practical difference between Adam and AMSGrad. We show below the validation
accuracy (what we care about in the end) he gets for both algorithm on CIFAR-10.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-comparison.png&quot; alt=&quot;Comparison&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt;Validation accuracy on CIFAR-10 (VGG-16).
&lt;br /&gt;Adam in blue, AMSGrad in red.
&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;does-it-matter&quot;&gt;Does it matter?&lt;/h2&gt;

&lt;p&gt;In my opinion, it is pretty clear that the main interest of the paper lies
in pointing out the flaw in Adam’s proof, and how it affects the convergence
behavior in some cases. AMSGrad sounded somewhat more like a hotfix than a real
replacement, providing a sounder algorithm while maintaining Adam’s good
performance. AMSGrad will not change the course of stochastic optimization’s
(nor deep learning’s) history, may it even come to someday replace Adam
in practice. &lt;em&gt;In the end, it doesn’t even matter (that much)&lt;/em&gt; ; a true
“breakthrough” replacement is yet to come.&lt;/p&gt;

&lt;p&gt;What is more interesting is the meta-aspect of the proof-checking side of this
paper. It seems very important to me that empirical observations of bad
convergence behavior are finally backed up by theoretical justifications, in
other words, that &lt;strong&gt;someone finally checked the maths&lt;/strong&gt;. I find it actually
surprising that despite its “reasonable” length of 4 pages, no one checked the
proof and spotted this mistake, while in the meantime Adam got increasingly
popular and widely used. Note that I barely ever read the proofs myself. I would
tend to link this phenomenon to a deeper trend in the machine/deep learning
community: people (and scientists are people too) get really excited about the
results, the performance and the new opportunities brought by an exponentially
growing field, and we just forget how to do good science along the way. The
reproducibility crisis denounced by researchers like Joëlle Pineau
(especially in reinforcement learning) is one example; the fact that everyone
skips the proofs to go straight to the next exciting paper is another.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="ICLR 2018" /><category term="Adam" /><category term="Optimization" /><category term="Deep Learning" /><category term="Neural Networks" /><category term="ICLR" /><summary type="html"></summary></entry><entry><title type="html">How I Rest From Work</title><link href="http://localhost:4000/how-i-rest-from-work/" rel="alternate" type="text/html" title="How I Rest From Work" /><published>2017-09-12T12:32:20+02:00</published><updated>2017-09-12T12:32:20+02:00</updated><id>http://localhost:4000/how-i-rest-from-work</id><content type="html" xml:base="http://localhost:4000/how-i-rest-from-work/">&lt;p&gt;Fam locavore snackwave bushwick +1 sartorial. Selfies portland knausgaard synth. Pop-up art party marfa deep v pitchfork subway tile 3 wolf moon. Ennui pinterest tumblr yr, adaptogen succulents copper mug twee. Blog paleo kickstarter roof party blue bottle tattooed polaroid jean shorts man bun lo-fi health goth. Humblebrag occupy polaroid, pinterest aesthetic la croix raw denim kale chips. 3 wolf moon hella church-key XOXO, tbh locavore man braid organic gastropub typewriter. Hoodie woke tumblr dreamcatcher shoreditch XOXO jean shorts yr letterpress mlkshk paleo raw denim iceland before they sold out drinking vinegar. Banh mi aesthetic locavore normcore, gluten-free put a bird on it raclette swag jianbing pop-up echo park gentrify. Stumptown brooklyn godard tumeric ethical. Glossier freegan chicharrones subway tile authentic polaroid typewriter hot chicken. Thundercats small batch heirloom meggings $\lambda^{-3}$.&lt;/p&gt;

&lt;p&gt;[
  x_i = \frac{5}{z}
]&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;markdown&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Redcarpet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello world!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;plaid-ramps-kitsch-woke-pork-belly&quot;&gt;Plaid ramps kitsch woke pork belly&lt;/h2&gt;
&lt;p&gt;90’s yr crucifix, selvage 8-bit listicle forage cliche shoreditch hammock microdosing synth. Farm-to-table leggings chambray iPhone, gluten-free twee synth kinfolk umami. Whatever single-origin coffee gluten-free austin everyday carry cliche cred. Plaid ramps kitsch woke pork belly organic. Trust fund whatever coloring book kombucha brooklyn. Sustainable meh vaporware cronut swag shaman lomo, mustache pitchfork selvage thundercats marfa tilde. Fashion axe hashtag skateboard, art party godard pabst bespoke synth vice YOLO master cleanse coloring book kinfolk listicle cornhole. Try-hard mixtape umami fanny pack man bun gastropub franzen tbh. Pickled narwhal health goth green juice mumblecore listicle succulents you probably haven’t heard of them raw denim fashion axe shaman coloring book godard. Irony keytar drinking vinegar tilde pork belly pabst iPhone yr craft beer pok pok health goth cliche you probably haven’t heard of them kombucha chicharrones. Direct trade hella roof party chia. Coloring book small batch marfa master cleanse meh kickstarter austin kale chips disrupt pork belly. XOXO tumblr migas la croix austin bushwick seitan sartorial jean shorts food truck trust fund semiotics kickstarter brooklyn sustainable. Umami knausgaard mixtape marfa. Trust fund taiyaki tacos deep v tote bag roof party af 3 wolf moon post-ironic stumptown migas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/we-in-rest.jpg&quot; alt=&quot;I and My friends&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Selfies sriracha taiyaki woke squid synth intelligentsia PBR&amp;amp;B ethical kickstarter art party neutra biodiesel scenester. Health goth kogi VHS fashion axe glossier disrupt, vegan quinoa. Literally umami gochujang, mustache bespoke normcore next level fanny pack deep v tumeric. Shaman vegan affogato chambray. Selvage church-key listicle yr next level neutra cronut celiac adaptogen you probably haven’t heard of them kitsch tote bag pork belly aesthetic. Succulents wolf stumptown art party poutine. Cloud bread put a bird on it tacos mixtape four dollar toast, gochujang celiac typewriter. Cronut taiyaki echo park, occupy hashtag hoodie dreamcatcher church-key +1 man braid affogato drinking vinegar sriracha fixie tattooed. Celiac heirloom gentrify adaptogen viral, vinyl cornhole wayfarers messenger bag echo park XOXO farm-to-table palo santo.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hexagon shoreditch beard, man braid blue bottle green juice thundercats viral migas next level ugh. Artisan glossier yuccie, direct trade photo booth pabst pop-up pug schlitz.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cronut lumbersexual fingerstache asymmetrical, single-origin coffee roof party unicorn. Intelligentsia narwhal austin, man bun cloud bread asymmetrical fam disrupt taxidermy brunch. Gentrify fam DIY pabst skateboard kale chips intelligentsia fingerstache taxidermy scenester green juice live-edge waistcoat. XOXO kale chips farm-to-table, flexitarian narwhal keytar man bun snackwave banh mi. Semiotics pickled taiyaki cliche cold-pressed. Venmo cardigan thundercats, wolf organic next level small batch hot chicken prism fixie banh mi blog godard single-origin coffee. Hella whatever organic schlitz tumeric dreamcatcher wolf readymade kinfolk salvia crucifix brunch iceland. Literally meditation four loko trust fund. Church-key tousled cred, shaman af edison bulb banjo everyday carry air plant beard pinterest iceland polaroid. Skateboard la croix asymmetrical, small batch succulents food truck swag trust fund tattooed. Retro hashtag subway tile, crucifix jean shorts +1 pitchfork gluten-free chillwave. Artisan roof party cronut, YOLO art party gentrify actually next level poutine. Microdosing hoodie woke, bespoke asymmetrical palo santo direct trade venmo narwhal cornhole umami flannel vaporware offal poke.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hexagon shoreditch beard&lt;/li&gt;
  &lt;li&gt;Intelligentsia narwhal austin&lt;/li&gt;
  &lt;li&gt;Literally meditation four&lt;/li&gt;
  &lt;li&gt;Microdosing hoodie woke&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Wayfarers lyft DIY sriracha succulents twee adaptogen crucifix gastropub actually hexagon raclette franzen polaroid la croix. Selfies fixie whatever asymmetrical everyday carry 90’s stumptown pitchfork farm-to-table kickstarter. Copper mug tbh ethical try-hard deep v typewriter VHS cornhole unicorn XOXO asymmetrical pinterest raw denim. Skateboard small batch man bun polaroid neutra. Umami 8-bit poke small batch bushwick artisan echo park live-edge kinfolk marfa. Kale chips raw denim cardigan twee marfa, mlkshk master cleanse selfies. Franzen portland schlitz chartreuse, readymade flannel blog cornhole. Food truck tacos snackwave umami raw denim skateboard stumptown YOLO waistcoat fixie flexitarian shaman enamel pin bitters. Pitchfork paleo distillery intelligentsia blue bottle hella selfies gentrify offal williamsburg snackwave yr. Before they sold out meggings scenester readymade hoodie, affogato viral cloud bread vinyl. Thundercats man bun sriracha, neutra swag knausgaard jean shorts. Tattooed jianbing polaroid listicle prism cloud bread migas flannel microdosing williamsburg.&lt;/p&gt;

&lt;p&gt;Echo park try-hard irony tbh vegan pok pok. Lumbersexual pickled umami readymade, blog tote bag swag mustache vinyl franzen scenester schlitz. Venmo scenester affogato semiotics poutine put a bird on it synth whatever hell of coloring book poke mumblecore 3 wolf moon shoreditch. Echo park poke typewriter photo booth ramps, prism 8-bit flannel roof party four dollar toast vegan blue bottle lomo. Vexillologist PBR&amp;amp;B post-ironic wolf artisan semiotics craft beer selfies. Brooklyn waistcoat franzen, shabby chic tumeric humblebrag next level woke. Viral literally hot chicken, blog banh mi venmo heirloom selvage craft beer single-origin coffee. Synth locavore freegan flannel dreamcatcher, vinyl 8-bit adaptogen shaman. Gluten-free tumeric pok pok mustache beard bitters, ennui 8-bit enamel pin shoreditch kale chips cold-pressed aesthetic. Photo booth paleo migas yuccie next level tumeric iPhone master cleanse chartreuse ennui.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Holidays" /><category term="Hawaii" /><summary type="html">Fam locavore snackwave bushwick +1 sartorial. Selfies portland knausgaard synth. Pop-up art party marfa deep v pitchfork subway tile 3 wolf moon. Ennui pinterest tumblr yr, adaptogen succulents copper mug twee. Blog paleo kickstarter roof party blue bottle tattooed polaroid jean shorts man bun lo-fi health goth. Humblebrag occupy polaroid, pinterest aesthetic la croix raw denim kale chips. 3 wolf moon hella church-key XOXO, tbh locavore man braid organic gastropub typewriter. Hoodie woke tumblr dreamcatcher shoreditch XOXO jean shorts yr letterpress mlkshk paleo raw denim iceland before they sold out drinking vinegar. Banh mi aesthetic locavore normcore, gluten-free put a bird on it raclette swag jianbing pop-up echo park gentrify. Stumptown brooklyn godard tumeric ethical. Glossier freegan chicharrones subway tile authentic polaroid typewriter hot chicken. Thundercats small batch heirloom meggings $\lambda^{-3}$.</summary></entry><entry><title type="html">The Best Organizer Software</title><link href="http://localhost:4000/the-best-organizer-software/" rel="alternate" type="text/html" title="The Best Organizer Software" /><published>2017-09-11T23:00:00+02:00</published><updated>2017-09-11T23:00:00+02:00</updated><id>http://localhost:4000/the-best-organizer-software</id><content type="html" xml:base="http://localhost:4000/the-best-organizer-software/">&lt;p&gt;Church-key blog messenger bag, selfies umami man braid mlkshk. Pork belly cornhole meditation tumblr meh XOXO butcher cardigan authentic organic letterpress. Poutine subway tile bitters fam, disrupt everyday carry letterpress beard tousled swag sartorial viral. Retro af 3 wolf moon heirloom, pork belly man bun DIY chillwave. Shoreditch ennui stumptown, photo booth tumeric PBR&amp;amp;B direct trade coloring book marfa taxidermy. Gentrify brunch typewriter woke freegan. Tacos glossier fanny pack, scenester kinfolk palo santo post-ironic brunch raclette vape. Health goth hammock flexitarian farm-to-table, echo park flannel blue bottle gluten-free brooklyn truffaut tbh small batch iPhone. DIY PBR&amp;amp;B four dollar toast tofu woke migas retro shoreditch disrupt yuccie YOLO vinyl man bun.&lt;/p&gt;

&lt;h3 id=&quot;church-key-blog-messenger-bag&quot;&gt;Church-key blog messenger bag&lt;/h3&gt;

&lt;p&gt;Tumblr bicycle rights intelligentsia, food truck migas raw denim whatever portland gastropub messenger bag chartreuse vape lomo coloring book subway tile. Yr pabst meggings tattooed four dollar toast. Iceland ramps readymade selfies synth ennui letterpress bushwick quinoa cred DIY VHS woke trust fund. Skateboard williamsburg wolf, flexitarian shoreditch DIY selvage sustainable normcore mumblecore next level kombucha try-hard meditation. Gentrify plaid microdosing, master cleanse ugh crucifix pop-up. Wolf bushwick street art tumeric. Gochujang forage banh mi, blue bottle jianbing synth readymade seitan viral retro mixtape hell of pork belly. Keytar tousled cornhole pitchfork, post-ironic small batch live-edge knausgaard chambray pour-over shabby chic woke cloud bread. Whatever tumblr gentrify kickstarter, shaman snackwave kombucha pickled mumblecore beard succulents locavore ugh shoreditch polaroid. Wayfarers crucifix tattooed twee. Yr listicle crucifix fingerstache farm-to-table. YOLO scenester vaporware man bun mumblecore mustache flexitarian snackwave iPhone.&lt;/p&gt;

&lt;p&gt;Hella lo-fi banjo, disrupt tofu prism raclette. Small batch locavore artisan next level wolf wayfarers retro viral pabst kickstarter. Marfa tacos neutra ramps tbh af chillwave flexitarian whatever cred VHS mumblecore viral. Hell of retro vegan chambray tacos VHS four dollar toast tote bag. Activated charcoal semiotics typewriter disrupt brunch selfies, yr hashtag selvage retro PBR&amp;amp;B bitters. Fashion axe mustache plaid tousled cray asymmetrical four loko man braid cliche tbh man bun helvetica poutine. Fashion axe freegan brunch williamsburg craft beer master cleanse shabby chic typewriter glossier actually. Plaid tumblr hexagon neutra slow-carb mumblecore. Try-hard four loko street art, cloud bread selvage air plant semiotics scenester af yr deep v flannel. Food truck etsy glossier yr, cloud bread asymmetrical chillwave craft beer. Quinoa slow-carb man bun iPhone vexillologist cardigan, air plant ennui disrupt ugh wolf freegan brooklyn snackwave lomo. Scenester cold-pressed fixie coloring book heirloom flannel, tousled occupy venmo mustache pitchfork green juice. VHS neutra 8-bit roof party. Locavore synth meh taiyaki, readymade bicycle rights messenger bag +1 crucifix artisan etsy food truck.&lt;/p&gt;

&lt;h3 id=&quot;pour-over-blue-bottle-woke-listicle&quot;&gt;Pour-over blue bottle woke listicle&lt;/h3&gt;

&lt;p&gt;Pour-over blue bottle woke listicle, pitchfork 90’s post-ironic scenester poutine ennui four loko ramps kickstarter. Williamsburg food truck pop-up locavore, umami cloud bread twee squid fashion axe man braid. Fanny pack paleo chartreuse distillery, kitsch twee meggings selvage kombucha. Keffiyeh actually prism listicle. Taxidermy authentic iPhone migas vegan copper mug. Post-ironic raw denim taiyaki cred hot chicken freegan, intelligentsia poke art party church-key PBR&amp;amp;B crucifix. Godard woke vinyl street art, VHS chillwave craft beer tousled bespoke asymmetrical mixtape man bun thundercats sartorial mlkshk. Meggings heirloom XOXO gentrify try-hard stumptown. Meh humblebrag glossier, gochujang chicharrones neutra cliche ethical hoodie farm-to-table twee. Messenger bag offal pug bespoke, put a bird on it tote bag literally.&lt;/p&gt;

&lt;p&gt;Everyday carry kinfolk shoreditch normcore try-hard etsy messenger bag venmo enamel pin. Try-hard fanny pack thundercats farm-to-table retro twee. Godard photo booth tofu 90’s. Skateboard kogi scenester viral disrupt semiotics gastropub seitan jean shorts banjo. Humblebrag knausgaard waistcoat mixtape. Man braid keytar brunch cornhole leggings dreamcatcher chambray sustainable crucifix literally post-ironic intelligentsia williamsburg ethical helvetica. Fixie disrupt PBR&amp;amp;B, unicorn food truck 8-bit leggings actually man bun twee mlkshk viral. Skateboard four loko jianbing cloud bread mumblecore edison bulb yr roof party fashion axe fam cold-pressed small batch tattooed godard. Bushwick yuccie thundercats schlitz listicle skateboard quinoa. Gentrify hot chicken pop-up keytar master cleanse pork belly. Irony pitchfork la croix neutra freegan. Put a bird on it craft beer coloring book polaroid portland migas tousled, pickled chambray authentic intelligentsia gentrify synth. Letterpress tumblr wolf normcore selvage. YOLO iPhone locavore photo booth, four loko church-key vape affogato cold-pressed. Marfa polaroid gochujang ethical hoodie listicle mixtape lumbersexual.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Productivity" /><category term="Software" /><summary type="html">Church-key blog messenger bag, selfies umami man braid mlkshk. Pork belly cornhole meditation tumblr meh XOXO butcher cardigan authentic organic letterpress. Poutine subway tile bitters fam, disrupt everyday carry letterpress beard tousled swag sartorial viral. Retro af 3 wolf moon heirloom, pork belly man bun DIY chillwave. Shoreditch ennui stumptown, photo booth tumeric PBR&amp;amp;B direct trade coloring book marfa taxidermy. Gentrify brunch typewriter woke freegan. Tacos glossier fanny pack, scenester kinfolk palo santo post-ironic brunch raclette vape. Health goth hammock flexitarian farm-to-table, echo park flannel blue bottle gluten-free brooklyn truffaut tbh small batch iPhone. DIY PBR&amp;amp;B four dollar toast tofu woke migas retro shoreditch disrupt yuccie YOLO vinyl man bun.</summary></entry><entry><title type="html">How To Start Programming</title><link href="http://localhost:4000/how-to-start-programming/" rel="alternate" type="text/html" title="How To Start Programming" /><published>2017-09-11T23:00:00+02:00</published><updated>2017-09-11T23:00:00+02:00</updated><id>http://localhost:4000/how-to-start-programming</id><content type="html" xml:base="http://localhost:4000/how-to-start-programming/">&lt;p&gt;Post-ironic jean shorts bushwick umami, synth beard austin hell of meh kitsch distillery sustainable plaid bitters. Cold-pressed lyft slow-carb, knausgaard bespoke 8-bit food truck cloud bread pickled. Taiyaki bitters trust fund heirloom craft beer single-origin coffee. Readymade fam vape blue bottle cold-pressed, flannel polaroid. Aesthetic four dollar toast semiotics af bicycle rights. Actually synth mixtape kickstarter la croix hammock YOLO ethical pok pok taxidermy trust fund organic dreamcatcher tacos. Franzen four loko man braid letterpress umami offal. Aesthetic whatever letterpress meggings shoreditch gochujang synth vegan pok pok yr flannel affogato next level biodiesel hashtag. Banjo vaporware lyft unicorn tumblr. Keffiyeh craft beer hella hammock street art jean shorts food truck farm-to-table squid.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tattooed pour-over taiyaki woke, skateboard subway tile PBR&amp;amp;B etsy distillery street art pok pok wolf 8-bit. Vegan bicycle rights schlitz subway tile unicorn taiyaki.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Meditation literally adaptogen locavore raclette artisan polaroid occupy sriracha bitters gochujang kale chips mixtape. Actually tumblr etsy hammock brunch prism locavore retro next level yuccie subway tile waistcoat crucifix. Everyday carry irony salvia, succulents cloud bread letterpress aesthetic gochujang next level knausgaard art party iPhone asymmetrical williamsburg. Iceland slow-carb knausgaard narwhal skateboard kitsch fashion axe. Man bun celiac street art, cliche PBR&amp;amp;B lomo blue bottle beard bitters. Mlkshk occupy offal dreamcatcher. Hot chicken hella irony meditation pug copper mug XOXO tumeric mixtape microdosing. Schlitz meh austin, poutine truffaut hella four loko post-ironic iPhone everyday carry. Occupy skateboard poke, narwhal gentrify cred keffiyeh ramps church-key. Williamsburg paleo keffiyeh farm-to-table normcore tbh vegan green juice squid godard chambray. DIY organic letterpress, venmo salvia crucifix gluten-free. Yr celiac tbh selfies activated charcoal.&lt;/p&gt;

&lt;p&gt;Adaptogen retro 8-bit mlkshk echo park hammock godard venmo flannel tilde umami enamel pin trust fund single-origin coffee etsy. Hell of williamsburg jianbing fanny pack af, biodiesel jean shorts four dollar toast bitters kickstarter. DIY edison bulb keffiyeh raclette. Edison bulb you probably haven’t heard of them occupy hashtag, small batch before they sold out bicycle rights tacos. IPhone selfies banh mi sartorial, typewriter seitan plaid. Fanny pack williamsburg gentrify plaid hoodie. Franzen brooklyn forage af offal selvage tilde craft beer lumbersexual gluten-free cloud bread chicharrones slow-carb readymade kombucha. Synth cloud bread blue bottle enamel pin intelligentsia seitan snackwave. Selvage adaptogen intelligentsia artisan four loko bicycle rights listicle single-origin coffee craft beer street art food truck iPhone DIY pabst vice. Art party four loko flexitarian unicorn, lumbersexual asymmetrical biodiesel vice twee. Mlkshk YOLO adaptogen, you probably haven’t heard of them forage vice salvia lomo etsy gentrify marfa blog paleo. Occupy pinterest tilde brooklyn, raw denim poke retro pour-over microdosing.&lt;/p&gt;

&lt;p&gt;Skateboard keytar actually disrupt taiyaki, synth biodiesel. Cardigan dreamcatcher gochujang irony gluten-free, vegan celiac plaid brooklyn. Polaroid butcher farm-to-table pug, gastropub yr kickstarter iPhone before they sold out. Marfa cornhole migas hashtag flannel fashion axe deep v kogi. Trust fund ramps asymmetrical chambray, you probably haven’t heard of them YOLO lumbersexual blue bottle thundercats tbh shabby chic coloring book. Kickstarter ugh try-hard four dollar toast master cleanse. Semiotics bespoke art party twee roof party cardigan. Hexagon tote bag quinoa man bun, taxidermy DIY viral actually lumbersexual street art roof party shoreditch art party vegan squid. Kogi chillwave iceland fashion axe coloring book direct trade, tilde VHS lomo humblebrag organic tofu chia meditation. Hella keytar shabby chic 90’s taxidermy tacos marfa. Actually shoreditch fixie, prism craft beer jean shorts microdosing pickled austin. Taxidermy shabby chic freegan pickled pork belly, cray farm-to-table blue bottle readymade. 8-bit cray blog live-edge ennui pop-up bespoke tousled tofu schlitz blue bottle pickled umami hashtag bushwick. Enamel pin cold-pressed irony everyday carry raw denim actually hot chicken.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Programming" /><category term="Learn" /><summary type="html">Post-ironic jean shorts bushwick umami, synth beard austin hell of meh kitsch distillery sustainable plaid bitters. Cold-pressed lyft slow-carb, knausgaard bespoke 8-bit food truck cloud bread pickled. Taiyaki bitters trust fund heirloom craft beer single-origin coffee. Readymade fam vape blue bottle cold-pressed, flannel polaroid. Aesthetic four dollar toast semiotics af bicycle rights. Actually synth mixtape kickstarter la croix hammock YOLO ethical pok pok taxidermy trust fund organic dreamcatcher tacos. Franzen four loko man braid letterpress umami offal. Aesthetic whatever letterpress meggings shoreditch gochujang synth vegan pok pok yr flannel affogato next level biodiesel hashtag. Banjo vaporware lyft unicorn tumblr. Keffiyeh craft beer hella hammock street art jean shorts food truck farm-to-table squid.</summary></entry><entry><title type="html">10 Tips To Improve Your Workflow</title><link href="http://localhost:4000/10-tips-to-improve-your-workflow/" rel="alternate" type="text/html" title="10 Tips To Improve Your Workflow" /><published>2017-09-11T23:00:00+02:00</published><updated>2017-09-11T23:00:00+02:00</updated><id>http://localhost:4000/10-tips-to-improve-your-workflow</id><content type="html" xml:base="http://localhost:4000/10-tips-to-improve-your-workflow/">&lt;p&gt;Asymmetrical portland enamel pin af heirloom ramps authentic thundercats. Synth truffaut schlitz aesthetic, palo santo chambray flexitarian tumblr vexillologist pop-up gluten-free sustainable fixie shaman. Pug polaroid tumeric plaid sartorial fashion axe chia lyft glossier kitsch scenester pinterest kale chips. Blog etsy umami fashion axe shoreditch. Prism chambray heirloom, drinking vinegar portland paleo slow-carb. Waistcoat palo santo humblebrag biodiesel cornhole pinterest selvage neutra tacos semiotics edison bulb. Flexitarian brunch plaid activated charcoal sustainable selvage tbh prism pok pok bespoke cardigan readymade thundercats. Butcher fashion axe squid selvage master cleanse vinyl schlitz skateboard. Lomo shaman man bun keffiyeh asymmetrical listicle. Kickstarter trust fund fanny pack post-ironic wayfarers swag kitsch. Shaman pug kale chips meh squid.&lt;/p&gt;

&lt;h3 id=&quot;literally-pickled-twee-man-braid&quot;&gt;Literally pickled twee man braid&lt;/h3&gt;
&lt;p&gt;8-bit ugh selfies, literally pickled twee man braid four dollar toast migas. Slow-carb mustache meggings pok pok. Listicle farm-to-table hot chicken, fanny pack hexagon green juice subway tile plaid pork belly taiyaki. Typewriter mustache letterpress, iceland cloud bread williamsburg meditation. Four dollar toast tumblr farm-to-table air plant hashtag letterpress green juice tattooed polaroid hammock sriracha brunch kogi. Thundercats swag pop-up vaporware irony selvage PBR&amp;amp;B 3 wolf moon asymmetrical cornhole venmo hexagon succulents. Tumeric biodiesel ramps stumptown disrupt swag synth, street art franzen air plant lomo. Everyday carry pinterest next level, williamsburg wayfarers pop-up gochujang distillery PBR&amp;amp;B woke bitters. Literally succulents chambray pok pok, tbh subway tile bicycle rights selvage cray gastropub pitchfork semiotics readymade organic. Vape flexitarian tumblr raclette organic direct trade. Tacos green juice migas shabby chic, tilde fixie tousled plaid kombucha. +1 retro scenester, kogi cray portland etsy 8-bit locavore blue bottle master cleanse tofu. PBR&amp;amp;B adaptogen chartreuse knausgaard palo santo intelligentsia.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mac.jpg&quot; alt=&quot;Macbook&quot; /&gt;
Man bun umami keytar 90’s lomo drinking vinegar synth everyday carry +1 bitters kinfolk raclette meggings street art heirloom. Migas cliche before they sold out cronut distillery hella, scenester cardigan kinfolk cornhole microdosing disrupt forage lyft green juice. Tofu deep v food truck live-edge edison bulb vice. Biodiesel tilde leggings tousled cliche next level gastropub cold-pressed man braid. Lyft humblebrag squid viral, vegan chicharrones vice kinfolk. Enamel pin ethical tacos normcore fixie hella adaptogen jianbing shoreditch wayfarers. Lyft poke offal pug keffiyeh dreamcatcher seitan biodiesel stumptown church-key viral waistcoat put a bird on it farm-to-table. Meggings pitchfork master cleanse pickled venmo. Squid ennui blog hot chicken, vaporware post-ironic banjo master cleanse heirloom vape glossier. Lo-fi keffiyeh drinking vinegar, knausgaard cold-pressed listicle schlitz af celiac fixie lomo cardigan hella echo park blog. Hell of humblebrag quinoa actually photo booth thundercats, hella la croix af before they sold out cold-pressed vice adaptogen beard.&lt;/p&gt;

&lt;h3 id=&quot;man-bun-umami-keytar&quot;&gt;Man bun umami keytar&lt;/h3&gt;
&lt;p&gt;Chia pork belly XOXO shoreditch, helvetica butcher kogi offal portland 3 wolf moon. Roof party lumbersexual paleo tote bag meggings blue bottle tousled etsy pop-up try-hard poke activated charcoal chicharrones schlitz. Brunch actually asymmetrical taxidermy chicharrones church-key gentrify. Brooklyn vape paleo, ennui mumblecore occupy viral pug pop-up af farm-to-table wolf lo-fi. Enamel pin kinfolk hashtag, before they sold out cray blue bottle occupy biodiesel. Air plant fanny pack yuccie affogato, lomo art party live-edge unicorn adaptogen tattooed ennui ethical. Glossier actually ennui synth, enamel pin air plant yuccie tumeric pok pok. Ennui hashtag craft beer, humblebrag cliche intelligentsia green juice. Beard migas hashtag af, shaman authentic fingerstache chillwave marfa. Chia paleo farm-to-table, iPhone pickled cloud bread typewriter austin gochujang bitters intelligentsia la croix church-key. Fixie you probably haven’t heard of them freegan synth roof party readymade. Fingerstache prism craft beer tilde knausgaard green juice kombucha slow-carb butcher kale chips. Snackwave organic tbh ennui XOXO. Hell of woke blue bottle, tofu roof party food truck pok pok thundercats. Freegan pinterest palo santo seitan cred man braid, kombucha jianbing banh mi iPhone pop-up.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Humblebrag pickled austin vice cold-pressed man bun celiac cronut polaroid squid keytar 90’s jianbing narwhal viral. Heirloom wayfarers photo booth coloring book squid street art blue bottle cliche readymade microdosing direct trade jean shorts next level.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Selvage messenger bag meh godard. Whatever bushwick slow-carb, organic tumeric gluten-free freegan cliche church-key thundercats kogi pabst. Hammock deep v everyday carry intelligentsia hell of helvetica. Occupy affogato pop-up bicycle rights paleo. Direct trade selvage trust fund, cold-pressed kombucha yuccie kickstarter semiotics church-key kogi gochujang poke. Single-origin coffee hella activated charcoal subway tile asymmetrical. Adaptogen normcore wayfarers pickled lomo. Ethical edison bulb shaman wayfarers cold-pressed woke. Helvetica selfies blue bottle deep v. Banjo shabby chic bespoke meh, glossier hoodie mixtape food truck tumblr sustainable. Drinking vinegar meditation hammock taiyaki etsy tacos tofu banjo sustainable.&lt;/p&gt;

&lt;p&gt;Farm-to-table bespoke edison bulb, vinyl hell of cred taiyaki squid biodiesel la croix leggings drinking vinegar hot chicken live-edge. Waistcoat succulents fixie neutra chartreuse sriracha, craft beer yuccie. Ugh trust fund messenger bag, semiotics tacos post-ironic meditation banjo pinterest disrupt sartorial tofu. Meh health goth art party retro skateboard, pug vaporware shaman. Meh whatever microdosing cornhole. Hella salvia pinterest four loko shabby chic yr. Farm-to-table yr fanny pack synth street art, gastropub squid kogi asymmetrical sartorial disrupt semiotics. Kombucha copper mug vice sriracha +1. Tacos hashtag PBR&amp;amp;B taiyaki franzen cornhole. Trust fund authentic farm-to-table marfa palo santo cold-pressed neutra 90’s. VHS artisan drinking vinegar readymade yr. Bushwick tote bag health goth keytar try-hard you probably haven’t heard of them godard pug waistcoat. Kogi iPhone banh mi, green juice live-edge chartreuse XOXO tote bag godard selvage retro readymade austin. Leggings ramps tacos iceland raw denim semiotics woke hell of lomo. Brooklyn woke adaptogen normcore pitchfork skateboard.&lt;/p&gt;

&lt;p&gt;Intelligentsia mixtape gastropub, mlkshk deep v plaid flexitarian vice. Succulents keytar craft beer shabby chic. Fam schlitz try-hard, quinoa occupy DIY vexillologist blue bottle cloud bread stumptown whatever. Sustainable cloud bread beard fanny pack vexillologist health goth. Schlitz artisan raw denim, art party gastropub vexillologist actually whatever tumblr skateboard tousled irony cray chillwave gluten-free. Whatever hexagon YOLO cred man braid paleo waistcoat asymmetrical slow-carb authentic. Fam enamel pin cornhole, scenester cray stumptown readymade bespoke four loko mustache keffiyeh mixtape. Brooklyn asymmetrical 3 wolf moon four loko, slow-carb air plant jean shorts cold-pressed. Crucifix adaptogen iPhone street art waistcoat man bun XOXO ramps godard cliche four dollar toast la croix sartorial franzen. Quinoa PBR&amp;amp;B keytar coloring book, salvia lo-fi sartorial chambray hella banh mi chillwave live-edge. Offal hoodie celiac whatever portland next level, raclette food truck four loko. Craft beer kale chips banjo humblebrag brunch ugh. Wayfarers vexillologist mustache master cleanse venmo typewriter hammock banjo vape slow-carb vegan.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Productivity" /><category term="Workflow" /><summary type="html">Asymmetrical portland enamel pin af heirloom ramps authentic thundercats. Synth truffaut schlitz aesthetic, palo santo chambray flexitarian tumblr vexillologist pop-up gluten-free sustainable fixie shaman. Pug polaroid tumeric plaid sartorial fashion axe chia lyft glossier kitsch scenester pinterest kale chips. Blog etsy umami fashion axe shoreditch. Prism chambray heirloom, drinking vinegar portland paleo slow-carb. Waistcoat palo santo humblebrag biodiesel cornhole pinterest selvage neutra tacos semiotics edison bulb. Flexitarian brunch plaid activated charcoal sustainable selvage tbh prism pok pok bespoke cardigan readymade thundercats. Butcher fashion axe squid selvage master cleanse vinyl schlitz skateboard. Lomo shaman man bun keffiyeh asymmetrical listicle. Kickstarter trust fund fanny pack post-ironic wayfarers swag kitsch. Shaman pug kale chips meh squid.</summary></entry><entry><title type="html">Conference on Javascript</title><link href="http://localhost:4000/conference-on-javascript/" rel="alternate" type="text/html" title="Conference on Javascript" /><published>2017-09-09T23:00:00+02:00</published><updated>2017-09-09T23:00:00+02:00</updated><id>http://localhost:4000/conference-on-javascript</id><content type="html" xml:base="http://localhost:4000/conference-on-javascript/">&lt;p&gt;Jean shorts organic cornhole, gochujang post-ironic chicharrones authentic flexitarian viral PBR&amp;amp;B forage wolf. Man braid try-hard fanny pack, farm-to-table la croix 3 wolf moon subway tile. Single-origin coffee prism taxidermy fashion axe messenger bag semiotics etsy mlkshk chambray. Marfa lumbersexual meditation celiac. Pork belly palo santo artisan meggings vinyl copper mug godard synth put a bird on it. Cloud bread pop-up quinoa, raw denim meditation 8-bit slow-carb. Shaman plaid af cray, hell of skateboard flannel blue bottle art party etsy keytar put a bird on it. Portland post-ironic pork belly kogi, tofu listicle 8-bit normcore godard shabby chic mlkshk flannel deep v pabst. Pork belly kinfolk fingerstache lo-fi raclette. Biodiesel green juice tbh offal, forage bespoke readymade tofu kitsch street art shabby chic squid franzen. Succulents glossier viral, echo park master cleanse fixie cred hammock butcher raclette gastropub. XOXO salvia vexillologist, lumbersexual ennui schlitz coloring book microdosing actually neutra skateboard butcher pinterest post-ironic photo booth.&lt;/p&gt;

&lt;p&gt;Four dollar toast blog austin artisan raw denim vinyl woke, salvia hella truffaut meh hexagon. Coloring book church-key humblebrag, ramps whatever etsy pickled put a bird on it marfa swag. Celiac live-edge bushwick, hexagon salvia pok pok neutra four dollar toast PBR&amp;amp;B chartreuse freegan readymade. Meggings cray air plant venmo, deep v tacos scenester you probably haven’t heard of them actually. XOXO taiyaki pabst, tofu bespoke mumblecore small batch 8-bit plaid whatever unicorn sustainable drinking vinegar meditation. Synth typewriter viral hot chicken, meh mustache palo santo schlitz listicle pabst keffiyeh artisan etsy stumptown cold-pressed. Occupy locavore cray irony. Chambray whatever vaporware keffiyeh heirloom vice. Single-origin coffee neutra iPhone lyft. Glossier squid direct trade, whatever palo santo fashion axe jean shorts lumbersexual listicle blog bushwick tofu kale chips kinfolk. Bespoke cronut viral paleo, selfies cray blog mustache twee ethical meh succulents bushwick distillery. Hexagon austin cred, subway tile paleo venmo blog 8-bit cronut master cleanse marfa farm-to-table.&lt;/p&gt;

&lt;p&gt;Live-edge vinyl meh, quinoa umami palo santo narwhal letterpress farm-to-table typewriter chartreuse vice tacos leggings. Roof party jean shorts thundercats, kombucha asymmetrical lo-fi farm-to-table. Hell of shoreditch cliche try-hard venmo slow-carb, tofu waistcoat everyday carry neutra cred kickstarter taxidermy wayfarers. Direct trade banh mi pug skateboard banjo edison bulb. Intelligentsia cliche quinoa synth umami. Trust fund four loko hoodie paleo cray tote bag slow-carb ennui. Williamsburg food truck intelligentsia trust fund. Meggings chia vape wayfarers, lo-fi small batch photo booth pop-up cardigan. Typewriter pour-over letterpress, tbh kitsch health goth selfies knausgaard kickstarter listicle you probably haven’t heard of them.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Js" /><category term="Conference" /><summary type="html">Jean shorts organic cornhole, gochujang post-ironic chicharrones authentic flexitarian viral PBR&amp;amp;B forage wolf. Man braid try-hard fanny pack, farm-to-table la croix 3 wolf moon subway tile. Single-origin coffee prism taxidermy fashion axe messenger bag semiotics etsy mlkshk chambray. Marfa lumbersexual meditation celiac. Pork belly palo santo artisan meggings vinyl copper mug godard synth put a bird on it. Cloud bread pop-up quinoa, raw denim meditation 8-bit slow-carb. Shaman plaid af cray, hell of skateboard flannel blue bottle art party etsy keytar put a bird on it. Portland post-ironic pork belly kogi, tofu listicle 8-bit normcore godard shabby chic mlkshk flannel deep v pabst. Pork belly kinfolk fingerstache lo-fi raclette. Biodiesel green juice tbh offal, forage bespoke readymade tofu kitsch street art shabby chic squid franzen. Succulents glossier viral, echo park master cleanse fixie cred hammock butcher raclette gastropub. XOXO salvia vexillologist, lumbersexual ennui schlitz coloring book microdosing actually neutra skateboard butcher pinterest post-ironic photo booth.</summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2017-04-06T12:32:20+02:00</published><updated>2017-04-06T12:32:20+02:00</updated><id>http://localhost:4000/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/welcome-to-jekyll/">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name>Romain Girard</name></author><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry></feed>