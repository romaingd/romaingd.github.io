<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-11-13T22:43:44+01:00</updated><id>http://localhost:4000/</id><title type="html">Divergences</title><subtitle>Working in different directions
</subtitle><author><name>Romain Girard</name></author><entry><title type="html">Religion grecque, singulier ou pluriel ?</title><link href="http://localhost:4000/CDF-Polytheisme_grec_mode_d_emploi/" rel="alternate" type="text/html" title="Religion grecque, singulier ou pluriel ?" /><published>2018-09-24T01:00:00+02:00</published><updated>2018-09-24T01:00:00+02:00</updated><id>http://localhost:4000/CDF-Polytheisme_grec_mode_d_emploi</id><content type="html" xml:base="http://localhost:4000/CDF-Polytheisme_grec_mode_d_emploi/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;collège-de-france---premier-cours&quot;&gt;Collège de France - Premier cours&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Dans cette série, je retranscris certains cours donnés au Collège de France à divers moments et sur divers sujets, sous la forme de notes de cours qui se veulent à la fois claires, concises et complètes, avec des redirections vers les approfondissements éventuels.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ces notes sont basées sur le cours&lt;/em&gt; &lt;a href=&quot;https://www.college-de-france.fr/site/vinciane-pirenne-delforge/course-2017-2018.htm&quot;&gt;Polythéisme grec, mode d’emploi&lt;/a&gt; &lt;em&gt;donné par Vinciane Pirenne-Delforge, titulaire de la chaire &lt;strong&gt;Religion, histoire et société dans le monde grec antique&lt;/strong&gt;, sur l’année 2017-2018.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction-et-enjeux&quot;&gt;Introduction et enjeux&lt;/h2&gt;

&lt;p&gt;Pour son premier cours au Collège de France, dans la traditionnelle chaire d’études hellénistiques, &lt;a href=&quot;http://web.philo.ulg.ac.be/thiasos/vinciane-pirenne-delforge-en/&quot;&gt;Vinciane Pirenne-Delforge&lt;/a&gt;, historienne des religions, s’emploie à développer un “mode d’emploi” du polythéisme grec. Avec sa multitude de dieux, ce monde reste en effet largement inconnu, difficilement accessible aux chercheurs occidentaux héritiers de deux millénaires de christianisme. Si, pendant longtemps, l’étude de ce monde fut dominée par l’analyse approfondie des grands textes littéraires comme ceux d’Homère ou d’Hésiode, Vinciane Pirenne-Delforge montre que ce point de vue est biaisé. Il ne permet en effet de n’accéder qu’à un niveau &lt;em&gt;général&lt;/em&gt; de compréhension, alors que &lt;strong&gt;l’épigraphie&lt;/strong&gt; (étude des inscriptions sur des supports durables), dont elle se fait l’avocate, permet une compréhension plus fine du polythéisme grec, dans sa dimension &lt;em&gt;locale&lt;/em&gt;. Elle rejoint en ce sens une longue tradition d’historiens de la Grèce antique enseignant au Collège de France, illustrée par son prédecesseur &lt;a href=&quot;https://fr.wikipedia.org/wiki/Denis_Knoepfler&quot;&gt;Denis Koepfler&lt;/a&gt; :&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[…] l’apport incessant des inscriptions grecques tient une place que le grand public, même cultivé, a quelque mal à mesurer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-pirenne-delforge.jpg&quot; alt=&quot;Vinciane Pirenne-Delforge&quot; class=&quot;center-image&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Vinciane Pirenne-Delforge au Collège de France (&lt;a href=&quot;http://www.communaute-hellenique.org/Events%202018/Vinciane%20PIRENNE-DELFORGE/Vinciane%20PIRENNE-DELFORGE.htm&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Le monde grec compte ainsi, au IV&lt;sup&gt;e&lt;/sup&gt; siècle avant notre ère, plus d’un millier de cités, et la religion grecque se décline dans chacune sous une &lt;strong&gt;forme propre&lt;/strong&gt;, dotée d’un panthéon et d’un calendrier spécifiques. Au coeur de ces contradictions, le travail du chercheur consiste justement à en comprendre &lt;strong&gt;l’articulation&lt;/strong&gt;, à interroger ces diverses représentations.&lt;/p&gt;

&lt;p&gt;La figure d’Aphrodite en fournit un exemple frappant. Réduite un peu rapidement dans l’imaginaire occidental à la déesse de &lt;em&gt;l’amour&lt;/em&gt;, Aphrodite est bien sûr très liée à la sexualité ; mais comment s’expliquer alors que les magistrats la remercient à l’issue de leur office ? Il faut pour cela invoquer la notion de &lt;em&gt;mélange&lt;/em&gt;, à laquelle cette divinité préside dans la culture grecque ; mélange certes des corps dans la sexualité, mais aussi mélange sur l’agora de la parole persuasive pour les magistrats, mélange au combat des corps qui s’entrechoquent. Entre ces domaines qui nous paraissent si &lt;strong&gt;hétérogènes&lt;/strong&gt;, les Grecs voyaient un &lt;strong&gt;lien&lt;/strong&gt;, tant et si bien que si on se contente de résumer Aphrodite à son étiquette facile de déesse de l’amour, &lt;em&gt;on ne comprend pas le polythéisme grec&lt;/em&gt;.&lt;/p&gt;

&lt;p style=&quot;margin-bottom: 30px&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-aphrodite_epistates.png&quot; alt=&quot;Dédicace d'épistates à Aphrodite&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Dédicace d’épistates (magistrats) à Aphrodite, Thasos. &lt;br /&gt;La première ligne indique Ἐπισταται Ἀφροδιτηι, les lignes suivantes les noms des magistrats. (&lt;a href=&quot;https://www.persee.fr/doc/bch_0007-4217_1958_num_82_1_2343&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Un dernier point nous permettra d’engager la réflexion sur nos sociétés contemporaines. &lt;strong&gt;Le polythéisme grec ne prétend pas à la vérité&lt;/strong&gt;, et d’ailleurs ne raisonne pas en terme de vérité et d’erreur, préférant voir dans les différents systèmes polythéistes une pluralité de points de vue. Les dieux du voisin, pourvu qu’ils soient nombreux, ne posent pas de problèmes, et peuvent même combler certaines lacunes dans le panthéon local. Dans notre époque marquée par des tensions religieuses qui restent fortes, cette approche ne peut manquer de susciter la réflexion.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Si l’Antiquité classique représente une part majeure de notre &lt;strong&gt;héritage culturel&lt;/strong&gt;, sa dimension religieuse polythéiste ne se laisse pas facilement appréhender par nos réflexes modernes, et nécessite une certaine distanciation. Les clefs du polythéisme grec ne se trouvent pas dans les statues ou les dictionnaires, et il nous faut nous méfier à la fois de notre fausse proximité ressentie à la religion, et de la tendance &lt;a href=&quot;https://www.universalis.fr/encyclopedie/apologetique/2-l-apologetique-chretienne-aux-premiers-siecles/&quot;&gt;apologétique chrétienne&lt;/a&gt; (centrée sur la défense méthodique du christianisme) qui a dominé les derniers siècles.&lt;/p&gt;

&lt;p&gt;Dans cette perspective, Vinciane Pirenne-Delforge emprunte un chemin qui suivra trois problématiques liées :&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Doit-on parler de religion grecque au singuler ou au pluriel ?&lt;/li&gt;
    &lt;li&gt;Les figures divines s'atomisent-elles dans la variété des lieux de culte, ou peut-on reconstituer un profil commun pour chacune d'elles ?&lt;/li&gt;
    &lt;li&gt;Les actes sacrificiels ne sont-ils fondés que sur des pratiques strictement locales, ou sont-elles liées par un fonds commun&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;le-mot-et-la-chose--religion&quot;&gt;Le mot et la chose : religion&lt;/h2&gt;

&lt;p&gt;Le terme de &lt;strong&gt;religion&lt;/strong&gt; est bien sûr culturellement très déterminé. Si l’acceptation moderne est relativement englobante, elle ne s’est cristallisée que vers la fin du XVI&lt;sup&gt;e&lt;/sup&gt; siècle en Europe occidentale, aussi il est utile d’en parcourir l’histoire. Des références pour approfondir le sujet sont disponibles &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL565076454571035790_Pirenne_Cours_1_2018_Fevr_1.pdf&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
&lt;i&gt;... religionem quae deorum cultu pio continentur, ...&lt;/i&gt;&lt;br /&gt;
... la religion qui revient à prendre soin des dieux de manière adéquate, ...&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Cicéron, &lt;i&gt;De natura deorum&lt;/i&gt;, I, 117&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Le terme latin de &lt;em&gt;religio&lt;/em&gt; renvoie à un &lt;strong&gt;accomplissement scrupuleux des différentes actions nécessaires aux bonnes relations avec les dieux&lt;/strong&gt;, comme en témoigne &lt;a href=&quot;https://fr.wikipedia.org/wiki/Cic%C3%A9ron&quot;&gt;Cicéron&lt;/a&gt; ci-dessus. Dans cette &lt;em&gt;religio&lt;/em&gt; comme &lt;em&gt;cultus deorum&lt;/em&gt;, les dieux font partie de la communauté à laquelle les hommes appartiennent, des cités, et on entre en relation avec eux de façon adéquate. Si Cicéron élargit un peu plus tard le sens du terme à certaines relations scrupuleuses strictement humaines, par exemple entre enfant et parents, cet usage reste exceptionnel. La &lt;em&gt;religio&lt;/em&gt;, en latin classique, regroupe ainsi à la fois &lt;strong&gt;une attitude et des pratiques scrupuleuses à l’égard des dieux&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Toutefois, les intellectuels chrétiens des premiers siècle de notre ère, écrivant également en latin, investissent le terme de &lt;em&gt;religio&lt;/em&gt; pour décrire &lt;em&gt;nostra religio&lt;/em&gt;, “notre religion”, infléchissant de manière nouvelle le sens du terme. Ainsi, prenant la défense de la foi chrétienne dans son &lt;em&gt;Apologétique&lt;/em&gt;, &lt;a href=&quot;https://fr.wikipedia.org/wiki/Tertullien&quot;&gt;Tertullien&lt;/a&gt; décrit au II&lt;sup&gt;e&lt;/sup&gt; la “vraie religion du vrai Dieu” :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Car s'il et certain que vos dieux n'existent pas, il est certain que votre religion n'existe pas non plus ; et s'il est certain que votre religion n'en est pas une, parce que vos dieux n'existent pas, il est certain aussi que nous ne sommes pas non plus coupables de lèse-religion. Mais au contraire, c'est sur vous que retombera le reproche que vous nous faites, sur vous qui adorez le mensonge et qui, non contents de négliger &lt;b&gt;la vraie religion du vrai Dieu (&lt;i&gt;veram religionem veri Dei&lt;/i&gt;)&lt;/b&gt;, allez jusqu'à la combattre, et qui vous rendez ainsi véritablemet coupables du crime d'une véritable irréligion (&lt;i&gt;crimen verae irreligiositatis&lt;/i&gt;).&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Tertullien, &lt;i&gt;Apologétique&lt;/i&gt;, XXIV, 1-2 (trad. J.-P. Walzing)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Cette qualification par l’épithète &lt;em&gt;verus&lt;/em&gt; de &lt;em&gt;religio&lt;/em&gt; &lt;strong&gt;n’est jamais attestée en latin classique (non chrétien)&lt;/strong&gt;, excepté dans un commentaire de Servius sur l’Énéide de Virgile ; ce commentaire ne fait toutefois pas l’opposition entre vraie et fausse religion, et peut s’expliquer par l’influence grandissante du latin chrétien sur le vocabulaire employé à cette époque, celui-ci reprenant à son compte l’opposition classique entre &lt;em&gt;superstitio&lt;/em&gt; (crainte excessive) et &lt;em&gt;religio&lt;/em&gt; pour qualifier les religions polythéistes de &lt;em&gt;supertsitio&lt;/em&gt;, et la religion chrétienne de &lt;em&gt;vera religio&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-augustin.jpg&quot; alt=&quot;Augustin&quot; class=&quot;center-image&quot; height=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;&lt;i&gt;Saint Augustin dans son cabinet de travail&lt;/i&gt;, Botticelli, ~1480 (&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Sandro_Botticelli_050.jpg?uselang=fr&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Ce &lt;strong&gt;transfert de mots&lt;/strong&gt; est très révélateur des changements religieux en arrière-plan. C’est ainsi qu’&lt;a href=&quot;https://fr.wikipedia.org/wiki/Augustin_d%27Hippone&quot;&gt;Augustin d’Hippone&lt;/a&gt; (ou Saint Augustin), évèque à l’époque de Servius, reprend la notion de “cité” (&lt;em&gt;civitas&lt;/em&gt;, également “droit de cité”) de manière subversive : ce n’est plus la &lt;em&gt;civitas&lt;/em&gt; qui est le lien de la communauté et le fondement de la relation aux dieux, mais c’est le lien de piété personnelle entre les chrétiens et Dieu qui constitue ce lien, désinvestissant la &lt;em&gt;civitas&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Augustin sent bien toutefois les limites des mots existants, et s’oblige à forger des usages qui conviennent mieux à ses idées. Dans sa &lt;em&gt;Cité de Dieu&lt;/em&gt; par exemple, il montre que l’acceptation polythéiste traditionnelle de la &lt;em&gt;religio&lt;/em&gt; en tant que “respect que nous avons de ce qui rapproche les hommes” ne correspond que mal à la conception chrétienne d’une transcendance verticale. L’influence chrétienne dessine ainsi une &lt;strong&gt;distinction entre la sphère religieuse et la sphère sociale&lt;/strong&gt;, distinction qui n’est pas opérée par les polythéismes grec et romain. Les chrétiens hellénophones, quant à eux, s’emparent du mot θρησκεία pour décrire leur conception de la religion (proche de la nôtre) ; très peu usité dans la littérature polythéiste, il ne se multiplie que sous la plume des auteurs juifs et chrétiens.&lt;/p&gt;

&lt;p&gt;À partir du XVI&lt;sup&gt;e&lt;/sup&gt; siècle, la compréhension moderne du terme de religion, comprenant diverses composantes comme la vénération, se cristallise. Au XVIII&lt;sup&gt;e&lt;/sup&gt; siècle, la perspective apologétique s’éloigne et la question se pose d’une &lt;strong&gt;religion naturelle&lt;/strong&gt;, fonds commun à tous, et dont les religions existantes seraient toutes des actualisations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Doit-on alors renoncer au terme de &lt;em&gt;religion&lt;/em&gt; ?&lt;/strong&gt; Le déterminisme culturel attaché à ce terme, le poids de l’héritage empêcheraient les chercheurs occidentaux modernes d’étudier d’un oeil neutre les différents systèmes religieux du monde ; il faudrait supprimer ce terme de la boîte à outils conceptuels d’analyse. Cependant, malgré la nécessité d’un concept pour l’analyse, il n’existe pas d’autre option viable permettant d’éviter à la fois le jargon (porteur de ses propres déterminismes), les anachronismes et les termes &lt;em&gt;ad hoc&lt;/em&gt; (natifs, empêchant la comparaison).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[Nous choisissons alors de maintenir] le terme de religion à titre de &lt;strong&gt;concept opératoire&lt;/strong&gt;, qui est à repenser et à affiner sans cesse dans les contextes dans lesquels on l’applique.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vinciane Pirenne-Delforge propose alors de retenir, pour l’analyse, une définition de la religion modifiant légèrement celle donnée par Spiro en 1966 :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;&lt;b&gt;La religion est une institution qui régit, selon des modèles culturels, les relations avec la sphère supra-humaine dont cette culture postule l'existence.&lt;/b&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;le-mot-et-la-chose--polythéisme&quot;&gt;Le mot et la chose : polythéisme&lt;/h2&gt;

&lt;p&gt;Parcourons désormais l’histoire et les usages du terme de polythéisme, pour comparer à notre analyse précédente ; des références sont disponibles &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL971292917039606906_Pirenne_Cours_2_2018_Fevr_8.pdf&quot;&gt;ici&lt;/a&gt;. Là où “religion” s’enracine dans la langue latine, “polythéisme” s’enracine dans la langue grecque, renvoyant à la multiplicité (πολυς) des dieux (θεος). Toutefois, &lt;strong&gt;on ne trouve pas en grec ancien (polythéiste) le terme πολυθεος&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;centeredquote&quot;&gt;
ὁρῶ τρίαιναν τήνδε, &lt;b&gt;σημεῖον&lt;/b&gt; θεοῦ&lt;br /&gt;
Je vois ce trident, signe/symbole du dieu&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;

πάντων δ ̓ ἀνάκτων τῶνδε &lt;b&gt;κοινοβωμίαν&lt;/b&gt; | σέβεσθε&lt;br /&gt;
Honorez la communauté d’autels / l’autel commun de tous ces seigneurs&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;

... ἐν θεῶν &lt;b&gt;ἕδραισιν&lt;/b&gt; ὧδ ̓ &lt;b&gt;ἱδρυμένας&lt;/b&gt; | ἐκδόντες ὑμᾶς ... &lt;br /&gt;
... vous livrant ainsi installées aux sièges des dieux ...&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;

μηδ ̓ ἴδῃς μ ̓ ἐξἑδρᾶν| &lt;b&gt;πολυθέων&lt;/b&gt; ῥυσιασθεῖσαν&lt;br /&gt;
Ne me laisse pas arracher aux sièges de ces nombreux dieux&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;

τὰν ἱκέτιν εἰσιδεῖν | ἀπὸ &lt;b&gt;βρετέων&lt;/b&gt; βίᾳ&lt;br /&gt;
arracher de force la suppliante aux statues
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Eschyle, &lt;i&gt;Suppliantes&lt;/i&gt;, v. 218, 222-223, 423-414, 423-425, 428-429&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;La seule attestation avant notre ère de termes similaires se trouve dans les &lt;em&gt;Suppliantes&lt;/em&gt; d’&lt;a href=&quot;https://fr.wikipedia.org/wiki/Eschyle&quot;&gt;Eschyle&lt;/a&gt;, dont des extraits se trouvent ci-dessus. Relatant l’histoire des &lt;a href=&quot;https://fr.wikipedia.org/wiki/Dana%C3%AFdes&quot;&gt;Danaïdes&lt;/a&gt; qui, pour échapper à un mariage avec leurs cousins, se réfugient dans un sanctuaire à l’entrée d’Argos et refusent d’en être arrachées par les humains, Eschyle déploie un vocabulaire caractéristique du polythéisme grec :&lt;/p&gt;
&lt;ul style=&quot;margin-top: -10px&quot;&gt;
    &lt;li&gt;&lt;b&gt;σημεῖον&lt;/b&gt; - plus qu'un signe, ce terme renvoie à un symbole spécifique, caractéristique distincte d'un dieu&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;κοινοβωμία&lt;/b&gt; - autel commun ou communauté d'autels, ce terme renvoie à une combinaison spécifique de dieux dans un même santuaire, sorte de panthéon réduit&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;ἕδραισιν&lt;/b&gt; ὧδ ̓ &lt;b&gt;ἱδρυμένας&lt;/b&gt; - ces termes de la même famille d'ἱδρυειν (fondation, installation), montrent que les dieux sont installés parmi les hommes&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;βρετας&lt;/b&gt; - les sièges des dieux sont leurs effigies, leurs statues&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Le polythéisme étant largement majoritaire dans le bassin méditerranéen, il va pendant longtemps “de soi” et n’est pas explicité. Ce n’est qu’avec l’arrivée d’&lt;strong&gt;auteurs judéo-chrétiens&lt;/strong&gt;, exprimant une franche hostilité pour les autres religions (contrastant avec la curiosité polythéiste), que la distinction apparaît. Dans la &lt;a href=&quot;https://fr.wikipedia.org/wiki/Septante&quot;&gt;Septante&lt;/a&gt; (traduction grecque de la Bible hébraïque), on ne trouve pas encore le terme πολυθεος, mais bien souvent les mots εἴδωλον et εἰδωλολατρία (idole, idolâtrie), renvoyant à l’illusion de la vénération des objets faits de main d’homme, et des dieux ainsi représentés. Moins que sur la pluralité des dieux, c’est sur l’illusion, l’erreur des polythéistes que l’εἰδωλολατρία met l’accent.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-philon.jpg&quot; alt=&quot;Philon&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Philon d’Alexandrie (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Philon_d%27Alexandrie#/media/File:PhiloThevet.jpg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Commentant la Septante dans un effort de rapprochement de la foi juive avec la philosophie antique (qui, à son grand dam, s’est développée dans un contexte polythéiste), &lt;a href=&quot;https://fr.wikipedia.org/wiki/Philon_d%27Alexandrie&quot;&gt;Philon d’Alexandrie&lt;/a&gt; mentionne explicitement le terme de πολυθεος, tranchant avec la tradition et le texte biblique. La pluralité est cette fois mise en avant, et opposée aux principes du &lt;a href=&quot;https://fr.wikipedia.org/wiki/D%C3%A9calogue&quot;&gt;Décalogue&lt;/a&gt; (“Tu n’auras point d’autres dieux”) : à l’ochlocratie (gouvernement de la masse) du ciel grec s’oppose &lt;strong&gt;le cosmos chrétien comme monarchie divine&lt;/strong&gt;. Philon livre également une typologie de la croyance polythéiste, en trois étapes : adoration de l’univers et de ses parties, fabrication d’idoles, culte des animaux. Il associe également le polythéisme aux ἀφιδρυμάτα (ce par quoi on installe un culte, lié à ἱδρυειν), montrant que &lt;strong&gt;les dieux sont dans le polythéisme installés parmi les hommes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-massa.jpg&quot; alt=&quot;Massa&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Francesco Massa (&lt;a href=&quot;http://unige.academia.edu/FrancescoMassa&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Les auteurs grecs chrétiens et apologistes feront leur, en plus de l’opposition entre monarchie et ochlocratie, la dichotomie opérée par Philon entre vérité (ἀλήθεια) et erreur, fausse croyance (ψευδῆ δόξα). Ces auteurs ont été longuement étudiés, en particulier par &lt;a href=&quot;https://www.unige.ch/lettres/antic/unites/hr/enseignants/massa/&quot;&gt;Francesco Massa&lt;/a&gt;. Celui-ci montre que, dans la civilisation grecque, la &lt;strong&gt;recherche de la vérité était l’apanage de la philosophie&lt;/strong&gt;, et non de la religion ; les auteurs hellènes chrétiens verront donc le christianisme d’abord comme une philosophie, supérieure à toutes les autres. Par ailleurs, dans la société hellène d’alors, les groupes n’étaient pas identifiés par leur identité religieuse, comme &lt;a href=&quot;https://journals.openedition.org/rhr/8829&quot;&gt;l’indique Massa&lt;/a&gt; :&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Les chrétiens ne constituaient pas un groupe religieux à part entière car ils n’appartenaient pas à un seul peuple. La seule catégorisation possible pour une communauté qui revendiquait pour elle un mode de vie particulier, sur la base d’un enseignement éthique et produisant une réflexion sur le divin, est celle d’&lt;em&gt;hairesis&lt;/em&gt;, c’est-à-dire d’école philosophique.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;De la nécessité subséquente de se distinguer à la fois des juifs et des religions traditionnelles de la Méditerranée antique, naîtront les outils langagiers en -isme dont nous sommes familiers. C’est ainsi que l’&lt;em&gt;hellenismos&lt;/em&gt; se détache peu à peu de sa signification de “grec” pour désigner majoritairement les polythéistes grecs (équivalent du &lt;strong&gt;paganisme&lt;/strong&gt; latin), les différenciant des adeptes du &lt;em&gt;iudaismos&lt;/em&gt; et du &lt;em&gt;christianismos&lt;/em&gt;. Au III&lt;sup&gt;e&lt;/sup&gt; siècle, &lt;a href=&quot;https://fr.wikipedia.org/wiki/Eus%C3%A8be_de_C%C3%A9sar%C3%A9e&quot;&gt;Eusèbe de Césarée&lt;/a&gt; voit dans la religion grecque une superstition issue de la théologie égyptienne et barbare, et qualifie la croyance en une multiplicité de dieux d’&lt;strong&gt;athéisme&lt;/strong&gt; (et non plus de &lt;em&gt;polythéisme&lt;/em&gt;), ce qui marquera longtemps la pensée chrétienne.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/2018-10-15-guillaume_bude.jpg&quot; alt=&quot;Guillaume Budé&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Guillaume Budé (&lt;a href=&quot;https://en.wikipedia.org/wiki/Guillaume_Bud%C3%A9#/media/File:Guillaume_Bud%C3%A9,_by_Jean_Clouet.jpg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Avec les Grandes découvertes, les termes de paganisme et d’idolâtrie seront également mobilisés pour décrire les religions du Nouveau Monde. Le terme de “polythéisme” quant à lui suit une trajectoire plus lente et complexe. Après des siècles d’oubli (et de domination latine), les études grecques reviennent avec la fondation du Collège de France, au début du XVI&lt;sup&gt;e&lt;/sup&gt; siècle, sous l’impulsion de &lt;a href=&quot;https://fr.wikipedia.org/wiki/Guillaume_Bud%C3%A9&quot;&gt;Guillaume Budé&lt;/a&gt;. Ce dernier, humaniste, pose la question de la réconciliation de la culture (héritage antique) et de la foi (comme les Pères de l’Église à leur époque) : comment promouvoir l’humanisme sans contrevenir à la croyance chrétienne ? Dans sa défense des belles lettres contre les théologiens qui les rejetaient, &lt;strong&gt;Budé exhume le terme de polythéisme en latin pour bien montrer qu’il faut chercher à s’en détacher&lt;/strong&gt; en le connaissant mieux :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Qu'y a-t-il en effet de plus repoussant que de voir, après la condamnation du polythéisme - lequel tomba jadis dans l’athéisme - (&lt;i&gt;post damnatam illam polytheiam, quae pro atheia quondam haud dubie fefellit&lt;/i&gt;), des hommes qui, pour avoir l’air savants et brillants, s’expriment encore comme si l’étude des bonnes lettres n’avait pas renié cette multitude de dieux d’en haut et d’en bas ?
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Guillaume Budé, &lt;i&gt;De Studio&lt;/i&gt;, 1577&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;En outre, Budé note très justement dans le &lt;em&gt;De transitu&lt;/em&gt; que l’arrivée du christianisme a pu non seulement “désacraliser les sanctuaires et les oracles des dieux”, mais aussi “détruire les droits existants entre les peuples, entre les familles, entre les races”. Autrement dit, le christianisme est venu remplacer les identités autres que l’identité religieuse : &lt;strong&gt;en contexte polythéiste, il n’y a pas d’identité religieuse&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/2018-10-15-jean_bodin.jpg&quot; alt=&quot;Jean Bodin&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Jean Bodin (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Jean_Bodin#/media/File:Jean_Bodin.jpg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Le terme français de &lt;em&gt;polythéisme&lt;/em&gt; apparaît avec Jean Bodin, juriste, dans son traité &lt;em&gt;De la démonomanie des sorciers&lt;/em&gt; de 1580 ; il l’utilise toutefois uniquement pour dénoncer les hérésies chrétiennes qui ne respectent pas l’unicité du divin. À partir de la fin du XVII&lt;sup&gt;e&lt;/sup&gt; siècle, le paradigme change et le terme de “polythéisme” apparaît de plus en plus (en même temps que naît “monothéisme”) à mesure que l’histoire des religions se détache de la Bible. Au cours du XIX&lt;sup&gt;e&lt;/sup&gt; siècle, le terme de “polythéisme” devient un concept commode désignant la pluralité des dieux, une étape majeure du développement religieux de l’humanité (pour Tylor, dans l’ordre animisme - polythéisme - monothéisme - science).&lt;/p&gt;

&lt;p&gt;Le terme reste peu usité par les hellénistes au XIX&lt;sup&gt;e&lt;/sup&gt; siècle, ou uniquement en contraste aux autres systèmes religieux. Il faut attendre 1930 pour que le “polythéisme” caractérise, en tant que label à part entière (et non plus par contraste), la religion grecque antique. Sa spécificité s’affirme au fil du XX&lt;sup&gt;e&lt;/sup&gt; siècle, jusqu’à la reconnaissance internationale, au début du XXI&lt;sup&gt;e&lt;/sup&gt; siècle.&lt;/p&gt;

&lt;p&gt;Par rapport à notre définition précédente de la religion, &lt;strong&gt;le polythéisme précise un certain profil pour la sphère supra-humaine dont il est question, non seulement en terme de pluralité des entités divines, mais aussi au niveau de la culture sous-jacente.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;hérodote-historien-des-religions-et-du-polythéisme&quot;&gt;Hérodote, “historien des religions et du polythéisme”&lt;/h2&gt;

&lt;p&gt;Avant d’en arriver à l’étude d’Hérodote, rappelons-nous de notre problématique principale. Une fois accepté le terme de &lt;em&gt;religion&lt;/em&gt;, la question est de savoir s’il faut l’utiliser au singulier ou au pluriel, s’il faut distinguer &lt;strong&gt;une ou des religions grecques&lt;/strong&gt;. Lorsque la Grèce était formée d’une collection de centaines de cités, chacune d’entre elles disposait d’un panthéon, d’un calendrier et de rituels spécifiques. C’est pourquoi certains chercheurs ont proposé et préféré le pluriel, à l’instar de &lt;a href=&quot;https://en.wikipedia.org/wiki/Simon_Price_(classicist)&quot;&gt;Simon Price&lt;/a&gt;, “to suggest the resulting variety, in both space and time” (&lt;em&gt;Religions of the Ancient Greeks&lt;/em&gt;). Cette tension entre le général (panhellénique) et le particulier (local) se retrouve dans le cadre plus large de la cité grecque, de la &lt;a href=&quot;https://fr.wikipedia.org/wiki/Polis&quot;&gt;πολις&lt;/a&gt; ; cette fois cependant, les auteurs n’hésitent pas à employer largement le singulier, incités probablement par l’existence du terme exact en grec ancien (alors que θρησκεία reste très peu usité).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/2018-10-15-herodote.jpg&quot; alt=&quot;Hérodote&quot; class=&quot;center-image&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote (&lt;a href=&quot;https://en.wikipedia.org/wiki/Herodotus#/media/File:Herodotos_Met_91.8.jpg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;C’est dans l’objectif de saisir cette tension, à la fois au niveau de la cité et de sa composante religieuse, qu’&lt;a href=&quot;https://fr.wikipedia.org/wiki/H%C3%A9rodote&quot;&gt;Hérodote&lt;/a&gt;, historien grec du V&lt;sup&gt;e&lt;/sup&gt; avant notre ère, apparaît comme une source privilégiée (voir approfondissements &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL8383353615914827232_Pirenne_Cours_3_2018_Fevr_15.pdf&quot;&gt;ici&lt;/a&gt; et &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL1528485476435301551_Pirenne_Cours_4_2018_Fevr_22.pdf&quot;&gt;ici&lt;/a&gt;), et ce pour plusieurs raisons.&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;L'ampleur du matériau disponible en matière de religion&lt;/li&gt;
    &lt;li&gt;La posture spécifique adoptée par Hérodote à l'égard de la sphère religieuse. Dès l'Antiquité, Hérodote est vu comme un Homère en prose, décrivant à la fois une grande guerre (comme l'Iliade) et le monde connu, les sociétés qui l'entourent (comme l'Odyssée) afin de les préserver de l'oubli. Toutefois, il ne se prévaut pas de l'inspiration divine, et surtout il change de point de vue sur ce qui est accessible et dicible à propos des dieux.&lt;/li&gt;
    &lt;li&gt;La traduction culturelle qu'il opère quand il décrit les traditions des autres peuples (ethnographie). Comme le dit &lt;a href=&quot;https://fr.wikipedia.org/wiki/Fran%C3%A7ois_Hartog&quot;&gt;François Hartog&lt;/a&gt;, &quot;[Hérodote] découpe le réel de l'autre selon des catégories grecques&quot; (&lt;i&gt;Le Miroir d'Hérodote&lt;/i&gt;) : la grille de lecture des données ethnographiques est helléno-centrée. Cela permet de découvrir, en creux, la représentation que les Grecs ont de leur propre religion.&lt;/li&gt;
    &lt;li&gt;Le déploiement de la tension entre unité et diversité des figures divines grecques en parallèle de la description des Guerres médiques.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comme il l’indique dès la préface, le travail d’Hérodote porte sur les faits mémorables des Grecs et des Barbares durant les &lt;a href=&quot;https://fr.wikipedia.org/wiki/Guerres_m%C3%A9diques&quot;&gt;Guerres médiques&lt;/a&gt; ; mais au fil de son &lt;em&gt;Enquête&lt;/em&gt;, il nous livre une description détaillée des traditions, coutumes et habitudes de vie des différentes populations engagées dans le conflit. Au sein de celles-ci, on trouve les diverses manières d’honorer les dieux, qui sont indissolublement liées aux pratiques traditionnelles sans posséder d’étiquette spécifique qui permette de les distinguer. C’est en fait le terme de &lt;strong&gt;νόμοι&lt;/strong&gt; qu’Hérodote emploie pour décrire les &lt;strong&gt;us et coutumes d’un peuple&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Le champ sémantique de νόμος renvoie à la division, la répartition selon l’usage, selon la convenance ; ce n’est pas une simple distribution, mais plutôt &lt;strong&gt;ce qui régit la vie en société&lt;/strong&gt;. Le terme apparaît pour la première fois non chez &lt;a href=&quot;https://fr.wikipedia.org/wiki/Hom%C3%A8re&quot;&gt;Homère&lt;/a&gt;, mais chez &lt;a href=&quot;https://fr.wikipedia.org/wiki/H%C3%A9siode&quot;&gt;Hésiode&lt;/a&gt; :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;καὶ γὰρ νῦν, ὅτε πού τις ἐπιχθονίων ἀνθρώπων ἔρδων ἱερὰ καλὰ &lt;b&gt;κατὰ νόμον&lt;/b&gt; ἱλάσκηται, κικλήσκει Ἑκάτην.&lt;br /&gt;

Aujourd'hui encore, tout humain d'ici-bas qui veut, par un beau sacrifice offert &lt;b&gt;selon la coutume&lt;/b&gt;, implorer une grâce invoque Hécate.&lt;/div&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hésiode, &lt;i&gt;Théogonie&lt;/i&gt;, v. 416-417&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Hérodote s’inscrit dans une réflexion qui lui est contemporaine, et qui pose la question des relation entre νόμος et φύσις, c’est-à-dire (grossièrement, suivant des critères anachroniques) entre &lt;em&gt;culture&lt;/em&gt; et &lt;em&gt;nature&lt;/em&gt; : comment s’opposent ou se lient les institutions humaines d’une part et la complexion du vivant, en développement indépendamment de l’initiative humaine, d’autre part ? Ce discours se retrouve de manière aiguë à l’époque d’Hésiode, à la fois chez les présocratiques et chez les disciples d’[Hippocrate. La conscience qu’Hérodote a de cette opposition est manifeste dans le dialogue entre Démarate (roi déchu de Sparte, exilé en Perse) et &lt;a href=&quot;https://fr.wikipedia.org/wiki/Xerx%C3%A8s_Ier&quot;&gt;Xerxès&lt;/a&gt; (roi des Perses), où ce dernier veut savoir si les Grecs oseront porter la main sur son armée surpuissante :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
La Grèce est nourrie dans la pauvreté (τῇ Ἑλλάδι πενίη μὲν αἰεί κοτε σύντροφός ἐστι), 
[mais] la vertu est acquise, fruit de la sagesse et d’une loi rigoureuse (ἀρετὴ δὲ ἔπακτος ἐστι, ἀπό τε σοφίης κατεργασμένη καὶ νόμου ἰσχυροῦ)
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, VII, 102-104&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;La dimension acquise et construite des νόμοι traverse l’ensemble de l’oeuvre : ce sont bien des &lt;strong&gt;constructions humaines qui se rapportent notamment à la sphère suprahumaine&lt;/strong&gt;. C’est ainsi qu’Hérodote décrit les νόμοι des &lt;a href=&quot;https://fr.wikipedia.org/wiki/Ach%C3%A9m%C3%A9nides&quot;&gt;Perses&lt;/a&gt; dès le premier livre :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Les Perses, à ma connaissance, observent les &lt;b&gt;coutumes&lt;/b&gt; suivantes (&lt;b&gt;νόμοισι&lt;/b&gt; τοιοισίδε χρεωμένους). Ils n’ont pas l’usage d’élever des statues ni des temples ni des autels (ἀγάλματα μὲν καὶ νηοὺς καὶ βωμοὺς οὐκ ἐν νόμῳ ποιευμένους ἱδρύεσθαι); tout au contraire, ils accusent de folie ceux qui le font; la raison en est, à mon avis, qu’ils n’ont jamais pensé, comme les Grecs, que les dieux soient de même complexion que les hommes (ὡς μὲν ἐμοὶ δοκέειν, ὅτι οὐκ ἀνθρωποφυέας ἐνόμισαν τοὺς θεοὺς κατά περ οἱ Ἕλληνες εἶναι). Leur coutume est de monter sur les plus hautes montagnes pour offrir des sacrifices à Zeus (οἱ δὲ νομίζουσι Διὶ μὲν ἐπὶ τὰ ὑψηλότατα τῶν ὀρέων ἀναβαίνοντες θυσίας ἔρδειν), dont ils donnent le nom à toute l’étendue circulaire du ciel. Ils sacrifient (θύουσι) au soleil, à la lune, à la terre, au feu, à l’eau, aux vents. Ce sont là les seuls dieux à qui ils sacrifient de toute antiquité; mais en outre ils ont appris (ἐπιμεμαθήκασι), des Assyriens et des Arabes, à sacrifier aussi à Aphrodite Ourania. Les Assyriens appellent cette déesse Mylitta, les Arabes, Alilat, les Perses, Mitra.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, I, 131&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;De cette description helléno-centrée, on peut tenter de &lt;strong&gt;reconstruire des aspects positifs de la religion grecque qui sont présents en creux&lt;/strong&gt;. On retrouve la notion &lt;em&gt;d’installation&lt;/em&gt; des figures divines (ἱδρυειν), les actes à la fois individuels et collectifs pour entrer en relation avec les dieux, une capacité à donner des noms grecs aux dieux étrangers (témoin d’une certaine plasticité), et l’importance de l’interaction entre peuples dans la constitution des panthéons ; on voit également poindre des pratiques sacrificielles ou rituelles. Englobant tous ces aspects, la religion relève effectivement des νόμοι comme &lt;em&gt;pratique socialement sanctionnée&lt;/em&gt;. Ce sont donc des modèles culturellement déterminés qui régissent les normes concernant la shpère supra-humaine, ce qui légitimerait &lt;em&gt;la&lt;/em&gt; religion grecque au singulier, tandis que la diversité des pratiques plaide pour une religion plurielle.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/2018-10-15-herodote_carte.png&quot; alt=&quot;Carte du monde selon l'Enquête d'Hérodote&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Carte du monde décrit par Hérodote dans son &lt;i&gt;Enquête&lt;/i&gt; (&lt;a href=&quot;https://fr.wikipedia.org/wiki/H%C3%A9rodote#/media/File:Herodotus_world_map-fr.svg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Au fil d’une description des pratiques d’autres peuples, Hérodote note particulièrement certains phénomènes ou coutumes : communautés de femmes, refus des dieux étrangers et replis sur les dieux “des pères”, vénération d’un &lt;em&gt;seul&lt;/em&gt; dieu (le soleil), … Il propose parfois des éclaircissements concernant certaines pratiques qui paraîtraient étranges à son lectorat : il explique ainsi une coutume de sacrifice de chevaux par un lien avec l’identité du dieu (le plus rapide des animaux pour le plus rapide des dieux), créant un parallèle, une analogie imagée. Plus loin, il décrit les usages des &lt;a href=&quot;https://fr.wikipedia.org/wiki/Scythes&quot;&gt;Scythes&lt;/a&gt;, traduisant leurs dieux en grec, et notant la non-installation des figures divines (due au caractère nomade du peuple) :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Les Scythes ont donc en abondance ce qui est de première importance; pour le reste, voici quelles sont leurs coutumes (τὰ δὲ λοιπὰ &lt;b&gt;νόμαια&lt;/b&gt; κατὰ τάδε σφι διάκειται). Les seules divinités à qui ils adressent des prières (θεοὺς μὲν μούνους τούσδε ἱλάσκονται) sont les suivantes: en premier lieu Hestia, puis Zeus et Gè (ils considèrent que Gèest l’épouse de Zeus [νομίζοντες τὴν Γῆν τοῦ Διὸς εἶναι γυναῖκα]), ensuite Apollon, Aphrodite Ourania, Héraclès et Arès ; ces dieux-là, tous les Scythes les reconnaissent et les honorent (τούτους μὲν πάντες Σκύθαι νενομίκασι); ceux qu’on appelle Scythes royaux offrent aussi des sacrifices à Poséidon. En langue scythe (ὀνομάζεται δὲ σκυθιστὶ), Hestia s’appelle Tabiti; Zeus, Papaios, nom qui, à mon avis, est très juste ; Gè, Api ; Apollon, Goitosyros ; Aphrodite Ourania, Argimpasa ; Poséidon, Thagimasadas. L’usage n’est pas chez eux d’élever des statues, des autels, ni des temples, sinon à Arès ; pour celui-là, c’est l’usage (ἀγάλματα δὲ καὶ βωμοὺς καὶ νηοὺς οὐ νομίζουσι ποιέειν πλὴν Ἄρεϊ· τούτῳ δὲ νομίζουσι). 
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, I, 216&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Revenant à l’extrait sur les coutumes des Perses, on constate que, pour les Grecs, les dieux sont à la fois &lt;em&gt;installés parmi les hommes&lt;/em&gt; et &lt;em&gt;de même complexion que les hommes&lt;/em&gt; (ἀνθρωποφυέας), c’est-à-dire non seulement anthropomorphes mais aussi nouant des relations similaires aux relations humaines. Ainsi, &lt;strong&gt;les dieux font véritablement société avec les humains qui les accueillent&lt;/strong&gt; ; par ailleurs, la représentation traditionnelle des dieux est intimement liée aux pratiques qu’elle semble induire (l’installation semble causer l’anthropomorphisme). L’utilisation de νομίζειν dans deux contextes différents montre l’enchâssement des représentations et des coutumes d’hommage.&lt;/p&gt;

&lt;p&gt;L’intégralité du livre II de l’&lt;em&gt;Enquête&lt;/em&gt; est consacrée à l’Égypte, et c’est également dans cette partie qu’Hérodote prend le plus explicitement position par rapport à la sphère religieuse : “&lt;strong&gt;Sur les choses divines (τὰ θεῖα) […], je ne suis pas disposé à les exposer&lt;/strong&gt;, à la seule exception de leurs noms, considérant que tous les hommes en ont une égale connaissance ; les mentions qu’il m’arrivera d’en faire, ce sera aux nécessités de la narration qu’elles seront dues. En ce qui concerne les affaires humaines (τὰ ἀνθρωπήια πρήγματα), …” (&lt;em&gt;Enquête&lt;/em&gt;, II, 3-4). Il met ainsi en regard les affaires divines (τὰ θεῖα πρήγματα) et les affaires humaines (τὰ ἀνθρωπήια πρήγματα), et émet une réserve sur les premières (sa piété, même si elle joue un rôle, n’en paraît pas l’unique justification). S’il ne se garde pas de fournir des interprétations religieuses &lt;em&gt;générales&lt;/em&gt;, employant à cette occasion un vocabulaire générique comme οἱ θεοί (les dieux) ou τὸ δαιμόνιον (le divin), il s’en tient pour ce qui concerne des divinités spécifiques à des discours rapportés, nécessaires à la narration. Les dieux particuliers ne sont pas des acteurs de l’&lt;em&gt;Enquête&lt;/em&gt;, ce qui tranche encore avec la tradition poétique : &lt;strong&gt;les dieux ont presque complètement disparu dans le rôle d’instances spécifiques agissantes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-amon.svg&quot; alt=&quot;Représentation d'Amon&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Représentation du dieu égyptien Amon, considéré par Hérodote comme l’équivalent de Zeus en terme de profil divin (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Amon#/media/File:Amun.svg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Hérodote adopte ainsi un point de vue exclusivement tourné vers les &lt;em&gt;affaires humaines&lt;/em&gt;. Il prend également une &lt;strong&gt;hypothèse diffusionniste sur la distribution des usages religieux&lt;/strong&gt;, en particulier sur les dénominations (οὐνόματα) donnés aux dieux (ou encore &lt;em&gt;l’usage d’utiliser des dénominations&lt;/em&gt;), qui seraient majoritairement passés de l’Égypte à la Grèce (via les Pélasges, peuple peuplant la Grèce avant les Grecs) :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Les dénominations (οὐνόματα) de presque tous les dieux sont venus d’Égypte en Grèce. Qu’elles viennent de chez les Barbares, mes enquêtes me le font constater ; et je pense que c’est surtout de l’Égypte. Car, à l’exception de Poséidon et des Dioscures, pour qui je l’ai déjà dit, et d’Héra, d’Hestia, de Thémis, des Charites et des Néréides, de tous les autres dieux les Égyptiens possèdent les dénominations de tout temps dans le pays [...]. Quant aux dénominations de dieux qu’ils disent ne pas connaître, il me semble qu’ils ont été nommés par les Pélasges, sauf Poséidon. Ils ont appris à connaître ce dieu auprès des Libyens (τοῦτον δὲ τὸν θεὸν παρὰ Λιβύων ἐπύθοντο). Car il n’est personne à posséder la dénomination de Poséidon dès le début, si ce n’est les Libyens, et ils honorent ce dieu de tout temps (οὐδαμοὶ γὰρ ἀπ ̓ ἀρχῆς Ποσειδέωνος οὔνομα ἔκτηνται εἰ μὴ Λίβυες καὶ τιμῶσι τὸν θεὸν τοῦτον αἰεί). Les Égyptiens ne rendent pas non plus de culte à des héros. [...] 
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
Autrefois, à ce que j’ai entendu dire à Dodone, les Pélasges offraient tous les sacrifices en invoquant «des dieux», sans désigner aucun d’entre eux par un surnom ou par un nom (ἐπωνυμίην δὲ οὐδ’ οὔνομα ἐποιεῦντο οὐδενὶ αὐτῶν) ; car ils n’avaient encore rien entendu de pareil. Ils les avaient appelés ainsi (θεούς) en partant de cette considération que c’est pour avoir établi (θέντες) l’ordre dans l’univers que les dieux présidaient à la répartition de toutes les choses. Plus tard, au bout de beaucoup de temps, les Pélasges apprirent à connaître, venus d’Égypte, les dénominations des dieux autres que Dionysos (ils apprirent bien plus tard celui de Dionysos) ; un temps passa encore, et ils consultèrent sur ces dénominations à Dodone; l’oracle de Dodone est regardé en effet comme le plus ancien qu’il y ait chez les Grecs, et il était le seul à cette époque. Les Pélasges demandèrent donc à Dodone s’ils adopteraient les dénominations qui venaient de chez les Barbares; et l’oracle leur répondit d’en faire usage (ἀνεῖλε τὸ μαντήιον χρᾶσθαι). À partir de là, ils sacrifièrent en utilisant les dénominations des dieux (ἀπὸ μὲν δὴ τούτου τοῦ χρόνου ἔθυον τοῖσι οὐνόμασι τῶν θεῶν χρεώμενοι). Et les Grecs, ensuite, les reçurent d’eux.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, II, 50-52&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Traduire ainsi οὐνόματα par &lt;em&gt;dénominations&lt;/em&gt; plutôt que &lt;em&gt;noms&lt;/em&gt; met l’accent sur un point important, rendant justice au texte : ce qui est passé des Égyptiens aux Pélasges, ces οὐνόματα, est moins le nom des dieux que &lt;strong&gt;le fait d’identifier, en leur attribuant un nom spécifique, des profils divins au sein d’un ensemble de dieux jadis indifférenciés&lt;/strong&gt;. Ces profils sont ainsi bien davantage susceptibles de passer d’une culture à l’autre, après interprétation et avec transposition du nom lui-même. C’est aux poètes, Hésiode et Homère, qu’Hérodote attribue la constitution et la configuration du panthéon grec, en déployant la dénomination des dieux établie avant eux : &lt;strong&gt;la créativité et les institutions humaines sont largement à l’oeuvre dans la représentation des dieux&lt;/strong&gt;, et on retrouve l’importance du νόμος.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dieux-grecs-ou-dieux-des-grecs-&quot;&gt;Dieux grecs ou dieux des Grecs ?&lt;/h2&gt;

&lt;p&gt;Revenons plus précisément à la tension entre le général et le particulier, qui reste notre fil rouge. Quand on quitte les livres odysséens (ethnographiques) pour se plonger dans les livres iliadiques (récit des Guerres médiques), quelle image percevons-nous des dieux ? Les Grecs engagés dans le conflit partagent-ils une même vision du monde divin, comme le laisse entendre la section précédente, ou perçoit-on dans l’&lt;em&gt;Enquête&lt;/em&gt; d’Hérodote une fragmentation des divinités en fonction des cités particulières ? &lt;strong&gt;Autrement dit, doit-on parler de dieux grecs ou de dieux des Grecs ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Commençons par définir ce que recouvre &lt;em&gt;le fait d’être grec&lt;/em&gt;, à l’aide des &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL8863268062282612292_Pirenne_Cours_5_2018_Mars_1.pdf&quot;&gt;approfondissements&lt;/a&gt;. Une des premières définition de ce qui constitue une communauté grecque se trouve dans un passage célèbre du livre VIII d’Hérodote. Donnons le contexte : après la bataille de Salamine, victoire maritime décisive contre les Perses, ceux-ci dépêchent un émissaire aux Athéniens pour les inciter à se détacher de la coalition grecque et à rejoindre l’empire perse. Les Athéniens refusent catégoriquement, et professent leur attachement à leurs alliés. La première raison qu’ils donnent pour justifier ce refus est “l’impérieux devoir de punir le plus sévèrement possible l’incendie, la réduction en un tas de décombres, des statues et des demeures des dieux (τῶν θεῶν τὰ ἀγάλματα καὶ τά οἰκήματα)”. Après plusieurs pillages d’Athènes, il s’agit de punir l’arrogance des Perses. Le deuxième argument nous en apprend davantage :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
[...] αὖτις δὲ τὸ Ἑλληνικόν, ἐὸν ὅμαιμόν τε καὶ ὁμόγλωσσον, καὶ &lt;b&gt;θεῶν ἱδρύματά τε κοινὰ καὶ θυσίαι&lt;/b&gt; ἤθεά τε ὁμότροπα [...]
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
[Il est inconcevable de trahir] l’Hellenikon, même sang et même langue, &lt;b&gt;des établissements communs des dieux ainsi que des sacrifices&lt;/b&gt;, des mœurs et des manières semblables
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, VIII, 144&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Cette énumération ajoute des éléments qui précisent ce que regroupe la &lt;em&gt;grécité&lt;/em&gt;, en particulier le fait de se retrouver dans des sanctuaires régionaux ou panhelléniques (Delphes, Olympie) où &lt;strong&gt;les dieux sont installés, enracinés localement&lt;/strong&gt;. Une telle énumération ne permet cependant pas de faire de la religion un facteur d’unité de l’&lt;em&gt;Hellenikon&lt;/em&gt;. Ce ne sont ainsi pas les dieux en eux-mêmes qui sont décrits comme communs, mais des lieux fondés par une décision commune. Il n’y a par ailleurs par de &lt;em&gt;territoire commun&lt;/em&gt; au sens propre. La communauté décrite par Hérodote est rituelle et transitoire, et on n’y trouve pas jusqu’ici d’identité religieuse partagée par tous les Grecs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-denys_halicarnasse.jpg&quot; alt=&quot;Denys d'Halicarnasse&quot; class=&quot;center-image&quot; height=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Denys d’Halicarnasse (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Denys_d%27Halicarnasse#/media/File:Dionigi_di_Alicarnasso.jpg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Il faut ensuite attendre les &lt;em&gt;Antiquités romaines&lt;/em&gt; de Denys d’Halicarnasse, au tournant du millénaire (près de quatre siècles après Hérodote), pour retrouver une définition de la grécité. Ce faisant, il évoque cette fois des dieux communs aux Grecs. Il s’agit pour lui d’exalter Rome comme cité idéale en l’enracinant dans la Grèce antique, inaltérée par le passage ultérieur des Barbares ; cette perspective téléologique l’inscrit dans une référence &lt;em&gt;culturelle&lt;/em&gt; (par opposition à la référence &lt;em&gt;cultuelle&lt;/em&gt; du texte d’Hérodote) et une certaine généralité dans le propos.&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
ἐπεὶ ἄλλοι γε συχνοὶ ἐν βαρβάροις οἰκοῦντες ὀλίγου χρόνου διελθόντος ἅπαν τὸ Ἑλληνικὸν ἀπέμαθον, ὡς μήτε φωνὴν Ἑλλάδα φθέγγεσθαι μήτε ἐπιτηδεύμασιν Ἑλλήνων χρῆσθαι, μήτε θεοὺς τοὺς αὐτοὺς νομίζειν, μήτε νόμους τοὺς ἐπιεικεῖς, ᾧ μάλιστα διαλλάσσει φύσις Ἑλλὰς βαρβάρου, μήτε τῶν ἄλλων συμβολαίων μηδ ̓ ὁτιοῦν.
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
Bien d'autres en effet, vivant au milieu des barbares, ont en peu de temps désappris tout l’Hellenikon au point de ne plus parler grec, de ne plus suivre les habitudes des Grecs, de ne pas reconnaître les mêmes dieux qu’eux, ni leurs lois tempérées – toutes choses qui principalement marquent la différence entre la nature grecque et la nature barbare –, ni même n’importe quel autre signe distinctif.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Denys d’Halicarnasse, &lt;i&gt;Antiquités romaines&lt;/i&gt;, I, 89, 4&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Hérodote ne mentionne, quant à lui, qu’une seule fois des &lt;em&gt;dieux communs&lt;/em&gt; (θεούς τε κοινοὺς), et fait parfois référence à des &lt;em&gt;dieux grecs&lt;/em&gt;. Chacune de ces évocations n’a toutefois lieu, à chaque fois, que dans le contexte d’appel à l’unité, à l’aide, à la fédération : il semble qu’il s’agisse de &lt;em&gt;représentations&lt;/em&gt; servant dans la lutte contre les Barbares, sorte d’emphase rhétorique. Dans le passage suivant néanmoins, des interlocuteurs venant de cités différentes, mais tous deux grecs, invoquent les &lt;em&gt;dieux des Grecs&lt;/em&gt; dans leur plaidoyer ; cette catégorie peut donc être mobilisée.&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
«ἐπιμαρτυρόμεθά τε ἐπικαλεόμενοι ὑμῖν &lt;b&gt;θεοὺς τοὺς Ἑλληνίους&lt;/b&gt; μὴ κατιστάναι τυραννίδας ἐς τὰς πόλις. οὔκων παύσεσθε ἀλλὰ πειρήσεσθε παρὰ τὸ δίκαιον κατάγοντες Ἱππίην; ἴστεὑ μῖν Κορινθίους γε οὐ συναινέοντας.» Σωκλέης μὲν ἀπὸ Κορίνθου πρεσβεύων ἔλεξε τάδε, Ἱππίης δὲ αὐτὸν ἀμείβετο τοὺς αὐτοὺς θεοὺς ἐπικαλέσας ἐκείνῳ...
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
« Nous vous abjurons, au nom des &lt;b&gt;dieux des Grecs&lt;/b&gt;, de ne pas établir de tyrans dans les villes. Ne renoncerez-vous pas à votre dessein ? Allez-vous entreprendre, contre toute justice, de ramener Hippias ? Sachez que, eux du moins, les Corinthiens ne vous approuvent pas.» (93) Ainsi parla Soclès, député de Corinthe. Et Hippias, invoquant les mêmes dieux qu’il avait évoqués, lui répondit ...
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, V, 92-93&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-naukratis.jpg&quot; alt=&quot;Naukratis dans le delta du Nil&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Naukratis, dans le delta du Nil (&lt;a href=&quot;http://www.britishmuseum.org/research/online_research_catalogues/ng/naukratis_greeks_in_egypt/introduction.aspx&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Passons désormais à un autre élément du dossier des dieux grecs : la description qu’Hérodote fait de la fondation du sanctuaire de Naukratis, appelé &lt;em&gt;Hellènion&lt;/em&gt;. Naukratis, cité du delta du Nil, apparaît dans la description faite par Hérodote au livre II ; selon lui, cette cité aurait été offerte aux Grecs par le pharaon Amasis pour leurs échanges commerciaux, au titre d’&lt;em&gt;emporion&lt;/em&gt; (comptoir commercial).&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
τοῖσι δὲ μὴ βουλομένοισι αὐτῶν ἐνοικέειν, αὐτόσε δὲ ναυτιλλομένοισι ἔδωκε χώρους ἐνιδρύσασθαι βωμοὺς καὶ τεμένεα θεοῖσι. τὸ μέν νυν μέγιστον αὐτῶν τέμενος καὶ ὀνομαστότατον ἐὸν καὶ χρησιμώτατον, &lt;b&gt;καλεόμενον δὲ Ἑλλήνιον, αἵδε πόλιές εἰσι αἱ ἱδρυμέναι κοινῇ&lt;/b&gt;· Ἰώνων μὲν Χίος καὶ Τέως καὶ Φώκαια καὶ Κλαζομεναί, Δωριέων δὲ Ῥόδος καὶ Κνίδος καὶ Ἁλικαρνησσὸς καὶ Φάσηλις, Αἰολέων δὲ ἡ Μυτιληναίων μούνη. τουτέων μέν ἐστι τοῦτο τὸ τέμενος, καὶ προστάτας τοῦ ἐμπορίου αὗται αἱ πόλιές εἰσι αἱ παρέχουσαι· &lt;b&gt;ὅσαι δὲ ἄλλαι πόλιες μεταποιεῦνται, οὐδέν σφι μετεὸν μεταποιεῦνται&lt;/b&gt;. χωρὶς δὲ Αἰγινῆται ἐπὶ ἑωυτῶν ἱδρύσαντο τέμενος Διός, καὶ ἄλλο Σάμιοι ῞Ηρης, καὶ Μιλήσιοι Ἀπόλλωνος.
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
À ceux qui ne voulaient pas habiter là, mais que la navigation y amenait, il concéda des emplacements pour y établir des autels et des enceintes pour des dieux. La plus grande de ces enceintes sacrées, la plus célèbre et la plus fréquentée, &lt;b&gt;appelée Hellènion, a été établie en commun par les cités que voici&lt;/b&gt; : les cités ioniennes de Chios, Téos, Phocée et Clazomènes ; les cités doriennes de Rhodes, Cnide, Halicarnasse, Phasélis ; et une seule cité éolienne, celle de Mytilène. Telles sont les cités à qui appartient l’enceinte sacrée, et ces mêmes cités sont celles qui fournissent les officiels du comptoir commercial ; &lt;b&gt;toutes les autres cités qui prétendent y avoir part le prétendent sans aucun droit&lt;/b&gt;. Indépendamment, les Éginètes pour eux-mêmes ont fondé un sanctuaire de Zeus ; les Samiens, un autre d’Héra ; les Milésiens, un d’Apollon.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, II, 178&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Rajoutons quelques éléments. Malgré sa consonance panhellénique, le nom d’&lt;em&gt;Hellènion&lt;/em&gt; opère plus probablement un contraste entre les communautés grecques installées à Naukratis, et l’environnement égyptien. Par ailleurs, il s’agit de la cristallisation d’une identité certes grecque, mais géographiquement très localisée : toutes les cités mentionnées se trouvent sur la côte de l’Asie mineure, à l’exception d’Égine (qui n’est pas fondatrice du sanctuaire). &lt;strong&gt;Le sanctuaire de Naukratis n’est ainsi pas panhellénique au sens strict&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Malheureusement pour nous, Hérodote ne mentionne pas explicitement le nom des dieux honorés dans le sanctuaire commun. L’archéologie moderne a toutefois pu découvrir et déchiffrer des inscriptions et poteries présentes sur le site, permettant de combler certaines lacunes : on trouve ainsi mention d’Aphrodite, Apollon, Hercule, les Dioscures, … Par ailleurs, une trentaine de tessons évoquent des &lt;em&gt;dieux génériques&lt;/em&gt;, qui nous intéressent particulièrement : les inscriptions rendent alors hommage à &lt;strong&gt;τοῖς Ἑλλήνων θεοῖς&lt;/strong&gt;, les dieux des Grecs. Cela laisse entendre qu’un &lt;strong&gt;groupe générique de dieux pouvait être honoré en tant que tel&lt;/strong&gt;, même si des dieux singuliers pouvaient également être honorés dans l’enceinte.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-stella_georgoudi.jpg&quot; alt=&quot;Stella Georgoudi&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Stella Georgoudi (&lt;a href=&quot;http://www.epirus-tv-news.gr/2017/03/blog-post_252.html&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Revenant à la tension entre le général et le particulier, il est intéressant de dresser un parallèle avec la situation des douze dieux. Hérodote atteste en effet de l’existence dès son époque de cette sorte de panthéon réduit, dont les Grecs auraient hérité les noms des Égyptiens. De cette observation, &lt;a href=&quot;http://ephe.academia.edu/StellaGeorgoudi&quot;&gt;Stella Georgoudi&lt;/a&gt; tire (voir &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL6258122956019020924_Pirenne_Cours_6_2018_Mars_8.pdf&quot;&gt;approfondissements&lt;/a&gt;) quatre constats :&lt;/p&gt;
&lt;ul style=&quot;margin-top: -5px;&quot;&gt;
    &lt;li&gt;L'ensemble que forme le groupement générique des douze dieux peut être assumé comme tel par les Grecs, et mobilisé sur le plan cultuel, sans qu'il y ait d'identification obligatoire des dieux singuliers : &lt;b&gt;le groupe lui-même est utilisable&lt;/b&gt;&lt;/li&gt;
    &lt;li&gt;Lorsque les dieux singuliers sont identifiés, cette identification est à géométrie variable suivant les lieux : &lt;b&gt;il n'existe pas de liste canonique des douze dieux&lt;/b&gt;, c'est un ensemble flexible et adaptable.&lt;/li&gt;
    &lt;li&gt;Le culte des douze dieux que l'on retrouve dans certaines cités (aux douze dieux &lt;i&gt;comme tels&lt;/i&gt;) correspond à une &lt;b&gt;sollicitation d'unité et de cohésion du corps civique&lt;/b&gt;.&lt;/li&gt;
    &lt;li&gt;Le groupe peut correspondre à des aspirations davantage panhelléniques face à ce qui n'est pas grec : &lt;b&gt;c'est une sorte d'affirmation de l'hellénisme&lt;/b&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Le groupe des douze dieux forme ainsi un ensemble qui peut rester tel quel : il s’agit alors d’une pluralité qui est conçue comme une entité cultuelle, destinataire d’un hommage rendu par les humains&lt;/strong&gt;. La constitution même de ce groupe, unité sous-tendant la diversité, traduit la tension entre général et particulier. Le groupe peut également, en fonction du contexte, se déployer en douze dieux distincts. De même que ces douze dieux, les “dieux des Grecs” de Naukratis forment une collectivité ; dans un contexte cultuel, l’hommage peut s’adresser au groupe comme à des dieux singuliers. La juxtaposition entre une entité plurielle et une pluralité d’entités est activable, et les dieux sont “singularisables”. Ainsi à Naukratis, les dieux honorés à l’Hellènion sont à la fois les dieux singuliers associés à ce sanctuaire, et une représentation générique protégeant les Grecs de ce qui n’est pas hellène.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Clarifions dès à présent un point central de nos considérations. Hérodote ne fait pas, comme on pourrait le penser, des dieux une figure purement théorique ; il n’opère pas de distinction entre les dieux des poètes, et ceux auxquels on adresse des sacrifices au temps des Pélasges. Cette distinction est en effet le sujet de débats, dans la communauté des chercheurs, entre deux positions qui rappellent la tension qui nous occupe :&lt;/p&gt;
&lt;ul style=&quot;margin-top: -5px;&quot;&gt;
    &lt;li&gt;La notion de &quot;dieux grecs&quot; ne forme qu'un arrière-plan rhétorique, décorrélé des pratiques cultuelles locales ; l'idée de dieux communs à tous les Grecs n'est qu'un leurre en dehors des sanctuaires majeurs.&lt;/li&gt;
    &lt;li&gt;&lt;b&gt;Il existe une continuité entre les dieux de la poésie et ceux des cultes&lt;/b&gt;, continuité qui reste à déterminer. C'est l'hypothèse favorisée par l'oeuvre d'Hérodote.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il y a ainsi continuité entre le nom des dieux, partagés par tous les Grecs, et les déclinaisons locales des différents surnoms qui sont leur sont donnés. C’est ce qu’illustre un nouveau passsage du livre I de l’&lt;em&gt;Enquête&lt;/em&gt;, où Zeus est invoqué sous divers surnoms en même temps. &lt;strong&gt;Cette pluralité inhérente à chaque dieu est une des clefs fondamentales du polythéisme grec&lt;/strong&gt; ; mais jusqu’à quel point doit-on l’envisager ?&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
περιημεκτέων δὲ τῇ συμφορῇ δεινῶς ἐκάλεε μὲν Δία καθάρσιον, μαρτυρόμενος τὰ ὑπὸ τοῦ ξείνου πεπονθὼς εἴη, ἐκάλεε δὲ ἐπίστιόν τε καὶ ἑταιρήιον, τὸν αὐτὸν τοῦτον ὀνομάζων θεόν, τὸν μὲν ἐπίστιον καλέων, διότι δὴ οἰκίοισι ὑποδεξάμενος τὸν ξεῖνον φονέα τοῦ παιδὸς ἐλάνθανε βόσκων, τὸν δὲ ἑταιρήιον, ὡς φύλακον συμπέμψας αὐτὸν εὑρήκοι πολεμιώτατον.
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
Dans l’excès d’affliction que lui causait son malheur, il invoquait Zeus comme patron des purifications, le prenant à témoin du mal que  l'étranger lui avait fait ; il l’invoquait comme protecteur du foyer et de  l'amitié – &lt;b&gt;c'était ce même dieu qu'il dénommait&lt;/b&gt; –, comme protecteur du foyer, parce qu'après avoir accueilli l'étranger dans sa demeure il avait nourri sans le savoir le meurtrier de son fils ; comme protecteur de l'amitié, parce qu'après l'avoir envoyé avec Atys en guise de gardien, il avait trouvé en lui son pire ennemi.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, I, 44&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-raphael-charites.jpg&quot; alt=&quot;Les trois Grâces de Raphaël, ou Charites pour les Grecs&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;&lt;i&gt;Les Trois Grâces&lt;/i&gt; de Raphaël, adaptations latines des Charites grecques (&lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Graces_(Raphael)&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Considérons les différents facteurs de multiplication inhérents à la sphère suprahumaine des Grecs.&lt;/p&gt;
&lt;ul style=&quot;margin-top: -5px;&quot;&gt;
    &lt;li&gt;Même si le terme de &lt;i&gt;polythéisme&lt;/i&gt; évoque d'abord des divinités, le monde suprahumain est peuplé de dizaine de &lt;b&gt;héros&lt;/b&gt;, destinataires d'hommages rituels ayant un ancrage local.&lt;/li&gt;
    &lt;li&gt;L'existence de dieux dont le &lt;b&gt;théonyme est un pluriel&lt;/b&gt; : les Moires, les Muses, les Heures, etc. Ces collectifs sont généralement honorés en tant que tels, mais peuvent être déployés en individualités (en nombre non fixé : les Charites sont 3 chez Hésiode, 2 à Sparte d'après Pausanias). On sait par ailleurs que les douze dieux d'Olympie étaient honorés par paires (Zeus &lt;i&gt;Olympios&lt;/i&gt;-Poséidon, Héra-Athéna, ...) dans certains sanctuaires ; le fait que l'une de ces paires soit constituée des Charites et de Dionysos contribue à asseoir l'unité des premières.&lt;/li&gt;
    &lt;li&gt;Le nom de certains dieux est régulièrement assorti d'une &lt;a href=&quot;https://fr.wikipedia.org/wiki/%C3%89picl%C3%A8se_(Antiquit%C3%A9)&quot;&gt;épiclèse&lt;/a&gt; (épithète permettant de préciser l'aspect spécifique de la divinité invoquée, dans une démarche cultuelle ; voir par exemple &lt;a href=&quot;http://www.brepols.net/Pages/ShowProduct.aspx?prod_id=IS-9782503516868-1&quot;&gt;&lt;i&gt;Nommer les dieux&lt;/i&gt;&lt;/a&gt;), comme le montre l'extrait précédent de l'&lt;i&gt;Enquête&lt;/i&gt;. Parmi les plus connus, on trouve Zeus &lt;i&gt;Olympios&lt;/i&gt; ou Athéna &lt;i&gt;Nikè&lt;/i&gt;. Chaque dieu est ainsi pluriel, avec des profils différents dans chaque cité.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quels liens peut-on alors tisser entre les différentes facettes d’un même dieu, Zeus ? Lorsque Hérodote précise que, malgré les trois épiclèses renvoyant à deux domaines de compétence différents, “c’était ce même dieu qu’il dénommait”, ce n’est pas parce que ses lecteurs pourraient penser qu’il s’agit de dieux différents, mais qu’ils pourraient considérer qu’il ne s’agit plus de Zeus (le théonyme non répété appelant une clarification formelle). Cette unité, si elle est mise à mal par les extraits suivants, résiste à une analyse &lt;em&gt;dans le contexte&lt;/em&gt; de ces derniers (&lt;strong&gt;“les dieux sont des &lt;em&gt;puissances&lt;/em&gt; et non des &lt;em&gt;personnes&lt;/em&gt;“&lt;/strong&gt; (J.-P. Vernant) &lt;strong&gt;, des entités dont les humains attendent qu’elles déploient leurs pouvoirs spécifiques à leur bénéfice&lt;/strong&gt;). Il s’agit de comprendre à quoi correspond chaque dénomination (regroupant plusieurs aspects ou facettes), par rapport aux autres divinités du panthéon.&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
εἰ μὲν οὖν μία ἐστὶν Ἀφροδίτη ἢ διτταί, Οὐρανία τε καὶ Πάνδημος, οὐκ οἶδα· καὶ γὰρ Ζεὺς ὁ αὐτὸς δοκῶν εἶναι πολλὰς ἐπωνυμίας ἔχει· ὅτι γε μέντοι χωρὶς ἑκατέρᾳ βωμοί τε καὶ ναοί εἰσι καὶ θυσίαι τῇ μὲν Πανδήμῳ ῥᾳδιουργότεραι, τῇ δὲ Οὐρανίᾳ ἁγνότεραι, οἶδα.
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
S’il existe une seule Aphrodite ou bien deux, Ourania et Pandèmos, je ne sais, car Zeus, qui paraît toujours le même, possède de nombreux noms. Ce que je sais, pourtant, c’est que, pour chacune des deux séparément, il existe des autels et des temples, et aussi des rites qui, pour la Pandèmos, sont pleins de relâchement, tandis qu’ils sont plus purs pour Ourania.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Xénophon, &lt;i&gt;Banquet&lt;/i&gt;, VIII, 9&lt;/span&gt;&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
τὰς Ἀφροδίτας – ἠ θεὸς γὰρ οὐ μία – ἠ Καστνιῆτις τῷ φρονεῖν ὑπερφέρει
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
Parmi les Aphrodites - car la déesse n'est pas une -, la Kastniètis l'emporte en sagesse.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Callimaque, &lt;i&gt;Iambes&lt;/i&gt;, 10&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;L’étude des οὐνόματα des dieux dans l’&lt;em&gt;Enquête&lt;/em&gt; renforce cette notion de puissance. Souvenons-nous en effet des différentes étapes décrites par Hérodote dans son histoire des dieux grecs.  Les dieux sont d’abord les &lt;strong&gt;θεοι indifférenciés&lt;/strong&gt; honorés par les Pélasges. Ils empruntent ensuite (aux Égyptiens) des dénominations, identificant ainsi les &lt;strong&gt;profils spécifiques&lt;/strong&gt; de certains dieux en leur attribuant des noms. Ce sont finalement les poètes qui donnent à ces profils des contours bien plus précis, en les inscrivant dans une véritable &lt;strong&gt;société divine&lt;/strong&gt; ; décrivant cette société comme régie par des règles de fonctionnement, des νόμοι, empruntées au monde des hommes, ils dotent ainsi les dieux d’une &lt;strong&gt;complexion humaine&lt;/strong&gt; (ἀνθρωποφυέας).&lt;/p&gt;

&lt;p&gt;Peu importe le crédit historique à apporter à ces 3 étapes ; ce qui est intéressant, c’est la façon dont Hérodote, témoin intérieur de la culture grecque, se représente le système religieux auquel il appartient. Et ce que l’on y trouve, c’est que &lt;strong&gt;les dieux sont d’abord des puissances, et que leur anthropomorphisme est surtout le fait des poètes qui leur ont appliqué des νόμοι humains&lt;/strong&gt;. La tension entre le général et le particulier est inhérente à la représentation des dieux grecs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;Νομίζειν-τοὺς-θεούς--reconnaître-et-honorer-les-dieux&quot;&gt;Νομίζειν τοὺς θεούς : reconnaître et honorer les dieux&lt;/h2&gt;

&lt;p&gt;Nous avons rencontré précédemment les νόμοι et le verbe νομίζειν, sans préciser explicitement ce que recouvrait ce dernier. Dans les passages de l’&lt;em&gt;Enquête&lt;/em&gt; que nous avons déjà analysés, il prend des sens différents et complémentaires : “considérer que”, “croire que” (proposition infinitive, cf ἐνόμισαν, I, 131) ; “avoir coutume de” (infinitif, cf νομίζουσι, I, 131) ; “avoir l’usage de” (avec un COD, cf νομίσαι, II, 4). Comment comprendre alors l’expression Νομίζειν τοὺς θεούς, qui apparaît régulièrement dans les textes ? C’est en effet dans la traduction que l’on donne de cette expression que se révèle la problématique de la croyance en Grèce antique. &lt;strong&gt;Peut-on traduire Νομίζειν τοὺς θεούς par “croire aux dieux” ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Replaçons cette problématique dans son contexte (grâce aux approfondissements &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL7087729268303567609_Pirenne_Cours_7_2018_Mars_15.pdf&quot;&gt;ici&lt;/a&gt;) afin de saisir son ampleur historiographique et les enjeux des débats. Nous l’avons vu, l’acception moderne du terme &lt;em&gt;religion&lt;/em&gt; nous est hérité d’une inflexion opérée par les penseurs chrétiens en latin, qui donnent à &lt;em&gt;religio&lt;/em&gt; le sens de “lien vertical personnel à un Dieu unique dont le message fonde toute autorité”. &lt;strong&gt;Cela transforme la portée du terme, puisqu’il recouvrait majoritairement, dans un contexte polythéiste, une &lt;em&gt;pratique&lt;/em&gt; plutôt qu’un &lt;em&gt;lien&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-durkheim.jpg&quot; alt=&quot;Portrait d'Émile Durkheim&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Émile Durkheim (&lt;a href=&quot;https://fr.wikipedia.org/wiki/%C3%89mile_Durkheim&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;L’acception chrétienne de la religion a pleinement intégré la notion de &lt;em&gt;croyance&lt;/em&gt;, et d’adhésion au message de Dieu, donnant un cadre à l’idée de &lt;em&gt;conversion&lt;/em&gt;. Dès lors, quand les sciences humaines en construction s’emparent de la notion de religion, au milieu du XIX&lt;sup&gt;e&lt;/sup&gt; siècle, elles associent à la fois pratiques et croyances à sa définition. Certains savants reconnaissent très vite la nécessité de nuancer ce propos : ainsi au tournant du XX&lt;sup&gt;e&lt;/sup&gt; siècle, &lt;a href=&quot;https://fr.wikipedia.org/wiki/%C3%89mile_Durkheim&quot;&gt;Émile Durkheim&lt;/a&gt;, père de la sociologie moderne, refuse de considérer que le divin est un élément constitutif de la religion, lui préférant l’idée de &lt;em&gt;choses sacrées&lt;/em&gt;. Il garde cependant en bonne place les croyances et les pratiques dans la définition de &lt;em&gt;religion&lt;/em&gt;, et explicite ce qu’il entend par celles-ci.&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Les phénomènes religieux se rangent tout naturellement en deux catégories fondamentales : les croyances et les rites. Les premières sont des états de l’opinion, elles consistent en représentations ; les secondes sont des modes d’action déterminés. Entre ces deux classes de faits, il y a toute la différence qui sépare la pensée du mouvement. [...]&lt;br /&gt;
Une religion est un système solidaire de croyances et de pratiques relatives à des choses sacrées, c’est-à-dire séparées, interdites, croyances et pratiques qui unissent en une même communauté morale, appelée Église, tous ceux qui y adhèrent.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Émile Durkheim, &lt;i&gt;Les formes élémentaires de la vie religieuse&lt;/i&gt; (1912), p. 92, 108-109&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Il faut attendre les années 1970 pour que s’enclenche une réflexion sur la notion de &lt;em&gt;croyance&lt;/em&gt; et sa pertinence en tant qu’outil (notamment avec l’ouvrage &lt;em&gt;Belief, Language and Experience&lt;/em&gt; de &lt;a href=&quot;https://fr.wikipedia.org/wiki/Rodney_Needham&quot;&gt;Rodney Needham&lt;/a&gt;, en 1972), sous l’impulsion des anthropologues qui ne cachent plus leur scepticisme. Pour nombre d’entre eux en effet, le terme de croyance ne correspond pas à ce qu’ils observent sur le terrain ; à l’inverse, pour les défenseurs du mot, on ne peut parler de religion sans croyance. &lt;strong&gt;Doit-on renoncer au champ sémantique de la croyance pour étudier la religion des Grecs ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Illustrée par l’essai &lt;em&gt;Les Grecs ont-ils cru à leurs mythes ?&lt;/em&gt; de &lt;a href=&quot;https://fr.wikipedia.org/wiki/Paul_Veyne&quot;&gt;Paul Veyne&lt;/a&gt; (1983), cette problématique s’est étendue des mythes à la croyance dans les dieux eux-mêmes. Ces travaux relèvent de différents points de vue. Ainsi, Paul Veyne exhibe différents régimes de vérité : les Grecs croyaient en leurs mythes, mais pas toujours de la même manière, en fonction du contexte. Mais lorsque Manuela Giordano-Zecharya &lt;a href=&quot;http://booksandjournals.brillonline.com/content/journals/10.1163/156852705774342824&quot;&gt;écrit&lt;/a&gt; &lt;em&gt;As Socrates shows, the Athenians did not believe in gods&lt;/em&gt;, ce n’est plus l’adhésion à des propositions cognitives (“Chronos a-t-il dévoré ses enfants ?”) qui est en jeu, mais &lt;strong&gt;la pertinence du fait de parler de croyance dans le cadre du polythéisme grec&lt;/strong&gt;, qu’elle nie ici. S’il y a généralement consensus sur &lt;strong&gt;l’absence de dogme&lt;/strong&gt; dans le polythéisme grec, sans révélation ni écritures sacrées, il est un peu extrême d’affirmer que les rituels se suffisent à eux-mêmes. Pour trouver un juste milieu, il s’agit de déterminer &lt;strong&gt;le rapport des Grecs à leurs dieux&lt;/strong&gt; : c’est ce que nous allons explorer dans le Νομίζειν τοὺς θεούς.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-mort_socrate.jpg&quot; alt=&quot;La mort de Socrate, par Jacques-Louis David&quot; class=&quot;center-image&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;&lt;i&gt;La mort de Socrate&lt;/i&gt;, par Jacques-Louis David, 1787 (&lt;a href=&quot;https://fr.wikipedia.org/wiki/%C3%89mile_Durkheim&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
L’accusation portée contre lui était formulée en ces termes: « Socrate est coupable de ne pas reconnaître les dieux reconnus par la cité, et d’introduire d’autres divinités, nouvelles ; il est aussi coupable de corrompre les jeunes gens. » (ἀδικεῖ Σωκράτης οὓς μὲν ἡ πόλις νομίζει θεοὺς οὐ νομίζων, ἕτερα δὲ καινὰ δαιμόνια εἰσφέρων· ἀδικεῖ δὲ καὶ τοὺς νέους διαφθείρων). Premièrement, pour ce qui est du fait qu’il ne reconnaissait pas les dieux reconnus par la cité (ὡς οὐκ ἐνόμιζεν οὓς ἡ πόλις νομίζει θεούς), de quelle preuve disposaient-ils donc ? Car on le voyait souvent faire des sacrifices à la maison, de même que sur les autels publics de la cité, et ce n’était pas non plus un secret qu’il avait recours à la divination.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Xénophon, &lt;i&gt;Mémorables&lt;/i&gt;, I, 1-2&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Cet extrait très célèbre de Xénophon montre encore différentes formes du verbe νομίζειν, et semble apporter de l’eau au moulin de Giordano (puisque seuls les rituels sont mentionnés pour la défense de Socrate). Il s’agit cependant d’être prudent, et d’examiner plus attentivement la disposition d’esprit induite par νομίζειν : nous l’avons vu, la variété des significations apparaissant dans l’&lt;em&gt;Enquête&lt;/em&gt; impose de considérer en première hypothèse à la fois la croyance (adhésion à une proposition formelle) et les pratiques (avoir l’usage de). L’étude de l’oeuvre de Pausanias offre alors un intérêt spécifique, pour trois raisons majeures :&lt;/p&gt;
&lt;ul style=&quot;margin-top: -5px&quot;&gt;
    &lt;li&gt;L'intérêt de l'auteur tant pour les vestiges de la vie religieuse du passé que pour l'actualité des rites de son époque&lt;/li&gt;
    &lt;li&gt;L'abondance du matériau disponible&lt;/li&gt;
    &lt;li&gt;L'émulation entre la démarche de Pausanias et l'&lt;i&gt;Enquête&lt;/i&gt; d'Hérodote, sept siècles plus tôt, qui lui servira de modèle.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
... Μαρα θωνίοις γάρ, ὡς αὐτοὶ λέγουσιν, ̔ Ηρακλῆς ἐνομίσθη θεὸς πρώτοις.&lt;br /&gt;
... parce que, selon leurs propres dires, Héraclès fut reconnu comme dieu par les Marathoniens les premiers.&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;

... σέβονται δὲ οἱ Μαραθώνιοι ... ̔Ηρακλέα, φάμενοι πρώτοις ̔Ελλήνων σφίσιν ̔Ηρακλέα θεὸν νομισθῆναι ...&lt;br /&gt;
... les gens de Marathon vénèrent Héraclès, disant qu'il fut reconnu comme dieu par eux, les premiers parmi les Grecs&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;

θεὸν δὲ ̓Αμφιάραον πρώτοις ̓Ωρωπίοις κατέστη νομίζειν, ὕστερον δὲ καὶ οἱ πάντες ῞Ελληνες ἥγηνται.&lt;br /&gt;
C'est chez les gens d'Oropos les premiers q'il fut établi de reconnaître Amphiaraos comme dieu, et ensuite tous les Grecs ont suivi.&lt;br /&gt;&lt;p style=&quot;margin-bottom: 0px&quot;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Pausanias, I, 15, 3 - 32, 4 - 34, 2&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;On voit là qu’Héraclès et Amphiaraos ont subi un &lt;strong&gt;changement de statut (vers le statut de dieux), qui justifie la traduction de &lt;em&gt;reconnaître&lt;/em&gt;&lt;/strong&gt;. Ce processus n’est pas réservé aux héros : Ilithyie, d’abord honorée par les Déliens (puis dont le nom ὄνομα est transmis aux autres comme chez Hérodote), est également &lt;em&gt;reconnue comme déesse&lt;/em&gt;. On constate par ailleurs dans la mise en scène des panthéons, tant chez Hérodote que chez Pausanias, que la dénomination d’un dieu est le &lt;em&gt;coeur&lt;/em&gt; de l’identification du profil divin. De même, &lt;strong&gt;la reconnaissance de ce profil est étroitement liée à l’établissement des honneurs dus à la figure divine&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Ce point éclaire la traduction de νομίζειν, qui devient, suivi d’un théonyme à l’accusatif, &lt;strong&gt;reconnaître et honorer&lt;/strong&gt;. Il entremêle ainsi les registres des représentations et des cultes, en explicitant l’inscription officielle d’un dieu dans les νόμοι du groupe qui l’accueille. C’est également, nous l’aurons compris, la traduction de &lt;strong&gt;νομίζειν τοὺς θεούς : reconnaître et honorer les dieux en s’inscrivant dans une tradition&lt;/strong&gt;. Il y a donc à a fois un processus cognitif et une démarche concrête ; c’est une combinaison, un phénomène que l’on retrouve de la même manière chez les Romains.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Revenons alors à la question qui nous a conduit ici : est-il pertinent de conserver la catégorie du &lt;em&gt;croire&lt;/em&gt; pour parler de religion grecque ? Tout dépend de ce que l’on entend dans le verbe &lt;em&gt;croire&lt;/em&gt;. L’acception moderne de &lt;em&gt;croyance&lt;/em&gt; tend en effet à désigner à la fois &lt;em&gt;ce que l’on croit&lt;/em&gt; et &lt;em&gt;le fait de croire&lt;/em&gt;, à la fois un contenu et l’adhésion à ce contenu, comme le montre &lt;a href=&quot;https://fr.wikipedia.org/wiki/Roberte_Hamayon&quot;&gt;Roberte Hamayon&lt;/a&gt; :&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Nous parlons aussi bien des croyances pour évoquer des conceptions religieuses (et l'emploi du pluriel est alors significatif) que de la croyance comme attitude mentale, psychique ou affective du sujet croyant. Et c'est cette dualité qui est source de paradoxe.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Roberte N. Hamayon, “L’anthropologie et la dualité paradoxale du ‘croire’ occidental”, &lt;i&gt;Revue du MAUSS&lt;/i&gt;, 20, p. 427-428, 2006&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;À l’inverse, &lt;strong&gt;de nombreuses cultures n’établissent pas un lien nécessaire entre ce que l’on croit (le contenu) et le fait de croire (l’adhésion à ce contenu)&lt;/strong&gt;. L’objet de la croyance est alors non pas exclusif, mais &lt;em&gt;négociable&lt;/em&gt;, flexible, pluriel, dans une attitude d’adhésion fluide. Il y a croyance, mais pas en un dieu ou ensemble de dieux fixes. Dans cette négociation avec des entités non-humaines, les actes rituels sont autant de mises à l’épreuve essentielles de l’objet de croyance, inscrites dans les νόμοι du groupe.&lt;/p&gt;

&lt;p&gt;C’est ce que Roberte Hamayon appelle la &lt;strong&gt;dynamique spéculative&lt;/strong&gt;, où l’objet de la croyance est dissocié du fait de croire. Cette dissociation est niée par les auteurs chrétiens qui choisissent de lier la &lt;em&gt;religion&lt;/em&gt; à la &lt;em&gt;vérité&lt;/em&gt; dans la &lt;em&gt;vera religio&lt;/em&gt;. Si la croyance chrétienne n’est pas toujours monolithique (comme en témoigne l’existence de quatre évangiles et de commentaires), l’Église s’impose comme institution forte, médiatrice entre les hommes (&lt;em&gt;croyants&lt;/em&gt;) et Dieu (&lt;em&gt;l’objet de croyance&lt;/em&gt;), renforçant le &lt;em&gt;lien&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Il n’y a pas, dans les religions grecques et romaines antiques, de croyance en ce sens, mais des représentations au sens de Durkheim ; ces représentations sont autant de manières d’identifier et de reconnaître les dieux que l’on honore et leurs manifestations. Les Grecs croyaient que les dieux étaient là et faisaient communauté avec les hommes ; les Grecs croyaient qu’il était bénéfique d’honorer les dieux. Il s’agit de la dimension cognitive du &lt;em&gt;croire que&lt;/em&gt;, qui laisse fondamentalement la place au doute quant à l’objet de la croyance. Ce doute enclenche la dynamique spéculative qui donne naissance à la fois aux représentations multiples et aux rituels, qui sont autant d’expérimentation de l’efficacité et de la validité de la représentation. C’est tout cela qui est contenu dans l’expression νομίζειν τοὺς θεούς.&lt;/p&gt;

&lt;p&gt;Il paraît donc nécessaire d’abandonner les termes de &lt;em&gt;croire à&lt;/em&gt;, &lt;em&gt;croire en&lt;/em&gt;, ou &lt;em&gt;croyants&lt;/em&gt; pour parler des religions grecques et romaines, sans pour autant les réduire à un ritualisme froid. Les Grecs pouvaient avoir &lt;em&gt;foi&lt;/em&gt;, &lt;em&gt;confiance&lt;/em&gt; dans les dieux qu’ils honoraient, mais &lt;em&gt;non pas avoir la foi&lt;/em&gt;. Dans la richesse de cette nouvelle acception de la relation aux dieux, &lt;strong&gt;les représentations et les hommages jouent un rôle majeur, sans que les Grecs puissent être qualifiés de croyants&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;Νομίζειν-τοὺς-θεούς--les-normes-sacrificielles&quot;&gt;Νομίζειν τοὺς θεούς : les normes sacrificielles&lt;/h2&gt;

&lt;p&gt;Après avoir étudié en détail la question des représentations, venons-en aux pratiques et aux sacrifices qui résident au coeur du dialogue instauré par les Grecs avec leurs dieux, à l’aide des approfondissements &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL5578843972916289128_Pirenne_Cours_8_2018_Mars_22.pdf&quot;&gt;ici&lt;/a&gt; et &lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL1196339589358700882_Pirenne_Cours_9_2018_Mars_28.pdf&quot;&gt;ici&lt;/a&gt;. L’articulation entre le général et le particulier concerne en effet l’ensemble du domaine religieux, et les rituels n’échappent pas aux questionnements précédents : &lt;strong&gt;quelle est, dans les pratiques religieuses, la part de ce qui unit les Grecs sur un arrière-plan partagé, et quelle est la part de ce qui les distingue ?&lt;/strong&gt; Si partage il y a, quel est le fondement de ce partage, en l’absence de toute autorité centralisée ?&lt;/p&gt;

&lt;p&gt;Comme pour les dieux de l’Olympe eux-mêmes, le rituel sacrificiel apparaît entièrement constitué dès l’Iliade et l’Odyssée, mettant en scène l’hommage aux dieux par la mise à mort d’un animal domestique. Les descriptions de ces sacrifices restent toutefois assez rares dans la tradition textuelle grecque, apparaissant surtout chez Homère, Aristophane et Ménandre. Quant au lien de ces descriptions à la réalité des sacrifices, plusieurs hypothèses s’offrent à nous :&lt;/p&gt;

&lt;ul&gt;
    &lt;li&gt;Ces descriptions sont un miroir à peine déformés de la réalité&lt;/li&gt;
    &lt;li&gt;Ces descriptions sont des constructions purement imaginaires, qui nous informent avant tout de l'intrigue dans laquelle elles prennent place&lt;/li&gt;
    &lt;li&gt;Le contrôle de l'auditoire de ces différents textes permet d'espérer saisir quelque chose de ce que les auteurs connaissaient des pratiques rituelles réelles &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Les nombreux points de convergence entre les différentes évocations, malgré les siècles d’écart, dressent une &lt;strong&gt;trame sacrificielle&lt;/strong&gt; en arrière-plan, c’est-à-dire &lt;strong&gt;la structure minimale associée au mot de &lt;em&gt;sacrifice&lt;/em&gt;, la θυσία&lt;/strong&gt;, quelque chose qui renvoie à la généralité de la pratique. Un extrait de l’&lt;em&gt;Enquête&lt;/em&gt; d’Hérodote offre un premier portrait en creux du sacrifice chez les Grecs, en contraste avec celui des Perses.&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Quant au sacrifice en usage chez les Perses pour les dieux que je viens de citer, voici ce qu’il est (θυσίη δὲ τοῖσι Πέρσῃσι περὶ τοὺς εἰρημένους θεοὺς ἥδε κατέστηκε). Ils ne dressent pas d’autels, ils n’allument pas de feu quand ils doivent offrir un sacrifice; ils n’usent ni de libations, ni de flûte, ni de bandelettes, ni d’orge sacrée. Un Perse veut-il sacrifier à l’un ou l’autre des dieux, il conduit la bête dans un lieu pur et il invoque le dieu (καλέει τὸν θεόν), portant sur sa tiare une couronne, de myrte de préférence. Il n’est pas permis à celui qui offre un sacrifice de se souhaiter du bien à lui seul en particulier ; il prie pour la prospérité de tous les Perses et du roi, lui-même étant compris dans l’ensemble des Perses. L’animal sacrificiel une fois découpé en menus morceaux et les chairs cuites (ἐπεὰν δὲ διαμιστύλας κατὰ μέρεα τὸ ἱρήιον ἑψήσῃ τὰ κρέα), il fait une litière d’herbe fraîche, de trèfle de préférence, et pose dessus tous les morceaux de viande. Lorsqu’il les a déposés, un mage, qui est là présent, psalmodie une théogonie (μάγος ἀνὴρ παρεστεὼς ἐπαείδει θεογονίην) – telle est d’après ce qu’ils disent la nature de ce chant – car la règle est chez eux de ne pas offrir de sacrifice sans un mage (ἄνευ γὰρ δὴ μάγου οὔ σφι νόμοςἐστὶ θυσίας ποιέεσθαι). Après quelques instants d’attente, celui qui a sacrifié emporte chez lui les viandes (ἀποφέρεται ὁ θύσας τὰ κρέα), et en use selon sa 
volonté.
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Hérodote, &lt;i&gt;Enquête&lt;/i&gt;, I, 132&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;La mise en regard de cet extrait avec les sacrifices décrit dans les comédies et les épopées permet d’esquisser le déroulement suivant, qui serait commun à tous, inscrit dans les νόμοι :&lt;/p&gt;

&lt;ol&gt;
    &lt;li&gt;Feu à l'autel en vue d'une combustion&lt;/li&gt;
    &lt;li&gt;Agent cultuel facultatif&lt;/li&gt;
    &lt;li&gt;Sacrifiants couronnés&lt;/li&gt;
    &lt;li&gt;Conduite de l'animal à l'autel, avec bandelette&lt;/li&gt;
    &lt;li&gt;Musique d'&lt;i&gt;aulos&lt;/i&gt;&lt;/li&gt;
    &lt;li&gt;Libations&lt;/li&gt;
    &lt;li&gt;Jet d'orges rituels&lt;/li&gt;
    &lt;li&gt;Invocation et prière, tant pour l'individu que pour la communauté&lt;/li&gt;
    &lt;li&gt;Mise à mort&lt;/li&gt;
    &lt;li&gt;Découpe en parts&lt;/li&gt;
    &lt;li&gt;Déposition de parts à l'autel, et éventuellement sur table&lt;/li&gt;
    &lt;li&gt;Enlèvement des parts restantes, et libre disposition de la viande (à cuire sur place ou à emporter)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Mais au-delà de la description des différentes étapes, il s’agit pour nous de déterminer le coeur, le noyau du sacrifice grec dans sa spécificité. &lt;strong&gt;Quelle est la manière grecque de sacrifier ?&lt;/strong&gt; Pour répondre à cette question, il nous faut quitter les textes littéraires pour nous plonger dans une autre source de documentation : les inscriptions et prescriptions gravées sur divers supports, pierre ou métal, par différents groupes sacrifiants. &lt;strong&gt;Si les textes &lt;em&gt;littéraires&lt;/em&gt; nous permettent d’atteindre un certain degré de généralité dans la structure du sacrifice, peut-on déceler un tel arrière-plan dans la documentation épigraphique, ou n’y trouve-t-on que des variations sur la trame ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-stele_larisa.png&quot; alt=&quot;Stèle de Larisa&quot; class=&quot;center-image&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Stèle découverte à Larisa (Thessalie) en 2002, datée du II-III&lt;sup&gt;e&lt;/sup&gt; siècle avant notre ère (&lt;a href=&quot;https://www.college-de-france.fr/media/vinciane-pirenne-delforge/UPL5578843972916289128_Pirenne_Cours_8_2018_Mars_22.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Cette stèle de Larisa, découverte dans une décharge de gravillons, mentionne un sanctuaire (dont elle est probablement originelle) et une déesse centrale qui reste anonyme, entourée d’Artémis, d’autres dieux grecs, Men, et de dieux non grecs dont les noms ont été ou non traduits. Ce document majeur souligne la coexistence de groupes Grecs et non-Grecs dans la Grèce hellénistique. Sur la face la mieux conservée (B), on trouve une liste de consignes pour la pratique du sacrifice dans le sanctuaire en question, avec entre autres des “Prescriptions pour le sacrifice à la déesse &lt;strong&gt;selon la norme grecque&lt;/strong&gt;”. Cela en fait une source sans précédent dans les inscriptions pour comprendre la manière grecque de sacrifier :&lt;/p&gt;

&lt;!-- &lt;ul&gt;
    &lt;li&gt;Inscription pour le péristyle : offrandes préliminaires à Phylakè et Mên&lt;/li&gt;
    &lt;li&gt;Prescription pour un sacrifice de volailles blanches ou d'agneaux&lt;/li&gt;
    &lt;li&gt;Prescriptions pour le sacrifice à la déesse &lt;b&gt;&quot;selon la norme grecque&quot;&lt;/b&gt;&lt;/li&gt;
    &lt;li&gt;Prescriptions pour le rituel &quot;de la table pleine&quot; pour la déesse&lt;/li&gt;
    &lt;li&gt;Presciptions pour le sacrifice d'une volaille ou d'une oie&lt;/li&gt;
    &lt;li&gt;Prescriptions pour le sacrifice d'un bovin&lt;/li&gt;
    &lt;li&gt;Prescriptions pour l'holocauste d'un bélier adulte, ou à défaut, d'un agneau mâle&lt;/li&gt;
    &lt;li&gt;Prescriptions pour l'holocauste d'une oie, d'un oiseau &lt;i&gt;trybba&lt;/i&gt;, d'une caille&lt;/li&gt;
&lt;/ul&gt;
 --&gt;

&lt;div class=&quot;centeredquote&quot; style=&quot;text-align: justify;&quot;&gt;
Ἐὰν δέ τις &lt;b&gt;θύειν&lt;/b&gt; βούληται &lt;b&gt;τῆι θεῶι ἑλληνικῶι νόμωι&lt;/b&gt;, ἔ̣ξεστι ὃ τι ἂμ βούληται πλὴν χοίρου. Ἐπὶ δὲ τῆ[ι] θυσίαι, φέρειν δεῖ ἐπὶ τὴν τραπέζαν τὰ ἐπιτιθέμενα χοίνικα λαγάνων, ὅμορας χοίνικα, καὶ τριώβολον εἰς θησαυρὸν καὶ ἐλαίου ἐπὶ λύχνον κοτύλην καὶ εἰς κρατῆρα οἴνου χοᾶ.ἀπὸ τοῦ ἱεροῦ, τὸ στῆθος ἑφθὸν ἐπὶ τὴν τραπέζαν καὶ τὸ σκέλος ὠμόν. τῆι ἱερείαι τὰ σπλάγχνα ἕψειν, ἧπαρ καὶ̀ πνεύμονα καὶ φρενὰς καὶ νεφρὸν ἀρίστερον καὶ γλῶσσαν· τὸν δὲ δεξιὸν νεφρὸν καὶ ἀκροκόλιον δεξιὸν καὶ καρδίαν καὶ ἐπίπλουν καὶ τὸ σκέλο[ς] τὸ ἀπὸ τοῦ στήθους καὶ τῆς κέρκου τὸ νομιζόμενον εἰς ἱερὰ ἐπὶ τὸ πῦρ.
&lt;br /&gt;&lt;p style=&quot;margin-top: -5px&quot;&gt;&lt;/p&gt;
Si quelqu’un souhaite sacrifier à la déesse selon la norme grecque, c’est possible avec ce que l’on veut, sauf du porc. Pour ce sacrifice, il faut apporter sur la table comme déposition : un chénicede lagana, un chénice d’homora, trois oboles pour le tronc à offrande, un cotyle d’huile pour lampe et, pour le cratère, un conge de vin; de l’animal sacrificiel, sur la table, la poitrine cuite et la patte crue ; pour la prêtresse, faire cuire les viscères, le foie, le poumon, le diaphragme, le rein gauche et la langue ; le rein droit, les extrémités droites, le cœur, l’omentum, la patte détachée de la poitrine et, de la queue, ce qui est d’usage, comme offrandes sacrées sur le feu. 
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Stèle de Larisa, B 34-42&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;De ce passage, on peut extraire que la “norme grecque”, telle qu’opposée à une autre norme non explicitée, comprend l’interdit du porc, la fourniture d’aliments et la déposition de parts sur la table, la cuisson de viscères en lien avec la prêtresse, parts à faire brûler à la flamme de l’autel. La construction, dans le texte, de l’&lt;strong&gt;opposition à cette autre norme, probablement moyen-orientale&lt;/strong&gt;, fait également ressortir les prescriptions pour le sacrifice d’un bovin. Celles-ci précisent des attentes quantitatives additionnelles pour cet animal plus massif (davantage de bois, de vin, d’huile, d’or), mais aussi et surtout, elles énoncent que le traitement des &lt;em&gt;parts sacrées&lt;/em&gt; doit être le même que pour le petit bétail ; cela signifie qu’une grande quantité de viande est brûlée à la flamme de l’autel, contrairement à ce que décrivait par exemple Aristophane.&lt;/p&gt;

&lt;p&gt;L’une des très rares mentions dans la littérature de cette expression de &lt;em&gt;manière grecque de sacrifier&lt;/em&gt; apparaît chez Pausanias, au livre I. Phrixos, “amené en Colchide [NB: territoire non grec] par le bélier [NB: à la toison d’or]”, sacrifie ce dernier et “ayant découpé les cuisses selon la norme des Grecs, il les regarde brûler.” Pausanias met ainsi en évidence la coutume grec de &lt;strong&gt;prélèvement et combustion d’une part de l’animal lors du sacrifice&lt;/strong&gt;, opposée à la tradition moyen-orientale attestée de combustion intégrale. Il apparaît donc qu’une &lt;em&gt;norme sacrificielle grecque&lt;/em&gt; existe bien, notamment en contraste avec celles des autre peuples où la déposition des parts n’implique aucune combustion : &lt;strong&gt;la manière grecque implique de faire se consumer &lt;em&gt;une partie de la viande&lt;/em&gt; à la flamme de l’autel.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Ayant dressé cet arrière-plan commun, il est désormais temps d’explorer la diversité des coutumes sacrificielles singulières apparaissant dans le corpus épigraphique. Mentionnons pour commencer qu’un tel corpus est disponible en accès libre grâce au projet &lt;a href=&quot;http://cgrn.ulg.ac.be/&quot;&gt;CGRN&lt;/a&gt; (Collection of Greek Ritual Norms), regroupant un ensemble de plus de 220 prescriptions et inscriptions sur les normes sacrificielles. Si ces prescriptions rituelles épigraphiques ont été, plus d’un siècle durant, regroupées sous le terme de &lt;em&gt;lois sacrées&lt;/em&gt;, on lui préfère aujourd’hui celui de &lt;em&gt;norme(s)&lt;/em&gt;. Cette notion permet en effet de rendre compte à la fois de l’importance de la coutume traditionnelle orale, et du caractère prescriptif de décisions collectives dont le statut n’est pas forcément ni “legal” ni “sacré”. Ajoutons à cette considération celle de la portée globale, du cadre de référence du sacrifice grec, la θυσια. &lt;strong&gt;Le sacrifice, quelle que soit la demande particulière associée, est une manière d’entrer en communication avec les dieux ; c’est une sorte de langage&lt;/strong&gt;. Pour filer la métaphore, on pourrait dire que la trame tissée précédemment correspond à la &lt;em&gt;langue&lt;/em&gt;, les motifs spécifiques et locaux à la &lt;em&gt;parole&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-10-15-sacrifice_visceres.jpg&quot; alt=&quot;Scène de sacrifice avec viscères embrochées&quot; class=&quot;center-image&quot; width=&quot;700px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Scène de sacrifice où les viscères, embrochées, grillent sur l’autel enflammé. &lt;br /&gt;Cratère du peintre de Pothos, 430-420 av. J.-C., Musée du Louvre (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Rites_de_la_religion_grecque_antique#/media/File:Sacrifice_Pothos_Painter_Louvre_G496.jpg&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;La matière épigraphique étant abondante (plus de 500 textes), choisissons de nous pencher plus particulièrement sur l’une des composantes majeures de la découpe en parts dans le sacrifice grec : &lt;strong&gt;les viscères, ou σπλάγχνα&lt;/strong&gt;. Elles étaient déjà mentionnées sur la stèle de Larisa, comme &lt;em&gt;parts sacrées&lt;/em&gt; ou ἱερα, indiquant leur situation intermédiaire, sur la voie entre monde humain et monde divin. Lors du sacrifice, une fois que les viscères sont grillées, et que le sacrifiant y goûte, il y a symboliquement &lt;strong&gt;commensalité&lt;/strong&gt; (fait de manger ensemble) et communication avec le destinataire divin (censé se repaître des fumées de combustion).&lt;/p&gt;

&lt;p&gt;Ces σπλάγχνα apparaissent surtout dans des documents réglant la vente des sacerdoces dans certaines cités des îles de l’Égée ou de la côte d’Asie mineure, par exemple à Chios, Milet, ou Halicarnasse.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="College de France" /><category term="Religion" /><category term="Histoire" /><category term="Antiquite" /><category term="Notes de cours" /><summary type="html"></summary></entry><entry><title type="html">Building bridges - AMSGrad, brittle NMT, MSDNet</title><link href="http://localhost:4000/Mixing-AMSGrad-brittleNMT-MSDNet/" rel="alternate" type="text/html" title="Building bridges - AMSGrad, brittle NMT, MSDNet" /><published>2018-09-24T01:00:00+02:00</published><updated>2018-09-24T01:00:00+02:00</updated><id>http://localhost:4000/Mixing-AMSGrad-brittleNMT-MSDNet</id><content type="html" xml:base="http://localhost:4000/Mixing-AMSGrad-brittleNMT-MSDNet/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;building-bridges---first-article&quot;&gt;Building bridges - First article&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;In this series, I try to mix the original ideas of several papers, in order to gain new insights or design concepts that are new to me. I do not claim that those are original or new to the community, as I may reinvent the wheel along the way. It is also possible that I unintentionally copy concepts that were already developped by somebody else.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is based on the papers:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=ryQu7f-RZ&quot;&gt;On the convergence of Adam and beyond&lt;/a&gt; &lt;em&gt;by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=BJ8vJebC-&quot;&gt;Synthetic and natural noise both break Neural Machine Translation&lt;/a&gt; &lt;em&gt;by Yonatan Belinkov and Yonatan Bisk.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=Hk2aImxAb&quot;&gt;Multi-Scale Dense Networks for resource efficient image classification&lt;/a&gt; &lt;em&gt;by Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten and Kilian Weinberger.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;key-insights-from-each-paper&quot;&gt;Key insights from each paper&lt;/h2&gt;

&lt;p&gt;Let us break down each paper into key insights, which will help us combining them later on.&lt;/p&gt;

&lt;h4 id=&quot;on-the-convergence-of-adam-and-beyond&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=ryQu7f-RZ&quot;&gt;On the convergence of Adam and beyond&lt;/a&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Adam’s proof is flawed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The most popular optimization algorithm in deep learning does not correctly converge in all settings.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The fact that Adam does not always converge does not matter too much&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adam does not always converge, since the proof wrongly assumed that the learning rate was non-increasing. A non-increasing learning rate seem to be required for convergence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The online optimization framework, where you minimize the regret, is equivalent (algorithms can be transposed) to the usual loss minimization framework.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adagrad works in theory, but remembers in practice too much of the past (average of all previous squared gradients), which shrinks the learning rates to zero too quickly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adam doesn’t work in theory, but works in practice (using an exponentially moving average / fixed window of past squared gradients), keeping reasonable learning rates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adam sometimes converges to the worst solution possible, and we generally do not care.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Adam forgets huge rare gradients, and favors small frequent ones.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Online convex optimization is harder than stochastic convex optimization.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You want a non-increasing quantity? Cap it using the max of previous values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you cheat with one quantity along the way, and it might bias your computations, then run the true computations in a hidden way, and compute the “cheated” quantities using the true ones.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We could quickfix Adam’s proof&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use sparse settings as soon as possible&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Independent implementation of paper results may disprove (performance) claims&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It takes a long time to check the math, even in optimization&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;synthetic-and-natural-noise-both-break-neural-machine-translation&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=BJ8vJebC-&quot;&gt;Synthetic and natural noise both break Neural Machine Translation&lt;/a&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Data matters&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We are unable to automatically generate quality natural noise&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are many noise types that humans are able to easily overcome&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NMT is very brittle&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Precision essentially measures the proportion of words of the candidate translation that actually appear in the reference translation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recall can be artificially inflated when considering multiple references&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sometimes it is better to consider the n-grams rather than raw words&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If a model is biased, penalize it&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Going form word embeddings to character embeddings is important, e.g. to reduce the storage need of the embedding matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;char2char accesses a sequence of characters embeddings (including spaces), Nematus accesses a sequence of sub-word units, charCNN accesses a sequence of words as sequences of character embeddings in an ordered structure (CNN).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SotA NMT models are unable to translate noisy texts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;System-breaking noise doesn’t need huge modifications, swap has the same effect than random scrambling.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pay attention to variance/invariance to effects, sensitivity/insensitivity to structure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;meanChar removes all information about structure, but also potentially a large amount of information about presence&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Graphs can flexibly retain or forget structure (using weighted edges on a complete graph), but retain presence given the importance of nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If the model fails on some inputs, train it on them&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training on a mix of noises is OK&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A convolution that relies on no structure cannot be anything else than averaging.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Have a look at the two pictures below. It probably took you an unnoticeable amount of time to recognize a horse on the left, and a very noticeable amount of time (say one second) to recognize a horse on the right. Naturally, we would expect models to face similar difficulties in classification of those images. Intuitively, it feels like a simple CNN with a couple layers (e.g. AlexNet) would be more than enough to classify the first picture, while the last one shall require a much more complex model for correct classification, being in the &lt;em&gt;tail&lt;/em&gt; of the “horse images” distribution (hence requiring a more precise approximation of this distribution by a neural network).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-horses.png&quot; alt=&quot;Two horses&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using the same model to classify both pictures generally means that you have to choose &lt;strong&gt;beforehand, and once and for all&lt;/strong&gt; (when you implement the model), between low resource consumption and high accuracy. In other words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Computationally intensive models are needed to classify such tail examples correctly, but are wasteful when applied to canonical images such as the left one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, generally speaking, we computer users don’t care, or at least this is not a question that we are used to ask. In the rare situations where we actually care about resource consumption (most of all about speed), we just define a minimum acceptable inference speed, pick the best performing model that satisfies this constraint, and that’s basically it. However, &lt;strong&gt;we phone users and (future) Internet of Things users are very much likely to &lt;a href=&quot;https://www.iotforall.com/computer-vision-iot/&quot;&gt;care&lt;/a&gt;&lt;/strong&gt;. From photo deblurring to real-time action recognition, computer vision &lt;em&gt;will&lt;/em&gt; become ubiquitous in everyday devices, that run on much lower resources than modern computers. Moreover, lower computational cost means &lt;strong&gt;lower time energy consumption&lt;/strong&gt;, which is highly desirable for &lt;strong&gt;ecological&lt;/strong&gt; (and economical) reasons.&lt;/p&gt;

&lt;p&gt;All in all, it feels frustrating not to recognize a horse on the right, so we use the winner of ImageNet 2017 (an ensemble of &lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot;&gt;Squeeze-and-Excitation Networks&lt;/a&gt;); but it is ridiculous to waste &lt;a href=&quot;https://github.com/albanie/convnet-burden&quot;&gt;440 MB of parameters and 21 GFLOPs&lt;/a&gt; on recognizing a horse on the right while we probably only need 50 times less resources to do it (as a reference, Apple’s iPhone 4 had a processor power of about 3 GFLOP/s, so it would actually take 7 seconds at 100% CPU to process each image). We feel torn between both issues, and nobody likes feeling &lt;a href=&quot;https://music.youtube.com/watch?v=rIGAt5yvfmw&amp;amp;list=RDAMVMrIGAt5yvfmw&quot;&gt;torn&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This begs the question: &lt;strong&gt;why do we choose between either wasting computational resources by applying an unnecessarily computationally expensive model to easy images, or making mistakes by using an efficient model that fails to recognize difficult images?&lt;/strong&gt; Ideally, our systems should automatically use small networks when test images are easy or computational resources limited, and use big networks when test images are hard or computation is abundant.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is why the authors propose the Multi-Scale Dense Network model (architecture below), which we will explore, that enables adaptive resource allocation for image classification thanks to the introduction of early classifiers in a feed-forward CNN structure. This way, easy images can be instantly classified, and harder ones can use more computational resources. Let’s see how this works on real-world tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-MSDNet_architecture.png&quot; alt=&quot;MSDNet architecture&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;inpost-figure-caption-centered&quot;&gt;The Multi-Scale Dense Network (MSDNet) architecture&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;real-word-tasks---classification-on-a-tight-budget&quot;&gt;Real-word tasks - classification on a (tight) budget&lt;/h2&gt;

&lt;p&gt;Let’s first define the situations where a resource-aware model is likely to be more useful than an off-the-shelf CNN. This will help us understand where and when exactly the new architecture helps. Computationally constrained tasks are numerous and diverse, and we will only focus on two major problems: anytime prediction, and budgeted batch classification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anytime-prediction&quot;&gt;Anytime prediction&lt;/h3&gt;

&lt;p&gt;In &lt;strong&gt;anytime prediction&lt;/strong&gt;, the model can be forced to output a prediction, at any given point, possibly before the full computation is complete. Good performance levels are typically achieved by models that are able to give crude estimates very quickly, and refine them with time until the full model is run. For example, imagine an autonomous car equipped with a network to detect and handle obstacles on the road. You want your car to &lt;em&gt;instantly&lt;/em&gt; detect and react to a pedestrian suddenly appearing in front of the car; there is no time to decide the precise distance at which it appears, whether it is an adult or a child, and at which speed it is going towards you. On the other hand, for distant and long-term obstacles, determining the precise distance between you and them enables better planning, smooth trajectories, better fuel management, etc.&lt;/p&gt;

&lt;p&gt;In a formal way, we assume that test samples $x$ and budgets $B$ are drawn from a joint distribution $P(x, B)$. The model outputs a prediction $f(x)$ within the computational budget $B$, and incurs a loss $L(f(x), B)$. The goal in anytime prediction is to find a model that minimizes the expected loss of individual prediction within (hard) budget constraint:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f \ \mathbb{E}_{P(x, B)} [L(f(x)), B]&lt;/script&gt;

&lt;p&gt;Another example of anytime prediction is real-time video classification. You are filming a scene, and you want your phone to identify the various elements present in the video while filming. It is not unreasonable to ask for a refresh rate of 10 Hz, which means a prediction every 0.1 s, &lt;em&gt;whatever the computational budget available on your phone at that time&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;budgeted-batch-classification&quot;&gt;Budgeted batch classification&lt;/h3&gt;

&lt;p&gt;In &lt;strong&gt;budgeted batch classification&lt;/strong&gt;, the model is granted a finite known computational budget to classify a set of examples, and can spend it freely across examples. Good performance levels are typically achieved by models that are able to quickly classify easy examples (left horse), in order to save some additional computation for harder examples (right horse). For example, imagine that you want to show your best friend all the pictures on your phone where both of you are present. Some pictures will be easily classified (you both clearly face the camera, no other people present), some will be much harder (you are in a crowd, disguised, wearing make-up or making funny faces). You don’t care much about that, and you only want your phone to give you a decent search result in less than 5 seconds.&lt;/p&gt;

&lt;p&gt;Formally, we consider a set of examples $\mathcal{D}_ {test} = \{ x_1, …, x_M \}$ and a computational budget $B$ that is known in advance. The model spends $B$ as it pleases across examples, outputs a set of predictions $f(\mathcal{D}_ {test})$ and incurs loss $L(f(\mathcal{D}_{test}), B)$. The goal in budgeted batch classification is to find a model that minimizes the expected loss of batch prediction within (soft) budget constraint:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f \ \mathbb{E}_{P(x)} [L(f(\mathcal{D}_{test})), B]&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-problems-with-early-classification&quot;&gt;The problems with early classification&lt;/h2&gt;

&lt;p&gt;Let us think it through. The simplest, most natural answer to address both these situations is to use multiple networks with increasing capacity (e.g. multiply the number of layers by a constant factor from one model to the next one), and evaluate them sequentially at test time. In &lt;em&gt;anytime prediction&lt;/em&gt;, you simply output the prediction of the last network evaluated; in &lt;em&gt;batch budgeted classification&lt;/em&gt;, you stop the evaluation once classification with sufficient confidence level is reached. This is illustrated in the next figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-model_sequence.png&quot; alt=&quot;Model sequence&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt; A “sequence of models” solution, featuring AlexNet (A), GoogLeNet (G) and ResNet (R). Green $\gamma$ blocks denote selection policies. The input is first evaluated by AlexNet, and the selection policy determines whether evaluation by more complex models is needed. (&lt;a href=&quot;https://arxiv.org/pdf/1702.07811.pdf&quot;&gt;Source&lt;/a&gt;) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The problem with this approach is that, when the first network isn’t confident enough, we switch to the second network without re-using any feature previously computed: for complex examples, we completely &lt;strong&gt;waste&lt;/strong&gt; the computational budget spent on the first networks. This is quite unsatisfying, and far from optimal.&lt;/p&gt;

&lt;p&gt;Then the opposite solution comes in mind: instead of building multiple networks with one classifier each, not sharing any feature, we could build one network as a cascade of multiple (early) classifiers along depth, re-using previous features to build the more advanced predictions. This would look like the next figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-classifiers_cascade.png&quot; alt=&quot;Cascade&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;A simple “cascade of classifiers” solution on a standard CNN architecture. Early classification of easy examples yields substantial savings in computational budget, that can be spend on the hard examples. (&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf&quot;&gt;Source&lt;/a&gt;) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Although this model doesn’t waste any feature, it leads to poor performance, for two distinct reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Early classifiers lack coarse-level, global features&lt;/strong&gt; - only fine-scale, local features are available at early stages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Early classifiers interfere with later classifiers&lt;/strong&gt; - early classifiers tend to optimize the early features for the short-term, conflicting with the long-term optimization of late classifiers, that achieve better performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two issues will drive the design of MSDNet through the inclusion of two specific components, each addressing one issue.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;early-classifiers-vs-dense-connections&quot;&gt;Early classifiers vs. dense connections&lt;/h3&gt;

&lt;p&gt;Let us take a common CNN architecture, &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt;, and attach an intermediate classifier at a (more or less) early stage of the architecture. We then train both (final and intermediate) classifiers jointly (here on CIFAR-100), weighting their losses equally, and look at the accuracy of the final classifier. If there a noticeable difference in performance with the standard setting, the presence of the intermediate classifier is likely to have an influence on the construction of the features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-final_acc_when_intermed.png&quot; alt=&quot;Final accuracy when intermediate classifier is attached&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As is clear from the figure, ResNet performance generally suffers a lot from the introduction of an intermediate classifier, especially at very early stages.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We postulate that this accuracy degradation in the ResNet may be caused by the intermediate classifier influencing the early features to be optimized for the short-term and not for the final layers. This improves the accuracy of the intermediate classifier but collapses information required to generate high quality features in later layers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This sounds like a reasonable and likely hypothesis. It would be interesting to visually examine the filters learned and the corresponding features for different locations of the intermediate classifier, providing us with some insights in this regard.&lt;/p&gt;

&lt;h4 id=&quot;solutionmitigation-dense-connections&quot;&gt;Solution/Mitigation: Dense connections&lt;/h4&gt;

&lt;p&gt;To mitigate this problem, the paper cites dense connections as an interesting line of work. Dense connections were introduced by our authors one year earlier in &lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNets&lt;/a&gt;, as a generalization of residual connections, the building blocks of ResNets. Remember what a residual connection looks like? A residual block is displayed in the next figure: the signal can bypass the layer thanks to an identity connection, and addition with the layer’s output.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-residual_block.png&quot; alt=&quot;Residual block&quot; class=&quot;center-image&quot; /&gt;
&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;A residual block, the building foundation of ResNets. (&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Dense connections go one step further by connecting each layer directly with &lt;em&gt;all previous layers&lt;/em&gt; (inside the same block). What’s more, instead of being summed, the previous features are concatenated to enable direct re-use. The resulting dense block is illustrated in the next figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-dense_block.png&quot; alt=&quot;Dense block&quot; class=&quot;center-image&quot; /&gt;
&lt;span class=&quot;inpost-figure-caption&quot;&gt;A dense block, the building foundation of DenseNets. At each stage, the features of all previous layers are concatenated to maximize information flow and allow layer bypassing as much as possible. (&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Now how will this help us? Compared to ResNets, DenseNets suffer much less from the introduction of intermediate classifiers at early levels (see the figure some blocks above). This is likely linked to the fact that the signal can bypass all layers, so that no layer results in a loss in information. Should an early layer get optimized for short-term classification, the original signal can still be recovered unperturbed by later layers. This greatly alleviates the influence between short-term and long-term optimization, and makes the final accuracy of DenseNets not too dependent on the location of the intermediate classifier, yielding a nice candidate to support early classifiers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;coarse-features-vs-multiple-scales&quot;&gt;Coarse features vs. multiple scales&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-feature_levels.png&quot; alt=&quot;Feature levels&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;Visualization of the features learned by a convolutional network along its depth, using DeconvNet. Low-level features match local and simple patterns, while higher level features retain incresingly global and complex patterns. (&lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;You’ve probably seen this picture a couple times already, or one alike. This visualization of the features learned by the network along the depth was obtained thanks to a &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;visualization DeconvNet&lt;/a&gt;. It supports the claim that first layers correspond to local, fine-scale features (the feature maps learned are close to the size of the original image), whereas deep layers correspond to global, coarse-scale features (the feature maps learned are much smaller than the original image, condensing global information such as “there is a dog in this region”).&lt;/p&gt;

&lt;p&gt;Now this is a problem for early classifiers. Image classification is much harder when you only have access to local features, matching only simple patterns such as straight lines in different directions, or circles of different sizes. To confirm this intuition, let us consider again our previous experiment: take a ResNet, attach an intermediate classifier at different locations along the depth of the network, but this time examine how this location influences the accuracy of the &lt;em&gt;intermediate&lt;/em&gt; classifier. If classification based on local, fine-scale features was as easy as classification based on global, coarse-scale features, the impact on accuracy should not be too noticeable (and, as a side note, we woudln’t need deep learning to begin with).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-intermed_acc_when_intermed.png&quot; alt=&quot;Intermediate accuracy&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It hurts. Pretty badly. DenseNets are once again less sensitive to this effect, but the performance of the intermediate classifier is still unsatisfying.&lt;/p&gt;

&lt;h4 id=&quot;solutionmitigation-multi-scale-feature-maps&quot;&gt;Solution/Mitigation: Multi-scale feature maps&lt;/h4&gt;

&lt;p&gt;To mitigate this harmful effect, the authors suggest maintaining feature maps of multiple scales at all layers. This is pretty annoying to describe with words, so let’s jump into the architecture to see how it is implemented.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-architecture&quot;&gt;The architecture&lt;/h2&gt;

&lt;p&gt;As expected, the resulting design combines dense connections with multi-scale feature maps to yield the architecture of the Multi-Scale DenseNet, taking the following form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-MSDNet_architecture.png&quot; alt=&quot;MSDNet&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;The MSDNet architecture. Each layer maintains feature maps at multiple scales, enabling early classifiers to operate on coarse-level features. The feature maps at a given scale and layer are the concatenation of two elements: the result (in blue) of strided convolution on the finer-scale features from the previous layer, and the result (in red) of regular convolution on the same scale features from multiple previous layers &lt;em&gt;via&lt;/em&gt; dense connections. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This is not your typical CNN architecture, so let’s break it down bit by bit: first the layers and the connections, then the classifiers and their repartition, and finally the training details and the loss function.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;layers&quot;&gt;Layers&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-layers_details.png&quot; alt=&quot;MSDNet layers details&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As described in the figures, the model is structured as a 2-dimensional grid $S \times L$, with $L$ the number of layers (analogous to the depth in standard networks) and $S$ the number of scales at each layer.&lt;/p&gt;

&lt;p&gt;The first layer $l=1$ is quite unique, since it “seeds” all scales $s = 1, …, S$ of the feature maps. The finer scale $s=1$ is the result of a regular convolution $h$ on the original image. For subsequent scales, the feature map at scale $s+1$ is the result of strided convolution $\tilde{h}$ (which is just really regular convolution with a stride of at least 2) to the features at scale $s$. Given that strided convolution results in downsampling of the input (like max pooling), the feature map at scale $s+1$ is smaller than the one at scale $s$. This enables coarser-level, global information to be encoded already in the first layer.&lt;/p&gt;

&lt;p&gt;The next layers maintain all scales that were previously seeded. The output of layer $l$ at scale $s$ is the concatenation of two elements:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The result of regular convolution $h$ on all previous feature maps (dense connection) at scale $s$ from layers $l’=1, …, l-1$ (in red).&lt;/li&gt;
  &lt;li&gt;The result of strided convolution $\tilde{h}$ on all previous feature maps (dense connection) at scale $s-1$ from layers $l’=1, …, l-1$ (in blue, performing the downsampling of finer-scale features that is common in CNNs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;classifiers&quot;&gt;Classifiers&lt;/h4&gt;

&lt;p&gt;Following the same principle of dense connections, the classifiers take as input the concatenation of all coarse feature maps from all previous layers. In practice they are only attached to some intermediate layers, and their behavior depends on the task at hand:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In &lt;em&gt;anytime prediction&lt;/em&gt;, the classifiers are evaluated sequentially until the budget is exhausted, and only the last prediction is output.&lt;/li&gt;
  &lt;li&gt;In &lt;em&gt;budgeted batch classification&lt;/em&gt;, for each test example, the classifiers are evaluated sequentially until a sufficient level of confidence in prediction is found. However, determining the “sufficient level” part can be quite tricky, since you have to distribute a budget across multiple samples without knowing the difficulty of each. You cannot afford to fully evaluate the network on each one, yet you have to be confident enough in your prediction, while not knowing how long it is going to take for individual examples to reach this level of confidence. The authors therefore rely on a probabilistic model of the difficulty of the inputs to design the optimal threshold that wll determine whether or not we stop running a given input through the network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;training-details&quot;&gt;Training details&lt;/h4&gt;

&lt;p&gt;We now have a network with multiple classifiers that need to be trained jointly. To do so, we unsurprisingly use cross-entropy $L(f_k)$ for all classifiers $f_k$, but we still need to combine those losses to perform joint optimization. The most natural solution is to take a weighted sum of all the individual losses, such as the final loss of the model is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(f) = \sum_k w_k L(f_k)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;If the budget distribution $P(B)$ is known, we can use the weights to incorporate prior knowledge about the budget $B$ in the learning. Empirically, we find that using uniform weights $\forall k, w_k=1$ works well in practice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-glance-at-the-results&quot;&gt;A glance at the results&lt;/h2&gt;

&lt;p&gt;Now that we’ve gone so far, let’s just check that MSDNet performs well on the tasks it’s been designed to solve, namely &lt;em&gt;anytime prediction&lt;/em&gt; and &lt;em&gt;budgeted batch classification&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anytime-prediction-1&quot;&gt;Anytime prediction&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-anytime_prediction.png&quot; alt=&quot;Anytime prediction&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In anytime prediction, for each test example, the model is run until an unknown budget $B$ is exhausted, and is forced to output its latest prediction. As discussed earlier, typical baselines to compare against are ensembles of CNNs, here ensembles of ResNets and ensemble of DenseNets, that are evaluated sequentially until the budget is exhausted. Other baselines include &lt;a href=&quot;http://proceedings.mlr.press/v38/lee15a.pdf&quot;&gt;deeply supervised networks&lt;/a&gt; (here ResNet&lt;sup&gt;MC&lt;/sup&gt; and DenseNet&lt;sup&gt;MC&lt;/sup&gt;) and &lt;a href=&quot;https://arxiv.org/abs/1605.07648&quot;&gt;FractalNet&lt;/a&gt;. Going into details of these models is beyond the scope of this article; check the original papers to read about these interesting architectures.&lt;/p&gt;

&lt;p&gt;MSDNet shows excellent performance compared to all other baselines across all budgets, except FractalNet in low budget, where the performances are comparable. Note that, in extremely low budgets, the ensembles have a significant advantage since the first network (the only one evaluated) is directly optimized for prediction; yet MSDNet performance remains very satisfying. The limitations of ensembles appear very quickly when the budget increases, since they waste a lot of computation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;budgeted-batch-classification-1&quot;&gt;Budgeted batch classification&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-budgeted_batch.png&quot; alt=&quot;Budgeted batch&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In budgeted batch classification, the model is granted a finite known computational budget $B$ to classify a set of examples, and can spend it freely across examples. This encourages the use of &lt;em&gt;dynamic evaluation&lt;/em&gt;, where easy examples are exited early while hard examples are run throughout the network until a sufficient level of confidence in classification is reached. The baselines are similar to the previous ones.&lt;/p&gt;

&lt;p&gt;Three MSDNets are trained with different depths, in order to cover a wide range of computational budgets, and the chosen model is selected depending on the budget, based on validation performance; this explains the three black curves. Once again, MSDNet prove very performant across all budgets, outperforming all baselines by a wide margin. In particular, MSDNet perform much better than deeply supervised networks, highlighting the importance of coarse-level features for classification at early stages.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bonus-points---visualization&quot;&gt;Bonus points - Visualization&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-hard_easy.png&quot; alt=&quot;Hard and easy examples&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;Samples from ImageNet classes &lt;em&gt;red wine&lt;/em&gt; and &lt;em&gt;volcano&lt;/em&gt;. Thanks to the dynamic evaluation, “easy” examples are early-exited while “hard” examples are run through larger parts of the network.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To assess the relevance of dynamic evaluation and the calibration of confidence of the classifiers, it is interesting to examine which examples are early exited, and which ones are run through the entire network. As the figure above suggests, MSDNet seem able to quickly classify typical class examples, while non-typical images are granted a larger budget.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Resource efficiency" /><category term="Noise" /><category term="Optimization" /><category term="Mixing ideas" /><category term="Personal work" /><summary type="html"></summary></entry><entry><title type="html">Multi-Scale DenseNet, a resource-aware CNN</title><link href="http://localhost:4000/ICLR2018-Multi_Scale_Dense_Networks/" rel="alternate" type="text/html" title="Multi-Scale DenseNet, a resource-aware CNN" /><published>2018-09-24T01:00:00+02:00</published><updated>2018-09-24T01:00:00+02:00</updated><id>http://localhost:4000/ICLR2018-Multi_Scale_Dense_Networks</id><content type="html" xml:base="http://localhost:4000/ICLR2018-Multi_Scale_Dense_Networks/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iclr-2018---3rd-article&quot;&gt;ICLR 2018 - 3rd article&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Oral papers are analyzed and
commented in an accessible way.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is based on the paper&lt;/em&gt;
&lt;a href=&quot;https://openreview.net/pdf?id=Hk2aImxAb&quot;&gt;Multi-Scale Dense Networks for resource efficient image classification&lt;/a&gt;
&lt;em&gt;by Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten and
Kilian Weinberger.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Have a look at the two pictures below. It probably took you an unnoticeable amount of time to recognize a horse on the left, and a very noticeable amount of time (say one second) to recognize a horse on the right. Naturally, we would expect models to face similar difficulties in classification of those images. Intuitively, it feels like a simple CNN with a couple layers (e.g. AlexNet) would be more than enough to classify the first picture, while the last one shall require a much more complex model for correct classification, being in the &lt;em&gt;tail&lt;/em&gt; of the “horse images” distribution (hence requiring a more precise approximation of this distribution by a neural network).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-horses.png&quot; alt=&quot;Two horses&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using the same model to classify both pictures generally means that you have to choose &lt;strong&gt;beforehand, and once and for all&lt;/strong&gt; (when you implement the model), between low resource consumption and high accuracy. In other words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Computationally intensive models are needed to classify such tail examples correctly, but are wasteful when applied to canonical images such as the left one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, generally speaking, we computer users don’t care, or at least this is not a question that we are used to ask. In the rare situations where we actually care about resource consumption (most of all about speed), we just define a minimum acceptable inference speed, pick the best performing model that satisfies this constraint, and that’s basically it. However, &lt;strong&gt;we phone users and (future) Internet of Things users are very much likely to &lt;a href=&quot;https://www.iotforall.com/computer-vision-iot/&quot;&gt;care&lt;/a&gt;&lt;/strong&gt;. From photo deblurring to real-time action recognition, computer vision &lt;em&gt;will&lt;/em&gt; become ubiquitous in everyday devices, that run on much lower resources than modern computers. Moreover, lower computational cost means &lt;strong&gt;lower time energy consumption&lt;/strong&gt;, which is highly desirable for &lt;strong&gt;ecological&lt;/strong&gt; (and economical) reasons.&lt;/p&gt;

&lt;p&gt;All in all, it feels frustrating not to recognize a horse on the right, so we use the winner of ImageNet 2017 (an ensemble of &lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot;&gt;Squeeze-and-Excitation Networks&lt;/a&gt;); but it is ridiculous to waste &lt;a href=&quot;https://github.com/albanie/convnet-burden&quot;&gt;440 MB of parameters and 21 GFLOPs&lt;/a&gt; on recognizing a horse on the right while we probably only need 50 times less resources to do it (as a reference, Apple’s iPhone 4 had a processor power of about 3 GFLOP/s, so it would actually take 7 seconds at 100% CPU to process each image). We feel torn between both issues, and nobody likes feeling &lt;a href=&quot;https://music.youtube.com/watch?v=rIGAt5yvfmw&amp;amp;list=RDAMVMrIGAt5yvfmw&quot;&gt;torn&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This begs the question: &lt;strong&gt;why do we choose between either wasting computational resources by applying an unnecessarily computationally expensive model to easy images, or making mistakes by using an efficient model that fails to recognize difficult images?&lt;/strong&gt; Ideally, our systems should automatically use small networks when test images are easy or computational resources limited, and use big networks when test images are hard or computation is abundant.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is why the authors propose the Multi-Scale Dense Network model (architecture below), which we will explore, that enables adaptive resource allocation for image classification thanks to the introduction of early classifiers in a feed-forward CNN structure. This way, easy images can be instantly classified, and harder ones can use more computational resources. Let’s see how this works on real-world tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-MSDNet_architecture.png&quot; alt=&quot;MSDNet architecture&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;inpost-figure-caption-centered&quot;&gt;The Multi-Scale Dense Network (MSDNet) architecture&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;real-word-tasks---classification-on-a-tight-budget&quot;&gt;Real-word tasks - classification on a (tight) budget&lt;/h2&gt;

&lt;p&gt;Let’s first define the situations where a resource-aware model is likely to be more useful than an off-the-shelf CNN. This will help us understand where and when exactly the new architecture helps. Computationally constrained tasks are numerous and diverse, and we will only focus on two major problems: anytime prediction, and budgeted batch classification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anytime-prediction&quot;&gt;Anytime prediction&lt;/h3&gt;

&lt;p&gt;In &lt;strong&gt;anytime prediction&lt;/strong&gt;, the model can be forced to output a prediction, at any given point, possibly before the full computation is complete. Good performance levels are typically achieved by models that are able to give crude estimates very quickly, and refine them with time until the full model is run. For example, imagine an autonomous car equipped with a network to detect and handle obstacles on the road. You want your car to &lt;em&gt;instantly&lt;/em&gt; detect and react to a pedestrian suddenly appearing in front of the car; there is no time to decide the precise distance at which it appears, whether it is an adult or a child, and at which speed it is going towards you. On the other hand, for distant and long-term obstacles, determining the precise distance between you and them enables better planning, smooth trajectories, better fuel management, etc.&lt;/p&gt;

&lt;p&gt;In a formal way, we assume that test samples $x$ and budgets $B$ are drawn from a joint distribution $P(x, B)$. The model outputs a prediction $f(x)$ within the computational budget $B$, and incurs a loss $L(f(x), B)$. The goal in anytime prediction is to find a model that minimizes the expected loss of individual prediction within (hard) budget constraint:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f \ \mathbb{E}_{P(x, B)} [L(f(x)), B]&lt;/script&gt;

&lt;p&gt;Another example of anytime prediction is real-time video classification. You are filming a scene, and you want your phone to identify the various elements present in the video while filming. It is not unreasonable to ask for a refresh rate of 10 Hz, which means a prediction every 0.1 s, &lt;em&gt;whatever the computational budget available on your phone at that time&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;budgeted-batch-classification&quot;&gt;Budgeted batch classification&lt;/h3&gt;

&lt;p&gt;In &lt;strong&gt;budgeted batch classification&lt;/strong&gt;, the model is granted a finite known computational budget to classify a set of examples, and can spend it freely across examples. Good performance levels are typically achieved by models that are able to quickly classify easy examples (left horse), in order to save some additional computation for harder examples (right horse). For example, imagine that you want to show your best friend all the pictures on your phone where both of you are present. Some pictures will be easily classified (you both clearly face the camera, no other people present), some will be much harder (you are in a crowd, disguised, wearing make-up or making funny faces). You don’t care much about that, and you only want your phone to give you a decent search result in less than 5 seconds.&lt;/p&gt;

&lt;p&gt;Formally, we consider a set of examples $\mathcal{D}_ {test} = \{ x_1, …, x_M \}$ and a computational budget $B$ that is known in advance. The model spends $B$ as it pleases across examples, outputs a set of predictions $f(\mathcal{D}_ {test})$ and incurs loss $L(f(\mathcal{D}_{test}), B)$. The goal in budgeted batch classification is to find a model that minimizes the expected loss of batch prediction within (soft) budget constraint:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f \ \mathbb{E}_{P(x)} [L(f(\mathcal{D}_{test})), B]&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-problems-with-early-classification&quot;&gt;The problems with early classification&lt;/h2&gt;

&lt;p&gt;Let us think it through. The simplest, most natural answer to address both these situations is to use multiple networks with increasing capacity (e.g. multiply the number of layers by a constant factor from one model to the next one), and evaluate them sequentially at test time. In &lt;em&gt;anytime prediction&lt;/em&gt;, you simply output the prediction of the last network evaluated; in &lt;em&gt;batch budgeted classification&lt;/em&gt;, you stop the evaluation once classification with sufficient confidence level is reached. This is illustrated in the next figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-model_sequence.png&quot; alt=&quot;Model sequence&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt; A “sequence of models” solution, featuring AlexNet (A), GoogLeNet (G) and ResNet (R). Green $\gamma$ blocks denote selection policies. The input is first evaluated by AlexNet, and the selection policy determines whether evaluation by more complex models is needed. (&lt;a href=&quot;https://arxiv.org/pdf/1702.07811.pdf&quot;&gt;Source&lt;/a&gt;) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The problem with this approach is that, when the first network isn’t confident enough, we switch to the second network without re-using any feature previously computed: for complex examples, we completely &lt;strong&gt;waste&lt;/strong&gt; the computational budget spent on the first networks. This is quite unsatisfying, and far from optimal.&lt;/p&gt;

&lt;p&gt;Then the opposite solution comes in mind: instead of building multiple networks with one classifier each, not sharing any feature, we could build one network as a cascade of multiple (early) classifiers along depth, re-using previous features to build the more advanced predictions. This would look like the next figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-classifiers_cascade.png&quot; alt=&quot;Cascade&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;A simple “cascade of classifiers” solution on a standard CNN architecture. Early classification of easy examples yields substantial savings in computational budget, that can be spend on the hard examples. (&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2017/papers/Ouyang_Chained_Cascade_Network_ICCV_2017_paper.pdf&quot;&gt;Source&lt;/a&gt;) &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Although this model doesn’t waste any feature, it leads to poor performance, for two distinct reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Early classifiers lack coarse-level, global features&lt;/strong&gt; - only fine-scale, local features are available at early stages.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Early classifiers interfere with later classifiers&lt;/strong&gt; - early classifiers tend to optimize the early features for the short-term, conflicting with the long-term optimization of late classifiers, that achieve better performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two issues will drive the design of MSDNet through the inclusion of two specific components, each addressing one issue.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;early-classifiers-vs-dense-connections&quot;&gt;Early classifiers vs. dense connections&lt;/h3&gt;

&lt;p&gt;Let us take a common CNN architecture, &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt;, and attach an intermediate classifier at a (more or less) early stage of the architecture. We then train both (final and intermediate) classifiers jointly (here on CIFAR-100), weighting their losses equally, and look at the accuracy of the final classifier. If there a noticeable difference in performance with the standard setting, the presence of the intermediate classifier is likely to have an influence on the construction of the features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-final_acc_when_intermed.png&quot; alt=&quot;Final accuracy when intermediate classifier is attached&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As is clear from the figure, ResNet performance generally suffers a lot from the introduction of an intermediate classifier, especially at very early stages.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We postulate that this accuracy degradation in the ResNet may be caused by the intermediate classifier influencing the early features to be optimized for the short-term and not for the final layers. This improves the accuracy of the intermediate classifier but collapses information required to generate high quality features in later layers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This sounds like a reasonable and likely hypothesis. It would be interesting to visually examine the filters learned and the corresponding features for different locations of the intermediate classifier, providing us with some insights in this regard.&lt;/p&gt;

&lt;h4 id=&quot;solutionmitigation-dense-connections&quot;&gt;Solution/Mitigation: Dense connections&lt;/h4&gt;

&lt;p&gt;To mitigate this problem, the paper cites dense connections as an interesting line of work. Dense connections were introduced by our authors one year earlier in &lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNets&lt;/a&gt;, as a generalization of residual connections, the building blocks of ResNets. Remember what a residual connection looks like? A residual block is displayed in the next figure: the signal can bypass the layer thanks to an identity connection, and addition with the layer’s output.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-residual_block.png&quot; alt=&quot;Residual block&quot; class=&quot;center-image&quot; /&gt;
&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;A residual block, the building foundation of ResNets. (&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Dense connections go one step further by connecting each layer directly with &lt;em&gt;all previous layers&lt;/em&gt; (inside the same block). What’s more, instead of being summed, the previous features are concatenated to enable direct re-use. The resulting dense block is illustrated in the next figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-dense_block.png&quot; alt=&quot;Dense block&quot; class=&quot;center-image&quot; /&gt;
&lt;span class=&quot;inpost-figure-caption&quot;&gt;A dense block, the building foundation of DenseNets. At each stage, the features of all previous layers are concatenated to maximize information flow and allow layer bypassing as much as possible. (&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Now how will this help us? Compared to ResNets, DenseNets suffer much less from the introduction of intermediate classifiers at early levels (see the figure some blocks above). This is likely linked to the fact that the signal can bypass all layers, so that no layer results in a loss in information. Should an early layer get optimized for short-term classification, the original signal can still be recovered unperturbed by later layers. This greatly alleviates the influence between short-term and long-term optimization, and makes the final accuracy of DenseNets not too dependent on the location of the intermediate classifier, yielding a nice candidate to support early classifiers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;coarse-features-vs-multiple-scales&quot;&gt;Coarse features vs. multiple scales&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-feature_levels.png&quot; alt=&quot;Feature levels&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;Visualization of the features learned by a convolutional network along its depth, using DeconvNet. Low-level features match local and simple patterns, while higher level features retain incresingly global and complex patterns. (&lt;a href=&quot;http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf&quot;&gt;Source&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;You’ve probably seen this picture a couple times already, or one alike. This visualization of the features learned by the network along the depth was obtained thanks to a &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;visualization DeconvNet&lt;/a&gt;. It supports the claim that first layers correspond to local, fine-scale features (the feature maps learned are close to the size of the original image), whereas deep layers correspond to global, coarse-scale features (the feature maps learned are much smaller than the original image, condensing global information such as “there is a dog in this region”).&lt;/p&gt;

&lt;p&gt;Now this is a problem for early classifiers. Image classification is much harder when you only have access to local features, matching only simple patterns such as straight lines in different directions, or circles of different sizes. To confirm this intuition, let us consider again our previous experiment: take a ResNet, attach an intermediate classifier at different locations along the depth of the network, but this time examine how this location influences the accuracy of the &lt;em&gt;intermediate&lt;/em&gt; classifier. If classification based on local, fine-scale features was as easy as classification based on global, coarse-scale features, the impact on accuracy should not be too noticeable (and, as a side note, we woudln’t need deep learning to begin with).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-intermed_acc_when_intermed.png&quot; alt=&quot;Intermediate accuracy&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It hurts. Pretty badly. DenseNets are once again less sensitive to this effect, but the performance of the intermediate classifier is still unsatisfying.&lt;/p&gt;

&lt;h4 id=&quot;solutionmitigation-multi-scale-feature-maps&quot;&gt;Solution/Mitigation: Multi-scale feature maps&lt;/h4&gt;

&lt;p&gt;To mitigate this harmful effect, the authors suggest maintaining feature maps of multiple scales at all layers. This is pretty annoying to describe with words, so let’s jump into the architecture to see how it is implemented.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-architecture&quot;&gt;The architecture&lt;/h2&gt;

&lt;p&gt;As expected, the resulting design combines dense connections with multi-scale feature maps to yield the architecture of the Multi-Scale DenseNet, taking the following form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-MSDNet_architecture.png&quot; alt=&quot;MSDNet&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;The MSDNet architecture. Each layer maintains feature maps at multiple scales, enabling early classifiers to operate on coarse-level features. The feature maps at a given scale and layer are the concatenation of two elements: the result (in blue) of strided convolution on the finer-scale features from the previous layer, and the result (in red) of regular convolution on the same scale features from multiple previous layers &lt;em&gt;via&lt;/em&gt; dense connections. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This is not your typical CNN architecture, so let’s break it down bit by bit: first the layers and the connections, then the classifiers and their repartition, and finally the training details and the loss function.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;layers&quot;&gt;Layers&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-layers_details.png&quot; alt=&quot;MSDNet layers details&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As described in the figures, the model is structured as a 2-dimensional grid $S \times L$, with $L$ the number of layers (analogous to the depth in standard networks) and $S$ the number of scales at each layer.&lt;/p&gt;

&lt;p&gt;The first layer $l=1$ is quite unique, since it “seeds” all scales $s = 1, …, S$ of the feature maps. The finer scale $s=1$ is the result of a regular convolution $h$ on the original image. For subsequent scales, the feature map at scale $s+1$ is the result of strided convolution $\tilde{h}$ (which is just really regular convolution with a stride of at least 2) to the features at scale $s$. Given that strided convolution results in downsampling of the input (like max pooling), the feature map at scale $s+1$ is smaller than the one at scale $s$. This enables coarser-level, global information to be encoded already in the first layer.&lt;/p&gt;

&lt;p&gt;The next layers maintain all scales that were previously seeded. The output of layer $l$ at scale $s$ is the concatenation of two elements:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The result of regular convolution $h$ on all previous feature maps (dense connection) at scale $s$ from layers $l’=1, …, l-1$ (in red).&lt;/li&gt;
  &lt;li&gt;The result of strided convolution $\tilde{h}$ on all previous feature maps (dense connection) at scale $s-1$ from layers $l’=1, …, l-1$ (in blue, performing the downsampling of finer-scale features that is common in CNNs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;classifiers&quot;&gt;Classifiers&lt;/h4&gt;

&lt;p&gt;Following the same principle of dense connections, the classifiers take as input the concatenation of all coarse feature maps from all previous layers. In practice they are only attached to some intermediate layers, and their behavior depends on the task at hand:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In &lt;em&gt;anytime prediction&lt;/em&gt;, the classifiers are evaluated sequentially until the budget is exhausted, and only the last prediction is output.&lt;/li&gt;
  &lt;li&gt;In &lt;em&gt;budgeted batch classification&lt;/em&gt;, for each test example, the classifiers are evaluated sequentially until a sufficient level of confidence in prediction is found. However, determining the “sufficient level” part can be quite tricky, since you have to distribute a budget across multiple samples without knowing the difficulty of each. You cannot afford to fully evaluate the network on each one, yet you have to be confident enough in your prediction, while not knowing how long it is going to take for individual examples to reach this level of confidence. The authors therefore rely on a probabilistic model of the difficulty of the inputs to design the optimal threshold that wll determine whether or not we stop running a given input through the network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;training-details&quot;&gt;Training details&lt;/h4&gt;

&lt;p&gt;We now have a network with multiple classifiers that need to be trained jointly. To do so, we unsurprisingly use cross-entropy $L(f_k)$ for all classifiers $f_k$, but we still need to combine those losses to perform joint optimization. The most natural solution is to take a weighted sum of all the individual losses, such as the final loss of the model is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(f) = \sum_k w_k L(f_k)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;If the budget distribution $P(B)$ is known, we can use the weights to incorporate prior knowledge about the budget $B$ in the learning. Empirically, we find that using uniform weights $\forall k, w_k=1$ works well in practice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-glance-at-the-results&quot;&gt;A glance at the results&lt;/h2&gt;

&lt;p&gt;Now that we’ve gone so far, let’s just check that MSDNet performs well on the tasks it’s been designed to solve, namely &lt;em&gt;anytime prediction&lt;/em&gt; and &lt;em&gt;budgeted batch classification&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anytime-prediction-1&quot;&gt;Anytime prediction&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-anytime_prediction.png&quot; alt=&quot;Anytime prediction&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In anytime prediction, for each test example, the model is run until an unknown budget $B$ is exhausted, and is forced to output its latest prediction. As discussed earlier, typical baselines to compare against are ensembles of CNNs, here ensembles of ResNets and ensemble of DenseNets, that are evaluated sequentially until the budget is exhausted. Other baselines include &lt;a href=&quot;http://proceedings.mlr.press/v38/lee15a.pdf&quot;&gt;deeply supervised networks&lt;/a&gt; (here ResNet&lt;sup&gt;MC&lt;/sup&gt; and DenseNet&lt;sup&gt;MC&lt;/sup&gt;) and &lt;a href=&quot;https://arxiv.org/abs/1605.07648&quot;&gt;FractalNet&lt;/a&gt;. Going into details of these models is beyond the scope of this article; check the original papers to read about these interesting architectures.&lt;/p&gt;

&lt;p&gt;MSDNet shows excellent performance compared to all other baselines across all budgets, except FractalNet in low budget, where the performances are comparable. Note that, in extremely low budgets, the ensembles have a significant advantage since the first network (the only one evaluated) is directly optimized for prediction; yet MSDNet performance remains very satisfying. The limitations of ensembles appear very quickly when the budget increases, since they waste a lot of computation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;budgeted-batch-classification-1&quot;&gt;Budgeted batch classification&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-budgeted_batch.png&quot; alt=&quot;Budgeted batch&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In budgeted batch classification, the model is granted a finite known computational budget $B$ to classify a set of examples, and can spend it freely across examples. This encourages the use of &lt;em&gt;dynamic evaluation&lt;/em&gt;, where easy examples are exited early while hard examples are run throughout the network until a sufficient level of confidence in classification is reached. The baselines are similar to the previous ones.&lt;/p&gt;

&lt;p&gt;Three MSDNets are trained with different depths, in order to cover a wide range of computational budgets, and the chosen model is selected depending on the budget, based on validation performance; this explains the three black curves. Once again, MSDNet prove very performant across all budgets, outperforming all baselines by a wide margin. In particular, MSDNet perform much better than deeply supervised networks, highlighting the importance of coarse-level features for classification at early stages.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bonus-points---visualization&quot;&gt;Bonus points - Visualization&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-24-hard_easy.png&quot; alt=&quot;Hard and easy examples&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;Samples from ImageNet classes &lt;em&gt;red wine&lt;/em&gt; and &lt;em&gt;volcano&lt;/em&gt;. Thanks to the dynamic evaluation, “easy” examples are early-exited while “hard” examples are run through larger parts of the network.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To assess the relevance of dynamic evaluation and the calibration of confidence of the classifiers, it is interesting to examine which examples are early exited, and which ones are run through the entire network. As the figure above suggests, MSDNet seem able to quickly classify typical class examples, while non-typical images are granted a larger budget.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="ICLR 2018" /><category term="Resource efficiency" /><category term="CNN" /><category term="Network architecture" /><category term="ICLR" /><summary type="html"></summary></entry><entry><title type="html">Neural Machine Translation of the Cambridge meme</title><link href="http://localhost:4000/ICLR_2-nmt_cambridge_meme/" rel="alternate" type="text/html" title="Neural Machine Translation of the Cambridge meme" /><published>2018-09-10T01:00:00+02:00</published><updated>2018-09-10T01:00:00+02:00</updated><id>http://localhost:4000/ICLR_2-nmt_cambridge_meme</id><content type="html" xml:base="http://localhost:4000/ICLR_2-nmt_cambridge_meme/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iclr-2018---2nd-article&quot;&gt;ICLR 2018 - 2nd article&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Oral papers are analyzed and
commented in an accessible way.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is based on the paper&lt;/em&gt;
&lt;a href=&quot;https://openreview.net/pdf?id=BJ8vJebC-&quot;&gt;Synthetic and natural noise both break Neural Machine Translation&lt;/a&gt;
&lt;em&gt;by Yonatan Belinkov and Yonatan Bisk.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you are reading these lines, there is a good chance that you have been
wandering the Internet for quite some time now. This means that there is also
a good chance that you’ve stumbled upon the &lt;em&gt;Cambridge research&lt;/em&gt; meme:&lt;/p&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht
oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and
lsat ltteer be at the rghit pclae.
&lt;/div&gt;

&lt;p&gt;There is even a greater chance that you have, for more or less legitimate
reasons, used Google Translate (or even better, its European equivalent, DeepL).
Of course, those automated translation systems have improved a lot in the past
few years; what was laughable at a couple years ago is now quite reasonable for
a number of languages. For simple texts, the translation even feels quite
natural, although not idiomatic (try translating “Il tombe des cordes.”, which
is the usual French equivalent of “It’s raining cats and dogs.”).&lt;/p&gt;

&lt;p&gt;Going back to the meme, you would probably have no trouble translating it to
whatever language you are familiar with. But how do automated translation
systems cope with this difficult situation? Let’s try translating a French
version into English:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Original &lt;br /&gt;French &lt;br /&gt; scrambled&lt;/td&gt;
      &lt;td&gt;Sleon un cehcruher de l’Uvinertisé de Cmabrigde, l’odrre des ltteers dnas un mot n’a pas d’ipmrotncae, la suele coshe ipmrotnate est que la pmeirère et la drenèire lteetrs sinoet à la bnnoe pclae.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Human translation&lt;/td&gt;
      &lt;td&gt;According to a researcher at Cambridge University, it doesn’t matter in what order the letters in a word are, the only important thing is that the first and last letters are in the right place.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Google Translate&lt;/td&gt;
      &lt;td&gt;According to an opinion of the Uvinised of Cmabrigde, the name of the ltteers in one word does not have a name, but the name of the message is that the letter and the letter are very brief at the same time.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepL&lt;/td&gt;
      &lt;td&gt;According to a cehcruher of the Uvinertized of Cmabrigde, the odd of the ltteers in a word has no ipmrotncae, the only coshe ipmrotnate is that the pmeirere and the drenere lteetrs sinoetrs in the bnnoe pclae.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This is quite unconvincing, to say the least. In other words, Google Translation
and DeepL are both very sensitive to this type of noise, whereas humans
seem impressively robust to it. This is just a symptom of the frailty of
automated translation systems:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While typos and noise are not new to NLP, our systems are rarely trained to
explicitly address them, as we instead hope that the relevant noise will occur
in the training data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It turns out that this kind of wishful thinking leaves &lt;strong&gt;Neural Machine
Translation (NMT) systems very
brittle&lt;/strong&gt;, with their performance dropping quickly in presence of noise. How can
we evaluate this lack of robustness to noise, and what can be done to improve
our models? Let’s jump into the paper.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evaluating-the-robustness-of-current-models&quot;&gt;Evaluating the robustness of current models&lt;/h2&gt;

&lt;p&gt;We would like to evaluate the performance of NMT models, and how it is affected
by noise. To do that we will, unsurprisingly, have to answer 3 questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Which performance metric?&lt;/li&gt;
  &lt;li&gt;Which NMT models?&lt;/li&gt;
  &lt;li&gt;Which kind of noise?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-performance-metric-bleu&quot;&gt;The performance metric: BLEU&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;&gt;BLEU&lt;/a&gt; (bilingual evaluation
understudy)&lt;/strong&gt; is a popular metric to evaluate the similarity between the
machine translation and a professional human translation. The idea behind it is
fairly simple: it is a modification of precision, computed on candidate
sentences (by comparison with reference sentences), and then averaged to produce
a corpus-level score of the translation’s quality.&lt;/p&gt;

&lt;p&gt;Let’s first remember why we don’t use precision as metric for this task.
Consider a sentence $s$ to be translated, with human reference translation $r$
and candidate translation $c$. Then precision is a score between 0 and 1
computed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{Precision}(c, r) =
  \dfrac{\sum_{w \in W(c)} m_w(c) \cdot \mathbb{1}_{w \in W(r)}}
    {\sum_{w \in W(c)} m_w(c)}&lt;/script&gt;

&lt;p&gt;with $W(c)$ the set of unique words that appear in $c$, and $m_w(c)$ the number
of occurences of $w$ in $c$. Precision essentially measures the proportion of
words of the candidate translation that actually appear in the reference
translation. To see why this is not sufficient, have a look at the table below.
Usually precision is considered coupled with recall to overcome such issues;
we don’t, because recall can be artificially inflated when considering multiple
references (see &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Reference&lt;/th&gt;
      &lt;th&gt;Candidate&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;BLEU (bigram)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;“I like trains a lot”&lt;/td&gt;
      &lt;td&gt;“I like”&lt;/td&gt;
      &lt;td&gt;$\dfrac{1+1}{2} = 1$&lt;/td&gt;
      &lt;td&gt;$\dfrac{1}{1} \cdot e^{1 - 2/5} = 0.55$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“I like trains a lot”&lt;/td&gt;
      &lt;td&gt;“a a a a a”&lt;/td&gt;
      &lt;td&gt;$\dfrac{5}{5} = 1$&lt;/td&gt;
      &lt;td&gt;$\dfrac{0}{4} \cdot 1 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“I like trains a lot”&lt;/td&gt;
      &lt;td&gt;“I like trains a a”&lt;/td&gt;
      &lt;td&gt;$\dfrac{5}{5} = 1$&lt;/td&gt;
      &lt;td&gt;$\dfrac{3}{4} \cdot 1 = 0.75$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;“It is raining cats and dogs”&lt;/td&gt;
      &lt;td&gt;“It is pouring”&lt;/td&gt;
      &lt;td&gt;$\dfrac{2}{3} = 0.66$&lt;/td&gt;
      &lt;td&gt;$\dfrac{1}{2} \cdot e^{1 - 3/6} = 0.30$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To solve those issues, the BLEU score brings a couple of modifications to the
precision:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Consider the n-grams $G(c)$ instead of the words $W(c)$. &lt;br /&gt;This helps with
the fluency of the translation: with words only, “trains lot I a like” gets the
same score as “I like trains a lot”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replace $\ \ \textrm{ }m_w(c) \cdot 1_{w \in W(r)}\ \ \textrm{ }$ by $\ \
\textrm{ }\min (m_w(c), m_w(r)) \cdot {1_{w \in W(r)}}$. &lt;br /&gt;This basically
says that, in the second example, only the first “a” will be counted as correct,
since “a” appears only once in the reference.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multiply by a factor $\ \ \min\left(1, \exp \left(1 - \frac{\textrm{length of reference
corpus}} {\textrm{length of candidate corpus}}\right)\right)$. &lt;br /&gt;Indeed, even with the
previous modifications, the constructed score still favors short translations
(see the first example). We therefore penalize translations that are shorter on
average than the reference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As illustrated in the last row of the table, even with these modifications the
final score is not ideal, since a perfectly natural candidate translation gets
a low score. It is, however, still much better than precision in a wide range
of situations. In the end, the BLEU score takes the following form
(the higher the better) for our example sentences:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{BLEU}(c, r) =
  \dfrac{\sum_{g \in G(c)} \min (m_g(r), m_g(c)) \cdot \mathbb{1}_{g \in G(r)}}
    {\sum_{g \in G(c)} m_g(c)}
  \cdot \min\left(1, \exp \left({1 -
  \frac{\textrm{length}(r)}{\textrm{length}(c)}}\right)\right)&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-nmt-models-nematus-and-char2char&quot;&gt;The NMT models: &lt;strong&gt;Nematus&lt;/strong&gt; and &lt;strong&gt;char2char&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;This will be much quicker. We want to see how state-of-the-art models
with different architectures (and especially different accesses to words,
sub-word units or characters) are able to cope with noise. The authors ran their
experiments on three distinct models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.03017&quot;&gt;&lt;strong&gt;char2char&lt;/strong&gt;&lt;/a&gt; (Lee et al., 2017) - a
sequence-to-sequence model with attention, trained on characters to characters
(&lt;a href=&quot;https://github.com/nyu-dl/dl4mt-c2c&quot;&gt;implementation&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://aclweb.org/anthology/E17-3017&quot;&gt;&lt;strong&gt;Nematus&lt;/strong&gt;&lt;/a&gt; (Sennrich et al., 2017) -
a competition-winning sequence-to-sequence model with some architectural
modifications that enable operating on sub-word units
(&lt;a href=&quot;http://data.statmt.org/rsennrich/wmt16_systems/&quot;&gt;implementation&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.06615&quot;&gt;&lt;strong&gt;charCNN&lt;/strong&gt;&lt;/a&gt; (Kim et al., 2015) - an
attentional sequence-to-sequence model based on a character convolutional
neural network. To quote the authors, “this model retains the notion of a word
but learns a character-dependent representation of words”, and “performs well on
morphologically rich languages”
(&lt;a href=&quot;https://github.com/harvardnlp/seq2seq-attn&quot;&gt;implementation&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-noise-types-nat-key-swap-mid-rand&quot;&gt;The noise types: Nat, Key, Swap, Mid, Rand&lt;/h3&gt;

&lt;p&gt;We finally need to define a number of noise types that we will use to perturb
the models.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Example&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Original&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;weather&lt;/td&gt;
      &lt;td&gt;Original correct word&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Nat&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;whether&lt;/td&gt;
      &lt;td&gt;Natural noise, e.g. spelling mistake&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;qeather&lt;/td&gt;
      &lt;td&gt;Replace a letter with an adjacent key (QWERTY keyboard)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Swap&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;wetaher&lt;/td&gt;
      &lt;td&gt;Swap two letters except the first and last ones&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Mid&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;whaeter&lt;/td&gt;
      &lt;td&gt;Scramble the letters except the first and last ones&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Rand&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;raewhte&lt;/td&gt;
      &lt;td&gt;Scramble all letters&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;facing-the-truth---the-inability-to-translate-noisy-texts&quot;&gt;Facing the truth - the (in)ability to translate noisy texts&lt;/h3&gt;

&lt;p&gt;With all the tools in hand, it is now time to experiment and face the truth:
how well are our NMT models able to cope with the different noise types? I’m
definitely too lazy to run all the tests by myself, so I’ll steal the paper’s
results instead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-noise_results.png&quot; alt=&quot;Experiments with noise&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-noise_plots.png&quot; alt=&quot;Performance decrease&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whatever the language, noise has a striking effect on performance.
As a quick note, the results presented in the table aren’t comparable across
noise types, since the noise intensity is not the same in all cases. For a fair
comparison, have a look at the graphs (German to English): surprisingly enough,
the Rand noise isn’t much worse than Swap, despite yielding much more dramatic
changes of the tokens. It is actually &lt;em&gt;very&lt;/em&gt; surprising, since Swap is strictly
included in Rand. This would mean that the NMT model (especially Nematus)
is basically unable to recover a word with virtually any Swap mistake.
Yet, the authors’ conclusion still holds true:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The important thing to note is that even small amounts of noise lead to
substantial drops in performance. […] This is true for both natural noise and
all kinds of synthetic noise. […] The degradation in quality is especially
severe in light of the humans’ ability to understand noisy texts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-noise&quot;&gt;Dealing with noise&lt;/h2&gt;

&lt;p&gt;Now that we know where we are, what can be done to increase the robustness of
our models? The authors propose two natural ideas, and show their effect on
performance:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;All models presented rely on word structure to build a representation. This
structure is, however, altered to some extent by most of the studied noise
types (Swap, Mid, Rand). The idea is therefore to make some architectural
modifications in order to learn a representation of words that is invariant to
their structure.&lt;/li&gt;
  &lt;li&gt;Training on noisy examples has been regularly reported to increase model
robustness to noise. Let’s try that too.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;structure-invariant-representations&quot;&gt;Structure invariant representations&lt;/h3&gt;

&lt;p&gt;As is clear in their architecture, all three of the models under study are
&lt;em&gt;by design&lt;/em&gt; sensitive to word structure, at a character level (due to
convolutional layers or the sub-word units considered). There is a fair chance
that a model that is &lt;em&gt;insensitive&lt;/em&gt; to this structure would be more robust to
noises that affect this structure; nothing groundbreaking there.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perhaps the simplest such model is to take the average character embedding as
a word representation. This model, referred to as meanChar, first generates a
word representation by averaging character embeddings, and then proceeds
with a word-level encoder similar to the charCNN model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-meanchar_performance.png&quot; alt=&quot;meanChar performance&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt;Performance of meanChar in different settings. By design, Scr is the
same as Vanilla for this model. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As is clear in the table, meanChar turns out to be pretty good for translating
scrambled text (much better than charCNN). Note that, by design, this model
sees no difference between vanilla and scrambled texts; the Scr performance
reported can therefore be compared to the vanilla performance of other models.
However, meanChar is still very sensitive to Nat and Key noise types, that do
not resemble Rand.&lt;/p&gt;

&lt;h4 id=&quot;personal-notes-on-structure-invariant-and-graph-based-representations&quot;&gt;Personal notes on structure-invariant and graph-based representations&lt;/h4&gt;

&lt;p&gt;To me, this model seems somewhat coarse. While it is true that averaging
all character embeddings removes the reliance on word structure, it also
discards a huge part of the information brought by individual character. Unless
you are using an embedding dimension that is of the order of the size of your
alphabet, the
signal transmitted to the convolutional layer dismisses &lt;em&gt;some&lt;/em&gt; information
about the &lt;em&gt;presence&lt;/em&gt; of individual characters, in addition to throwing away
all information about &lt;em&gt;order&lt;/em&gt;.
In other words, not only anagrams will get the same representation, but also
some totally different words could by random chance.
One could argue that, with a good encoding
(e.g. simply using the powers of 2, which is basically cheating by pretending
that a full-dimensional one-hot vector can be reduced to a 1-dimensional scalar
value), we could always recover the presences; however I cannot think of one
that would suit the convolutional structure of the next layer. To be fair, the
authors themselves conceded, in a response to a reviewer, that “meanChar may
not be the ideal architecture for capturing noise, but it’s a simple,
structure-invariant representation that works reasonably well. We have tried
several other architectures, including a self-attention mechanism, but haven’t
been able to improve beyond it.”&lt;/p&gt;

&lt;p&gt;Have a look at Figures 2 and 3 below, depicting the charCNN vs. meanChar
word representations. I am pretty sure
that a better word representation can be found, discarding only the structure
while preserving the presence (instead of “jeter le bébé avec l’eau du bain”,
like we say in French). A first thought is a complete &lt;strong&gt;graph of individual
characters&lt;/strong&gt; (see Figure 4), with a graph-convolutional layer that would follow.
This seems to me like a more natural generalization of the previous models:
thinking in terms of graphs where nodes are the characters, previous
representations saw a word as a chain of characters, each of them linked only to
the next one by a directed edge. Discarding structure would simply mean linking
all nodes together by undirected edges.&lt;/p&gt;

&lt;p&gt;This could even
be adapted to retain &lt;em&gt;some weak&lt;/em&gt; information about the structure by &lt;strong&gt;weighting
edges&lt;/strong&gt; using the original order (the closer two characters in the original
word, the higher the weight; see Figure 5). These weights would for example play
a role in interaction with regularization, for example $L_2$-regularization on
the coefficients of the convolutional filters (for very low weights, it will
be too expensive to take those into account unless there is a very good reason).
While not completely structure-invariant, this has a potential to make the
model more robust to
structure change, while retaining all information about the presence of
individual characters and not forgetting all about structure like in meanChar.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_chain.png&quot; alt=&quot;Graph chain&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt; &lt;strong&gt;Figure 2&lt;/strong&gt; - charCNN word representation
as a chain of character embeddings. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_meanchar.png&quot; alt=&quot;Graph chain&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt; &lt;strong&gt;Figure 3&lt;/strong&gt; - meanChar word representation as an average
of character embeddings. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_complete.png&quot; alt=&quot;Graph complete&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt; &lt;strong&gt;Figure 4&lt;/strong&gt; - graphChar word
representation as a complete graph of character embeddings. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-graph_weighted.png&quot; alt=&quot;Graph weighted&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption&quot;&gt; &lt;strong&gt;Figure 5&lt;/strong&gt; - wgraphChar word
representation as a weighted graph of character embeddings. The weights, here
arbitrary, could reflect the structure to some extent. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-on-noisy-texts&quot;&gt;Training on noisy texts&lt;/h3&gt;

&lt;p&gt;Another natural idea is to expose the model to noisy texts during training. I am
not too sure that we can adequately call this &lt;em&gt;black-box adversarial training&lt;/em&gt;
like the authors did, but the concept remains the same: is the model
systematically failing to handle a specific type of inputs? Fine, let’s train
it on some of these specific inputs, and hope that it has the capacity to learn
a relevant pattern.&lt;/p&gt;

&lt;h4 id=&quot;with-meanchar---personal-doubts&quot;&gt;With meanChar - personal doubts&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-meanchar_noisy_training.png&quot; alt=&quot;meanChar noisy training&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is not very convincing. All in all, training on noise type A seems to
improve performance when testing on type A noisy texts in French and German, but
not in Czech. There is no conclusive result in this table. Note that we didn’t
expect meanChar to deal well with Key and Nat noises anyway.&lt;/p&gt;

&lt;p&gt;There is however something quite troubling with this table. Remember that
meanChar is supposedly &lt;em&gt;invariant to word structure&lt;/em&gt;, and should therefore
remain unaffected by Rand noise (scrambling), since the model &lt;em&gt;does not make any
difference between the original word and the scrambled version&lt;/em&gt;. How comes there
is such a huge difference in performance between training on Rand+Key and on Key
alone? What’s more, this difference is not even consistent across languages.
Rand is affecting the training way too much, which means &lt;strong&gt;one of the following
is wrong: the implementation, the results (including their expected stability),
or my understanding of the paper&lt;/strong&gt;. I’d rather it be the latter.&lt;/p&gt;

&lt;h4 id=&quot;with-charcnn---insights-on-the-filters&quot;&gt;With charCNN - insights on the filters&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-charcnn_noisy_training.png&quot; alt=&quot;charCNN noisy training&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More extensive experiments show that the more complicated charCNN is, generally
speaking, &lt;strong&gt;robust to the noise types is was exposed to during training, and
only those&lt;/strong&gt;. Additionally, except when trained on Nat noise alone, all charCNN
models keep good performance on vanilla test texts. A model trained on all noise
types mixed is not the best in any specific setting, but is the best on average.&lt;/p&gt;

&lt;p&gt;From these results, the authors draw three major insights:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When trained on a mix of different noise types, the charCNN is robust to all
noise types that it has been exposed to. In particular, the model trained on
Rand+Key+Nat shows good performance in all settings, and gives (according to
the authors) a reasonable translation of the Cambridge meme:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;centeredquote&quot;&gt;
According to a study of Cambridge University, it doesn't matter
which technology in a word is going to get the letters in a word that is the
only important thing for the first and last letter.
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When trained on synthetic noise, no charCNN model was able to correctly handle
natural noise. This is actually a huge problem, since natural noise is what we
mostly care about, while synthetic noise is what we can easily generate. More
on that in the next section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It is puzzling that, despite the sensitivity of convolutions to the structure
of the input, charCNN is able to perform well on scrambled texts when trained
on the corresponding noise.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us dwell a bit more on the last point. The authors propose an interesting
analysis of the convolution filters learned in different settings. They
investigate, one dimension (out of the 25 of character embeddings) at a time,
what the variance of the weights of the filters are. Their idea is the
following: in the scrambled setting, their is no pattern to detect in the
character ordering; therefore the variance of weights along a given dimension
of the character embeddings should be low, i.e. those weights should lie close
to one another.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-09-10-weight_variances.png&quot; alt=&quot;Weight variances&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To test their hypothesis, the authors take, for each filter (out of 1000) and
each embedding dimension (out of 25), the variance of the weights across the
filter width (6 characters). Intuitively, we expect this variance to be small
when the filter is close to being uniform, that is, when the filter is actually
performing an average of the embeddings over this specific dimension.
They then average these variances across the
filters, yielding 25 average variances, which are plotted above.&lt;/p&gt;

&lt;p&gt;These results tend to support the hypothesis, although I disagree with the
authors’ assertion that “low average variance means that different filters tend
to learn similar behavior, while high average variance means that they learn
different patterns”. From my understanding, high average variance means that
many filters tend to learn a non-uniform pattern, yielding many high individual
variances, and a high average. For example, if all filters learned were
$[1,2,3,4,5,6]$, the average variance would be equal to the individual variance
of $2.9$, which is quite high; nonetheless, all filters learn the same exact
pattern. I would also like to note that small variance does not always mean
uniform behavior, and that variances can not be so easily compared. For example,
if all filters learned were $[0.1,0.2,0.3,0.4,0.5,0.6]$, the variance would be
$0.029$, which is much lower than the previous one; yet it is not obvious that
the filters learned correspond to a more uniform pattern than the previous case.
I doubt however that there is such a scale difference in the actual models
trained, so I would buy the authors’ interpretation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[…] with random scrambling there are no patterns to detect in the data, so
filters resort to close to uniform weights. [… Moreover], in the Rand model,
the variance of variances [size of the box] is close to zero, indicating that in
all character embedding dimensions the learned weights are of small variance;
[…] the model learned to reproduce a representation similar to the meanChar
model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;in-the-end-a-call-for-better-noise&quot;&gt;In the end, a call for better noise&lt;/h2&gt;

&lt;p&gt;Let’s recap. In the major field of Neural Machine Translation, state-of-the-art
models fail to correctly handle even a small amount of noise. This holds true
for a variety of noise types, including altering the order of letters in a word,
key swapping based on keyboard proximity, and natural noise. While scrambling
noise can be correctly handled by structure invariant word representation, such
as an average of character embeddings (see the meanChar model), it remains
challenging to develop models that are robust to key swapping and natural noise.
Training the model on noisy texts yields an increased robustness to the noise
types that the model has been exposed to, but decreases general performance,
calling for better representations, architectures and understanding of the
effect of noise, in order to develop satisfyingly robust models, that could
approach human insensitivity to noise in natural language processing.&lt;/p&gt;

&lt;p&gt;One point in particular stands out of the paper: natural noise can not be
handled by training on synthetic noise. Moreover, it appears that the former
qualitatively differs from the latter, being mostly composed of phonological
mistakes, character omissions, and incorrect conjugations of verbs. This is a
major issue, since natural noise is what we ultimately (in most cases) care
about, and synthetic noise is what we are able to easily generate (hence to
gather data on). The lack of mechanisms to generate realistic natural human
errors in an automatic way remains a huge issue. All in all:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We believe that more work is necessary in order to immune NMT models against
natural noise. As corpora with natural noise are limited, another approach to
future work is to design better NMT architectures that would be robust to noise
without seeing it in the training data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I personally trust that we will eventually find a good NMT model
paradigm that will enable correct translation of both scrambling noise, through
structure invariant (or less sensitive) word representation, and key swapping,
by integrating a confusion matrix (built from the keyboard) to the character
embeddings. However, I think it is still a long way until we find architectures
that can handle natural noise without being previously trained on it, so I would
vote for algorithms to generate more realistic synthetic noise.&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="ICLR 2018" /><category term="Neural Machine Translation" /><category term="Noise" /><category term="NLP" /><category term="ICLR" /><summary type="html"></summary></entry><entry><title type="html">The Gini coefficient is not the best inequality metric</title><link href="http://localhost:4000/gini-coefficient/" rel="alternate" type="text/html" title="The Gini coefficient is not the best inequality metric" /><published>2018-08-21T00:00:00+02:00</published><updated>2018-08-21T00:00:00+02:00</updated><id>http://localhost:4000/gini-coefficient</id><content type="html" xml:base="http://localhost:4000/gini-coefficient/">&lt;p&gt;Or is it?&lt;/p&gt;

&lt;p&gt;I’d like this thing not to be included&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="Miscellaneous" /><category term="Econometrics" /><category term="Inequality" /><category term="Atkinson" /><summary type="html">Or is it?</summary></entry><entry><title type="html">Adam’s convergence proof is flawed</title><link href="http://localhost:4000/ICLR/" rel="alternate" type="text/html" title="Adam's convergence proof is flawed" /><published>2018-08-19T01:00:00+02:00</published><updated>2018-08-19T01:00:00+02:00</updated><id>http://localhost:4000/ICLR</id><content type="html" xml:base="http://localhost:4000/ICLR/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iclr-2018---1st-article&quot;&gt;ICLR 2018 - 1st article&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;In this series, we explore the 2018 edition of the International Conference
on Learning Representations. Oral papers are analyzed and
commented in an accessible way.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article is based on the paper&lt;/em&gt;
&lt;a href=&quot;https://openreview.net/pdf?id=ryQu7f-RZ&quot;&gt;On the convergence of Adam and beyond&lt;/a&gt;
&lt;em&gt;by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you asked people in the deep learning community what their favorite optimization
algorithm is, you’d probably see &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;
in the top 2 (with Stochastic gradient descent + momentum),
far ahead of alternatives like Adagrad or RMSProp (we’ll come to
these in a minute). &lt;b&gt;Adam has become very popular in deep
learning since it was proposed by Kingma &amp;amp; Ba in December 2014&lt;/b&gt;.
The reasons are easy to
understand: it exhibits impressive performance, uses many important ideas of
previous works, comes with predefined settings that already work very well,
and does not require careful hyper-parameter tuning.
&lt;em&gt;Better performance with less work, what more do you want?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Naturally, great power comes with great responsibilities, even for optimization
algorithms like Adam. At the very least, we expect some theoretical guarantees
on the convergence properties, to make sure that the algorithm is actually doing
its job when required. This is a necessary sanity check, confirming by
the maths that we can legitimately have faith in its correctness.&lt;/p&gt;

&lt;p&gt;The convergence proof was provided in the original Adam paper.
However, our authors show in their paper that &lt;strong&gt;this original proof is flawed,
and Adam does not correctly converge in all problems&lt;/strong&gt;, as illustrated by this
theorem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; [With mild assumptions on the parameters], there is a
stochastic convex optimization problem for which Adam does not converge to the
optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;How did we reach this point, where the most widely used
optimization algorithm in deep learning does not converge in some simple
convex problems? Let’s go back some years, and see what lead us there.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-and-adaptive-methods&quot;&gt;Gradient descent and adaptive methods&lt;/h2&gt;

&lt;p&gt;To facilitate analysis, let’s define our online optimization framework.
At each time step $t$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The algorithm picks a
point $x_t$ (weights of the model) in the feasible set $\mathcal{F}$.&lt;/li&gt;
  &lt;li&gt;We then
get access to the next mini-batch, with the associated loss function $f_t$,
and we incur the loss $f_t(x_t)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, our goal is to find an optimal
parameter $x$ such that the loss $f(x)$ on the entire training set is minimal
(intermediate loss functions $f_t$ are used as stochastic approximations of
$f$). Here, in the (equivalent) online setup, the goal
is to &lt;strong&gt;minimize the total regret at time $T$&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_T = \sum_{i=1}^T f_t(x_t) - \min_{x \in
\mathcal{F}} \left(\sum_{i=1}^T f_t(x)\right)&lt;/script&gt;

&lt;p&gt;A large number of optimization algorithms were proposed to achieve this goal.
For a detailed and intuitive overview (and introduction to gradient descent),
I recommend this excellent
&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;blog post&lt;/a&gt; by Sebastian
Ruder. We will just quickly go over a couple major steps that lead us to Adam.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Mini-batch/Stochastic gradient descent&lt;/strong&gt; is the first variant of gradient
descent that actually works decently well in deep learning. In our online setup,
it is reframed as &lt;em&gt;online gradient descent&lt;/em&gt;, where the points $x_t$ are updated
by moving in the opposite direction of the gradient $g_t = \nabla f_t(x_t)$,
computed on the available mini-batch. The update size is determined by a
learning rate $\alpha_t$ (typically $\alpha/\sqrt{t}$ for some $\alpha$),
and the result is projected back to the feasible set thanks to the projection
operator $\Pi_{\mathcal{F}}$. &lt;br /&gt;
We thus get the &lt;strong&gt;update rule of SGD:
$\ x_{t+1} = \Pi_\mathcal{F}(x_t - \alpha_t g_t)$&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although the convergence of SGD requires a decreasing learning rate, choosing
an adequate learning rate decrease schedule can be painful. Aggressive decays,
such as $\alpha/\sqrt{t}$, or small learning rates, yield very slow convergence
and mediocre performance. On the other hand, gentle decays and high learning
rates yield very unstable training, even divergence sometimes. To overcome these
issues, &lt;strong&gt;adaptive methods&lt;/strong&gt; have been developed around the key idea that &lt;strong&gt;each
weight, that is each coordinate of $x_t$, should be updated using its own
learning rate&lt;/strong&gt;, automatically computed based on the knowledge of past updates.
This way, parameters that are frequently updated take only small steps (to avoid
divergence), while parameters that are rarely updated take rather huge steps
(to speed up convergence). This is summed up in the generic adaptive framework
below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-adaptive-methods.png&quot; alt=&quot;Adaptive methods&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We call $\alpha_t$ the step size, $\alpha_t/\sqrt{V_t}$ the learning rate,
and restrict ourselves to diagonal variants $V_t = \text{diag}(v_t)$.
This framework essentially allows us to compute a different learning rate for
each weight, by rescaling the step size (common to all weights) with a function
of past gradients (of the loss function with respect to the considered weight,
thus &lt;em&gt;weight-specific&lt;/em&gt;). Using this framework, we can follow the evolution of
adaptive methods over time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;SGD&lt;/a&gt;) Stochastic gradient descent&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = \mathbb{I}$&lt;/center&gt;
    &lt;p&gt;SGD is also an adaptive method, with a specific strategy of not adapting at
all: it forgets everything about the past of each weight, and relies only on
the current gradient to perform the update, with a step size
(and learning rate) $\alpha_t =\alpha/\sqrt{t}$, an aggressive decay.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;AdaGrad&lt;/a&gt;) Adaptive gradient descent&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}(\dfrac{1}{t} \sum_{i=1}^t g_i^2)}$&lt;/center&gt;
    &lt;p&gt;With the same step size $\alpha_t = \alpha/\sqrt{t}$, this yields an adaptive
and much more reasonable learning rate of $\alpha/\sqrt{\sum_i g_{ij}}$ for
a the $j$-th weight. When the gradients $\{g_{ij}\}$ are sparse, i.e.
the $j$-th weight is not frequently updated, this &lt;strong&gt;considerably speeds up
convergence&lt;/strong&gt; (updates are still rare, but much bigger than with SGD).&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://www.coursera.org/lecture/neural-networks/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude-YQHki&quot;&gt;RMSProp&lt;/a&gt;)&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = g_t\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta)\sum_{i=1}^t \beta^{t-i} g_i^2)}$&lt;/center&gt;
    &lt;p&gt;Despite its huge benefits in some settings, AdaGrad tends to shrink the
learning rates of frequently updated parameters, very quickly to virtually
zero. To overcome this issue, RMSProp restricts the average of past gradients
to a fixed window, instead of the entire past, to avoid shrinking the
learning rates to virtually zero too quickly. In practice, this is
implemented by an &lt;strong&gt;exponentially moving average of past gradients&lt;/strong&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(&lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;&gt;Adam&lt;/a&gt;) Adaptive momentum estimation&lt;/strong&gt;
    &lt;center&gt;$\phi_t(g_1, ..., g_t) = (1-\beta_1)\sum_{i=1}^t \beta_1^{t-i} g_i^2{\ }\text{  }$ and
$\text{  }\psi_t(g_1, ..., g_t) = {\text{diag}((1-\beta_2)\sum_{i=1}^t \beta_2^{t-i} g_i^2)}$&lt;/center&gt;
    &lt;p&gt;To further speed up the convergence, Adam adds to RMSProp the idea of
&lt;strong&gt;momentum&lt;/strong&gt; (instead of moving in the direction
of the current gradient only, move in a direction that is a weighted average
of current gradient and previous update, like a rolling ball’s movement is
is influenced by the both the current slope and its momentum; see
&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;Ruder’s post&lt;/a&gt;).
The momentum parameter $\beta_1&amp;gt;0$ considerably
improves the performance of the algorithm, especially in deep learning.
Combining momentum with adaptive methods, Adam is very efficient, and
immensely popular.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-non-convergence-of-adam&quot;&gt;The non-convergence of Adam&lt;/h2&gt;

&lt;p&gt;So, what’s wrong with Adam? It turns out that in the convergence proof given
by Kingma &amp;amp; Ba, a flaw resides in the consideration of a specific quantity,
namely:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ \Gamma_{t+1} = \left(\dfrac{\sqrt{V_{t+1}}}{\alpha_{t+1}} -
\dfrac{\sqrt{V_{t}}}{\alpha_{t}}\right)&lt;/script&gt;

&lt;p&gt;Intuitively, $\Gamma_t$ measures the
variations of the inverse of the learning rate. As we saw, convergence requires
a &lt;strong&gt;non-increasing learning rate&lt;/strong&gt; (otherwise the algorithm oscillates too much,
or even diverges), which directly translates to $\Gamma_t \succeq O$ (easy to
see with a diagonal $V_t$). This is the case for both SGD ($v_t$ constant,
$\alpha_t$ decreasing) and AdaGrad ($v_t$ non-decreasing,
$\alpha_t$ decreasing). However, it is no longer the case for Adam (nor RMSProp)
because of the exponentially moving average of past gradients.&lt;/p&gt;

&lt;p&gt;This is actually a huge issue, meaning that in some cases Adam actually
converges to the worst solution possible. Following the example provided by the
authors, consider $C&amp;gt;2$ and $\mathcal{F} = [-1,1]$, and the following losses:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-ADAM_counterexample2.png&quot; alt=&quot;ADAM counterexample&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The optimal solution would be $x=-1$. However, choosing $\beta_1 = 0$ and
$\beta_2 = {1}/{(1+C^2)}$, Adam actually converges to the worst possible point
$x = +1$. The intuition for this behavior is the following: although the
algorithm observes a large gradient $C$ every $3$ steps,
&lt;strong&gt;it forgets (scales down) this large gradient too quickly (due to the
exponential weights)&lt;/strong&gt; to counterbalance the wrong but most frequent updates.&lt;/p&gt;

&lt;p&gt;So you thought Adam was reliable in all convex (“easy”) cases? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt; There is an online convex optimization problem where Adam has
non-zero average regret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wait, but this is for a specific choice of parameters. We allowed the choice of
a very small $\beta_2$, and to quote the paper, “this example also provides
intuition for why large $\beta_2$ is advisable […], and indeed in practice
using large $\beta_2$ helps”. So maybe we are safe when using correctly chosen
parameters? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt; For any constant $\beta_1,\beta_2 \in [0,1)$ such that $\beta_1
&amp;lt; \sqrt{\beta_2}$ [typically satisfied by default settings], there is an online
convex optimization problem where Adam has non-zero average regret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;All right, but this was in online optimization, where one could carefully design
a sequence of loss functions to specifically fool Adam. We should be fine with
stochastic optimization where such a design is impossible, right? Well, no:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; For any constant $\beta_1,\beta_2 \in [0,1)$ such that $\beta_1
&amp;lt; \sqrt{\beta_2}$ [typically satisfied by default settings], there is
a stochastic convex optimization problem for which Adam does not converge to the
optimal solution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now this is actually a major concern, since stochastic convex optimization is
actually one of the simplest problems that we expect Adam to be able to solve
(and deep learning consists mostly of non-convex stochastic optimization,
which is reputed much harder). In practice, “fixing Adam” in cases of bad
convergence behavior would typically require using different hyper-parameters
for each dimension which, well, makes you wonder why you’re using adaptive
methods in the first place (especially given the very high number of dimensions
in typical deep learning applications).&lt;/p&gt;

&lt;p&gt;Another important note is that this analysis remains valid for &lt;strong&gt;any adaptive
method that is based on exponentially weighted moving averages (EWMA) of the
gradients&lt;/strong&gt;, including RMSProp, AdaDelta and NAdam, which are therefore also
flawed.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;amsgrad-a-new-and-fixed-variant&quot;&gt;AMSGrad, a new (and fixed) variant&lt;/h2&gt;

&lt;p&gt;Now that the maths have spoken and revealed the problem with Adam,
what can be done? EWMA-based algorithms
have actually brought a lot in terms of performance improvement and robustness
to hyper-parameters choice, and it would be quite a pity to simply throw it all
away for just a hole discovered in the proof.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-AMSGRAD.png&quot; alt=&quot;AMSGrad&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To overcome this issue, the authors propose a new variant, AMSGrad, described
in the algorithm above, that we explain in the next few lines.
Remember that the issue with Adam resides in $\Gamma_t$, which should be
semi-definite positive, while in some cases it is not because
$\frac{v_{t+1, j}}{\alpha_{t+1}} - \frac{v_{t, j}}{\alpha_{t}} &amp;lt; 0$ for some
index $j$. Well, a quick hotfix would be to replace $v_{t+1}$ by $\max(v_t,
v_{t+1})$, which would ensure $\Gamma_t \succeq 0$. But this would bias our
computations of the EWMA…
However we can decouple the two processes by keeping
two running variables: one “true” running variable $v_{t+1}$ used to accurately
compute the EWMA (correctly accounting for the past), and one “used-in-updates”
running variable ${\hat v_{t+1}} = {\max(v_{t+1}, \hat v_t)}$ used to ensure
a non-increasing learning rate (hence the correctness of the algorithm).&lt;/p&gt;

&lt;p&gt;Theoretical justifications of AMSGrad back up this claim: for reasonable choices
of parameters, the regret is bounded in $O(\sqrt{T})$. Additionally, for a
specific choice of momentum decay $\beta_{1t} = \beta_1 \lambda^{t-1}$, the
authors prove a bound on the regret that can hugely benefit from sparse
gradients, potentially “considerably better than $O(\sqrt{dT})$ regret of SGD”.&lt;/p&gt;

&lt;p&gt;Does this translate into good empirical performance? The paper provides the
convergence curves for a synthetic example that is close to the previous
example (with $\mathcal{F} = [-1, 1]$, the optimal solution being $x=-1$):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-synthetic_example.png&quot; alt=&quot;Synthetic example&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-convergence_curves.png&quot; alt=&quot;Convergence curves&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This example shows that AMSGrad works on the synthetic example where Adam fails
to converge to the true solution, which is always a good sign, since it was why
it was designed in the first place. But what about standard convergence
benchmarks on more interesting problems, like logistic regression or neural
networks? Reddi et al. provide graphics that claim for AMSGrad
equal or better performance compared to Adam for logistic regression,
feed-forward neural net (1 hidden layer) on MNIST, and CIFARNET. However, in an
independent
&lt;a href=&quot;https://fdlm.github.io/post/amsgrad/&quot;&gt;implementation by Filip Korzeniowski&lt;/a&gt;
tested in various settings (you should definitely check his post for extensive
details and comparisons), the experiments do not support any claim of
practical difference between Adam and AMSGrad. We show below the validation
accuracy (what we care about in the end) he gets for both algorithm on CIFAR-10.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2018-08-19-comparison.png&quot; alt=&quot;Comparison&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;inpost-figure-caption-centered&quot;&gt;Validation accuracy on CIFAR-10
(VGG-16). &lt;br /&gt;Adam in blue, AMSGrad in red.
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;does-it-matter&quot;&gt;Does it matter?&lt;/h2&gt;

&lt;p&gt;In my opinion, it is pretty clear that the main interest of the paper lies
in pointing out the flaw in Adam’s proof, and how it affects the convergence
behavior in some cases. AMSGrad sounded somewhat more like a hotfix than a real
replacement, providing a sounder algorithm while maintaining Adam’s good
performance. AMSGrad will not change the course of stochastic optimization’s
(nor deep learning’s) history, may it even come to someday replace Adam
in practice. &lt;em&gt;In the end, it doesn’t even matter (that much)&lt;/em&gt; ; a true
“breakthrough” replacement is yet to come.&lt;/p&gt;

&lt;p&gt;What is more interesting is the meta-aspect of the proof-checking side of this
paper. It seems very important to me that empirical observations of bad
convergence behavior are finally backed up by theoretical justifications, in
other words, that &lt;strong&gt;someone finally checked the maths&lt;/strong&gt;. I find it actually
surprising that despite its “reasonable” length of 4 pages, no one checked the
proof and spotted this mistake, while in the meantime Adam got increasingly
popular and widely used. Note that I barely ever read the proofs myself. I would
tend to link this phenomenon to a deeper trend in the machine/deep learning
community: people (and scientists are people too) get really excited about the
results, the performance and the new opportunities brought by an exponentially
growing field, and we just forget how to do good science along the way. The
reproducibility crisis denounced by researchers like Joëlle Pineau
(especially in reinforcement learning) is one example; the fact that everyone
skips the proofs to go straight to the next exciting paper is another.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Romain Girard</name></author><category term="ICLR 2018" /><category term="Adam" /><category term="Optimization" /><category term="Deep Learning" /><category term="Neural Networks" /><category term="ICLR" /><summary type="html"></summary></entry></feed>